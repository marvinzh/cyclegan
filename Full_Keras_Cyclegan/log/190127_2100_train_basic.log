_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         (None, 1, 600, 1)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 1, 600, 64)        256       
_________________________________________________________________
leaky_re_lu_5 (LeakyReLU)    (None, 1, 600, 64)        0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 1, 600, 128)       24704     
_________________________________________________________________
leaky_re_lu_6 (LeakyReLU)    (None, 1, 600, 128)       0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 76800)             0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               39322112  
_________________________________________________________________
leaky_re_lu_7 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 512)               262656    
_________________________________________________________________
leaky_re_lu_8 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
D_A_Out (Dense)              (None, 1)                 513       
=================================================================
Total params: 79,220,482
Trainable params: 39,610,241
Non-trainable params: 39,610,241
_________________________________________________________________
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 1, 600, 1)         0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 1, 600, 64)        256       
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU)    (None, 1, 600, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 1, 600, 128)       24704     
_________________________________________________________________
leaky_re_lu_2 (LeakyReLU)    (None, 1, 600, 128)       0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 76800)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               39322112  
_________________________________________________________________
leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
D_B_Out (Dense)              (None, 1)                 513       
=================================================================
Total params: 79,220,482
Trainable params: 39,610,241
Non-trainable params: 39,610,241
_________________________________________________________________
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
ivec_b (InputLayer)             (None, 1, 600, 1)    0                                            
__________________________________________________________________________________________________
ivec_a (InputLayer)             (None, 1, 600, 1)    0                                            
__________________________________________________________________________________________________
model_4 (Model)                 (None, 1, 600, 1)    653345      ivec_b[0][0]                     
                                                                 model_3[1][0]                    
                                                                 ivec_a[0][0]                     
__________________________________________________________________________________________________
model_3 (Model)                 (None, 1, 600, 1)    653345      ivec_a[0][0]                     
                                                                 model_4[1][0]                    
                                                                 ivec_b[0][0]                     
__________________________________________________________________________________________________
model_2 (Model)                 (None, 1)            39610241    model_4[1][0]                    
__________________________________________________________________________________________________
model_1 (Model)                 (None, 1)            39610241    model_3[1][0]                    
==================================================================================================
Total params: 80,527,172
Trainable params: 1,306,690
Non-trainable params: 79,220,482
__________________________________________________________________________________________________
Start CycleGAN training.
[Epoch 1/20] [Batch 0/516] [D loss: 1.634658, acc:   0%] [G loss: 22.110607, adv: 0.181872, cyc: 0.989987, id: 0.973325] time: 0:02:05.535951 
[Epoch 1/20] [Batch 100/516] [D loss: 0.166389, acc:  80%] [G loss: 14.893788, adv: 0.557493, cyc: 0.637570, id: 0.499450] time: 0:03:15.573329 
[Epoch 1/20] [Batch 200/516] [D loss: 0.021245, acc: 100%] [G loss: 13.138221, adv: 0.797093, cyc: 0.533545, id: 0.437135] time: 0:04:25.570079 
[Epoch 1/20] [Batch 300/516] [D loss: 0.020234, acc:  99%] [G loss: 12.081014, adv: 0.716523, cyc: 0.491680, id: 0.417255] time: 0:05:35.535776 
[Epoch 1/20] [Batch 400/516] [D loss: 0.019735, acc: 100%] [G loss: 11.447143, adv: 0.655687, cyc: 0.467430, id: 0.386607] time: 0:06:45.496721 
[Epoch 1/20] [Batch 500/516] [D loss: 0.010834, acc: 100%] [G loss: 11.775419, adv: 0.805575, cyc: 0.468048, id: 0.378062] time: 0:07:55.451844 
[Epoch 2/20] [Batch 0/516] [D loss: 0.013340, acc: 100%] [G loss: 12.040821, adv: 0.889361, cyc: 0.472141, id: 0.400320] time: 0:08:06.644522 
[Epoch 2/20] [Batch 100/516] [D loss: 0.074143, acc:  87%] [G loss: 11.704600, adv: 1.023709, cyc: 0.444151, id: 0.352229] time: 0:09:16.604459 
[Epoch 2/20] [Batch 200/516] [D loss: 0.010095, acc: 100%] [G loss: 10.818395, adv: 0.951300, cyc: 0.410586, id: 0.348649] time: 0:10:26.548793 
[Epoch 2/20] [Batch 300/516] [D loss: 0.005336, acc: 100%] [G loss: 10.705077, adv: 0.991418, cyc: 0.401248, id: 0.347702] time: 0:11:36.497764 
[Epoch 2/20] [Batch 400/516] [D loss: 0.005206, acc:  99%] [G loss: 10.495710, adv: 1.004414, cyc: 0.390268, id: 0.323770] time: 0:12:46.439069 
[Epoch 2/20] [Batch 500/516] [D loss: 0.009449, acc: 100%] [G loss: 10.448053, adv: 0.947685, cyc: 0.392965, id: 0.319458] time: 0:13:56.383338 
[Epoch 3/20] [Batch 0/516] [D loss: 0.007090, acc: 100%] [G loss: 10.547198, adv: 0.985075, cyc: 0.394305, id: 0.337984] time: 0:14:07.576264 
[Epoch 3/20] [Batch 100/516] [D loss: 0.002936, acc: 100%] [G loss: 10.257247, adv: 0.931185, cyc: 0.386025, id: 0.312648] time: 0:15:17.509011 
[Epoch 3/20] [Batch 200/516] [D loss: 0.008795, acc:  99%] [G loss: 9.611689, adv: 0.834947, cyc: 0.365591, id: 0.316222] time: 0:16:27.450793 
[Epoch 3/20] [Batch 300/516] [D loss: 0.007108, acc: 100%] [G loss: 9.764204, adv: 0.946606, cyc: 0.362100, id: 0.319247] time: 0:17:37.389572 
[Epoch 3/20] [Batch 400/516] [D loss: 0.010263, acc: 100%] [G loss: 9.843469, adv: 1.053469, cyc: 0.355579, id: 0.298258] time: 0:18:47.321367 
[Epoch 3/20] [Batch 500/516] [D loss: 0.064070, acc:  88%] [G loss: 9.953825, adv: 1.026056, cyc: 0.362735, id: 0.300626] time: 0:19:57.265490 
[Epoch 4/20] [Batch 0/516] [D loss: 0.002701, acc: 100%] [G loss: 9.928346, adv: 0.989708, cyc: 0.365308, id: 0.312694] time: 0:20:08.456555 
[Epoch 4/20] [Batch 100/516] [D loss: 0.017784, acc:  98%] [G loss: 9.798093, adv: 0.990814, cyc: 0.358755, id: 0.291380] time: 0:21:18.386999 
[Epoch 4/20] [Batch 200/516] [D loss: 0.001548, acc: 100%] [G loss: 9.510781, adv: 0.998438, cyc: 0.345536, id: 0.295466] time: 0:22:28.312047 
[Epoch 4/20] [Batch 300/516] [D loss: 0.004013, acc: 100%] [G loss: 9.439592, adv: 0.979617, cyc: 0.343739, id: 0.299130] time: 0:23:38.246527 
[Epoch 4/20] [Batch 400/516] [D loss: 0.001889, acc: 100%] [G loss: 9.350679, adv: 0.981652, cyc: 0.339125, id: 0.284374] time: 0:24:48.174749 
[Epoch 4/20] [Batch 500/516] [D loss: 0.066142, acc:  90%] [G loss: 9.335063, adv: 0.891477, cyc: 0.346850, id: 0.288156] time: 0:25:58.100517 
[Epoch 5/20] [Batch 0/516] [D loss: 0.001624, acc: 100%] [G loss: 9.538802, adv: 0.983629, cyc: 0.347497, id: 0.302154] time: 0:26:09.287908 
[Epoch 5/20] [Batch 100/516] [D loss: 0.005030, acc: 100%] [G loss: 9.439729, adv: 0.970120, cyc: 0.344084, id: 0.279448] time: 0:27:19.211754 
[Epoch 5/20] [Batch 200/516] [D loss: 0.002817, acc: 100%] [G loss: 9.197855, adv: 0.991384, cyc: 0.331782, id: 0.290106] time: 0:28:29.137491 
[Epoch 5/20] [Batch 300/516] [D loss: 0.002786, acc: 100%] [G loss: 9.128340, adv: 0.941036, cyc: 0.332710, id: 0.295918] time: 0:29:39.072360 
[Epoch 5/20] [Batch 400/516] [D loss: 0.001713, acc: 100%] [G loss: 9.053138, adv: 0.957729, cyc: 0.327678, id: 0.288371] time: 0:30:48.993576 
[Epoch 5/20] [Batch 500/516] [D loss: 0.001829, acc: 100%] [G loss: 9.176291, adv: 0.947645, cyc: 0.334364, id: 0.278503] time: 0:31:58.917172 
[Epoch 6/20] [Batch 0/516] [D loss: 0.002042, acc: 100%] [G loss: 9.336654, adv: 0.992070, cyc: 0.337389, id: 0.293675] time: 0:32:17.447731 
[Epoch 6/20] [Batch 100/516] [D loss: 0.008821, acc:  99%] [G loss: 9.010846, adv: 0.855695, cyc: 0.335010, id: 0.271577] time: 0:33:27.374284 
[Epoch 6/20] [Batch 200/516] [D loss: 0.003392, acc: 100%] [G loss: 9.045841, adv: 0.981855, cyc: 0.324975, id: 0.280651] time: 0:34:37.313231 
[Epoch 6/20] [Batch 300/516] [D loss: 0.006384, acc:  99%] [G loss: 9.131262, adv: 1.040069, cyc: 0.323459, id: 0.293628] time: 0:35:47.228734 
[Epoch 6/20] [Batch 400/516] [D loss: 0.000853, acc: 100%] [G loss: 8.860721, adv: 0.979190, cyc: 0.317029, id: 0.278492] time: 0:36:57.150206 
[Epoch 6/20] [Batch 500/516] [D loss: 0.000653, acc: 100%] [G loss: 9.046551, adv: 0.955312, cyc: 0.327075, id: 0.270747] time: 0:38:07.068409 
[Epoch 7/20] [Batch 0/516] [D loss: 0.001980, acc: 100%] [G loss: 9.153641, adv: 0.977613, cyc: 0.330242, id: 0.295938] time: 0:38:18.257496 
[Epoch 7/20] [Batch 100/516] [D loss: 0.003029, acc: 100%] [G loss: 9.083893, adv: 0.981795, cyc: 0.326502, id: 0.271873] time: 0:39:28.190167 
[Epoch 7/20] [Batch 200/516] [D loss: 0.002314, acc: 100%] [G loss: 9.039395, adv: 0.990739, cyc: 0.323022, id: 0.272874] time: 0:40:38.118672 
[Epoch 7/20] [Batch 300/516] [D loss: 0.001054, acc: 100%] [G loss: 8.835201, adv: 0.965633, cyc: 0.316504, id: 0.286786] time: 0:41:48.027915 
[Epoch 7/20] [Batch 400/516] [D loss: 0.001620, acc: 100%] [G loss: 8.809398, adv: 0.923751, cyc: 0.318834, id: 0.291763] time: 0:42:57.940377 
[Epoch 7/20] [Batch 500/516] [D loss: 0.000531, acc: 100%] [G loss: 9.052883, adv: 0.971822, cyc: 0.325799, id: 0.292184] time: 0:44:07.835161 
[Epoch 8/20] [Batch 0/516] [D loss: 0.001635, acc: 100%] [G loss: 8.942194, adv: 0.899045, cyc: 0.327443, id: 0.303848] time: 0:44:19.018623 
[Epoch 8/20] [Batch 100/516] [D loss: 0.000817, acc: 100%] [G loss: 8.958076, adv: 0.989653, cyc: 0.319819, id: 0.268493] time: 0:45:28.908484 
[Epoch 8/20] [Batch 200/516] [D loss: 0.016660, acc:  98%] [G loss: 8.817805, adv: 1.000043, cyc: 0.312329, id: 0.291338] time: 0:46:38.768163 
[Epoch 8/20] [Batch 300/516] [D loss: 0.000486, acc: 100%] [G loss: 8.635753, adv: 0.967334, cyc: 0.307228, id: 0.285884] time: 0:47:48.624449 
[Epoch 8/20] [Batch 400/516] [D loss: 0.000867, acc: 100%] [G loss: 8.601848, adv: 0.975251, cyc: 0.305158, id: 0.269149] time: 0:48:58.475447 
[Epoch 8/20] [Batch 500/516] [D loss: 0.000321, acc: 100%] [G loss: 8.784366, adv: 0.961321, cyc: 0.314048, id: 0.264051] time: 0:50:08.317044 
[Epoch 9/20] [Batch 0/516] [D loss: 0.001465, acc: 100%] [G loss: 8.925935, adv: 0.980969, cyc: 0.318907, id: 0.286142] time: 0:50:19.492180 
[Epoch 9/20] [Batch 100/516] [D loss: 0.001486, acc: 100%] [G loss: 8.835155, adv: 0.950334, cyc: 0.317225, id: 0.267045] time: 0:51:29.336607 
[Epoch 9/20] [Batch 200/516] [D loss: 0.001268, acc: 100%] [G loss: 8.584682, adv: 0.971949, cyc: 0.304546, id: 0.279036] time: 0:52:39.178133 
[Epoch 9/20] [Batch 300/516] [D loss: 0.001963, acc: 100%] [G loss: 8.659520, adv: 0.948522, cyc: 0.309246, id: 0.271939] time: 0:53:49.021836 
[Epoch 9/20] [Batch 400/516] [D loss: 0.000605, acc: 100%] [G loss: 8.559935, adv: 0.986864, cyc: 0.302007, id: 0.268249] time: 0:54:58.875170 
[Epoch 9/20] [Batch 500/516] [D loss: 0.000556, acc: 100%] [G loss: 8.743253, adv: 0.981810, cyc: 0.310300, id: 0.265875] time: 0:56:08.716380 
[Epoch 10/20] [Batch 0/516] [D loss: 0.000815, acc: 100%] [G loss: 8.823657, adv: 0.969003, cyc: 0.315031, id: 0.286222] time: 0:56:19.891471 
[Epoch 10/20] [Batch 100/516] [D loss: 0.001230, acc: 100%] [G loss: 8.782185, adv: 0.969697, cyc: 0.312639, id: 0.258251] time: 0:57:29.731343 
[Epoch 10/20] [Batch 200/516] [D loss: 0.001048, acc: 100%] [G loss: 8.566219, adv: 0.984069, cyc: 0.302547, id: 0.269945] time: 0:58:39.576942 
[Epoch 10/20] [Batch 300/516] [D loss: 0.002345, acc: 100%] [G loss: 8.485086, adv: 0.987292, cyc: 0.298341, id: 0.272812] time: 0:59:49.426255 
[Epoch 10/20] [Batch 400/516] [D loss: 0.000604, acc: 100%] [G loss: 8.415016, adv: 0.964332, cyc: 0.297070, id: 0.255320] time: 1:00:59.274927 
[Epoch 10/20] [Batch 500/516] [D loss: 0.001457, acc: 100%] [G loss: 8.550358, adv: 0.922017, cyc: 0.307058, id: 0.267146] time: 1:02:09.113683 
[Epoch 11/20] [Batch 0/516] [D loss: 0.009882, acc:  99%] [G loss: 8.876886, adv: 0.987430, cyc: 0.315333, id: 0.299250] time: 1:02:21.832614 
[Epoch 11/20] [Batch 100/516] [D loss: 0.001010, acc: 100%] [G loss: 8.687505, adv: 0.966462, cyc: 0.309032, id: 0.266601] time: 1:03:31.684558 
[Epoch 11/20] [Batch 200/516] [D loss: 0.001305, acc: 100%] [G loss: 8.030038, adv: 0.774825, cyc: 0.297267, id: 0.263873] time: 1:04:41.534329 
[Epoch 11/20] [Batch 300/516] [D loss: 0.001716, acc: 100%] [G loss: 8.476926, adv: 0.998465, cyc: 0.296412, id: 0.266890] time: 1:05:51.375981 
[Epoch 11/20] [Batch 400/516] [D loss: 0.000683, acc: 100%] [G loss: 8.310061, adv: 0.954527, cyc: 0.293015, id: 0.259423] time: 1:07:01.226676 
[Epoch 11/20] [Batch 500/516] [D loss: 0.002584, acc: 100%] [G loss: 8.348953, adv: 0.839460, cyc: 0.304673, id: 0.256774] time: 1:08:11.069175 
[Epoch 12/20] [Batch 0/516] [D loss: 0.001744, acc: 100%] [G loss: 8.699155, adv: 0.994025, cyc: 0.306663, id: 0.285958] time: 1:08:22.245707 
[Epoch 12/20] [Batch 100/516] [D loss: 0.001168, acc: 100%] [G loss: 8.732581, adv: 0.985888, cyc: 0.308766, id: 0.271761] time: 1:09:32.097967 
[Epoch 12/20] [Batch 200/516] [D loss: 0.000282, acc: 100%] [G loss: 8.357174, adv: 0.974705, cyc: 0.293333, id: 0.272328] time: 1:10:41.947615 
[Epoch 12/20] [Batch 300/516] [D loss: 0.010938, acc: 100%] [G loss: 8.561808, adv: 1.000348, cyc: 0.299695, id: 0.283662] time: 1:11:51.789852 
[Epoch 12/20] [Batch 400/516] [D loss: 0.000606, acc: 100%] [G loss: 8.330034, adv: 0.945588, cyc: 0.294013, id: 0.264296] time: 1:13:01.644602 
[Epoch 12/20] [Batch 500/516] [D loss: 0.011078, acc:  98%] [G loss: 8.569664, adv: 0.985631, cyc: 0.301996, id: 0.266203] time: 1:14:11.492331 
[Epoch 13/20] [Batch 0/516] [D loss: 0.001434, acc: 100%] [G loss: 8.634936, adv: 0.986108, cyc: 0.304195, id: 0.278646] time: 1:14:22.669923 
[Epoch 13/20] [Batch 100/516] [D loss: 0.000356, acc: 100%] [G loss: 8.281637, adv: 0.830230, cyc: 0.302197, id: 0.263848] time: 1:15:32.518775 
[Epoch 13/20] [Batch 200/516] [D loss: 0.001002, acc: 100%] [G loss: 8.304479, adv: 0.961851, cyc: 0.291839, id: 0.258957] time: 1:16:42.364792 
[Epoch 13/20] [Batch 300/516] [D loss: 0.000465, acc: 100%] [G loss: 8.363939, adv: 0.931446, cyc: 0.296495, id: 0.260384] time: 1:17:52.207388 
[Epoch 13/20] [Batch 400/516] [D loss: 0.000587, acc: 100%] [G loss: 8.244423, adv: 0.984622, cyc: 0.286775, id: 0.258973] time: 1:19:02.062340 
[Epoch 13/20] [Batch 500/516] [D loss: 0.000318, acc: 100%] [G loss: 8.462370, adv: 0.971241, cyc: 0.297717, id: 0.260018] time: 1:20:11.906484 
[Epoch 14/20] [Batch 0/516] [D loss: 0.001207, acc: 100%] [G loss: 8.543250, adv: 0.980112, cyc: 0.300685, id: 0.278314] time: 1:20:23.083127 
[Epoch 14/20] [Batch 100/516] [D loss: 0.000491, acc: 100%] [G loss: 8.451989, adv: 0.955121, cyc: 0.298173, id: 0.253919] time: 1:21:32.934325 
[Epoch 14/20] [Batch 200/516] [D loss: 0.001179, acc: 100%] [G loss: 8.184885, adv: 0.955024, cyc: 0.286474, id: 0.267029] time: 1:22:42.779567 
[Epoch 14/20] [Batch 300/516] [D loss: 0.000918, acc: 100%] [G loss: 8.233409, adv: 0.981656, cyc: 0.286230, id: 0.283148] time: 1:23:52.623600 
[Epoch 14/20] [Batch 400/516] [D loss: 0.001941, acc: 100%] [G loss: 8.254187, adv: 0.972432, cyc: 0.287846, id: 0.283203] time: 1:25:02.474207 
[Epoch 14/20] [Batch 500/516] [D loss: 0.000174, acc: 100%] [G loss: 8.490881, adv: 0.976829, cyc: 0.298233, id: 0.279308] time: 1:26:12.317698 
[Epoch 15/20] [Batch 0/516] [D loss: 0.000390, acc: 100%] [G loss: 8.479784, adv: 0.957922, cyc: 0.299248, id: 0.276239] time: 1:26:23.491979 
[Epoch 15/20] [Batch 100/516] [D loss: 0.000350, acc: 100%] [G loss: 8.473486, adv: 0.952849, cyc: 0.299005, id: 0.262854] time: 1:27:33.347999 
[Epoch 15/20] [Batch 200/516] [D loss: 0.000221, acc: 100%] [G loss: 8.172127, adv: 0.959942, cyc: 0.285442, id: 0.260085] time: 1:28:43.204670 
[Epoch 15/20] [Batch 300/516] [D loss: 0.000263, acc: 100%] [G loss: 8.094726, adv: 0.930794, cyc: 0.284205, id: 0.279131] time: 1:29:53.065143 
[Epoch 15/20] [Batch 400/516] [D loss: 0.000570, acc: 100%] [G loss: 8.007887, adv: 0.910329, cyc: 0.282288, id: 0.251803] time: 1:31:02.911056 
[Epoch 15/20] [Batch 500/516] [D loss: 0.000634, acc: 100%] [G loss: 8.458359, adv: 0.964822, cyc: 0.297206, id: 0.291660] time: 1:32:12.763725 
[Epoch 16/20] [Batch 0/516] [D loss: 0.000207, acc: 100%] [G loss: 8.420608, adv: 0.927297, cyc: 0.298950, id: 0.304015] time: 1:32:25.533837 
[Epoch 16/20] [Batch 100/516] [D loss: 0.000572, acc: 100%] [G loss: 8.371757, adv: 0.954778, cyc: 0.293933, id: 0.257426] time: 1:33:35.389423 
[Epoch 16/20] [Batch 200/516] [D loss: 0.000195, acc: 100%] [G loss: 8.045924, adv: 0.913898, cyc: 0.283385, id: 0.283846] time: 1:34:45.247584 
[Epoch 16/20] [Batch 300/516] [D loss: 0.000475, acc: 100%] [G loss: 7.906362, adv: 0.818653, cyc: 0.285721, id: 0.257987] time: 1:35:55.090750 
[Epoch 16/20] [Batch 400/516] [D loss: 0.000548, acc: 100%] [G loss: 8.151591, adv: 0.991188, cyc: 0.281580, id: 0.252637] time: 1:37:04.930303 
[Epoch 16/20] [Batch 500/516] [D loss: 0.000307, acc: 100%] [G loss: 8.262405, adv: 0.962120, cyc: 0.288815, id: 0.270245] time: 1:38:14.773703 
[Epoch 17/20] [Batch 0/516] [D loss: 0.021172, acc:  98%] [G loss: 8.612370, adv: 0.961397, cyc: 0.304401, id: 0.288957] time: 1:38:25.947940 
[Epoch 17/20] [Batch 100/516] [D loss: 0.000422, acc: 100%] [G loss: 8.362273, adv: 0.973377, cyc: 0.292121, id: 0.262712] time: 1:39:35.799208 
[Epoch 17/20] [Batch 200/516] [D loss: 0.000704, acc: 100%] [G loss: 8.018816, adv: 0.918303, cyc: 0.281741, id: 0.249490] time: 1:40:45.649400 
[Epoch 17/20] [Batch 300/516] [D loss: 0.000307, acc: 100%] [G loss: 8.085291, adv: 0.985913, cyc: 0.278684, id: 0.278125] time: 1:41:55.496386 
[Epoch 17/20] [Batch 400/516] [D loss: 0.000264, acc: 100%] [G loss: 8.047534, adv: 0.953626, cyc: 0.279733, id: 0.283463] time: 1:43:05.349630 
[Epoch 17/20] [Batch 500/516] [D loss: 0.000405, acc: 100%] [G loss: 8.087176, adv: 0.909822, cyc: 0.285551, id: 0.252886] time: 1:44:15.202760 
[Epoch 18/20] [Batch 0/516] [D loss: 0.003536, acc:  99%] [G loss: 8.084061, adv: 0.876377, cyc: 0.288826, id: 0.265731] time: 1:44:26.379365 
[Epoch 18/20] [Batch 100/516] [D loss: 0.000277, acc: 100%] [G loss: 8.053267, adv: 0.841153, cyc: 0.289751, id: 0.251113] time: 1:45:36.234855 
[Epoch 18/20] [Batch 200/516] [D loss: 0.001257, acc: 100%] [G loss: 8.247187, adv: 1.005811, cyc: 0.283803, id: 0.301952] time: 1:46:46.088077 
[Epoch 18/20] [Batch 300/516] [D loss: 0.000361, acc: 100%] [G loss: 8.023794, adv: 0.983494, cyc: 0.276148, id: 0.268207] time: 1:47:55.938175 
[Epoch 18/20] [Batch 400/516] [D loss: 0.001536, acc: 100%] [G loss: 7.631616, adv: 0.758318, cyc: 0.278049, id: 0.284860] time: 1:49:05.797151 
[Epoch 18/20] [Batch 500/516] [D loss: 0.000796, acc: 100%] [G loss: 8.260079, adv: 0.970660, cyc: 0.287249, id: 0.248348] time: 1:50:15.648876 
[Epoch 19/20] [Batch 0/516] [D loss: 0.000477, acc: 100%] [G loss: 8.329979, adv: 0.951817, cyc: 0.291924, id: 0.283016] time: 1:50:26.823173 
[Epoch 19/20] [Batch 100/516] [D loss: 0.000582, acc: 100%] [G loss: 8.254292, adv: 0.932935, cyc: 0.290299, id: 0.276052] time: 1:51:36.661742 
[Epoch 19/20] [Batch 200/516] [D loss: 0.000473, acc: 100%] [G loss: 8.018696, adv: 0.981696, cyc: 0.276134, id: 0.256433] time: 1:52:46.499906 
[Epoch 19/20] [Batch 300/516] [D loss: 0.000320, acc: 100%] [G loss: 8.017780, adv: 0.974897, cyc: 0.276266, id: 0.282317] time: 1:53:56.350371 
[Epoch 19/20] [Batch 400/516] [D loss: 0.000271, acc: 100%] [G loss: 7.956409, adv: 0.964917, cyc: 0.274053, id: 0.254080] time: 1:55:06.213232 
[Epoch 19/20] [Batch 500/516] [D loss: 0.000518, acc: 100%] [G loss: 8.255987, adv: 0.979881, cyc: 0.286274, id: 0.283625] time: 1:56:16.054862 
[Epoch 20/20] [Batch 0/516] [D loss: 0.000140, acc: 100%] [G loss: 8.253805, adv: 0.959909, cyc: 0.287860, id: 0.291247] time: 1:56:27.228468 
[Epoch 20/20] [Batch 100/516] [D loss: 0.000088, acc: 100%] [G loss: 8.198293, adv: 0.963978, cyc: 0.285292, id: 0.258872] time: 1:57:37.070486 
[Epoch 20/20] [Batch 200/516] [D loss: 0.007512, acc:  99%] [G loss: 7.961426, adv: 0.956401, cyc: 0.275599, id: 0.259824] time: 1:58:46.921988 
[Epoch 20/20] [Batch 300/516] [D loss: 0.000271, acc: 100%] [G loss: 7.966492, adv: 0.900901, cyc: 0.279861, id: 0.259194] time: 1:59:56.772552 
[Epoch 20/20] [Batch 400/516] [D loss: 0.000201, acc: 100%] [G loss: 7.925821, adv: 0.914950, cyc: 0.276229, id: 0.257233] time: 2:01:06.615967 
[Epoch 20/20] [Batch 500/516] [D loss: 0.000307, acc: 100%] [G loss: 8.160024, adv: 0.930679, cyc: 0.285117, id: 0.274993] time: 2:02:16.458617 
Train Finished.
