_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         (None, 1, 600, 1)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 1, 600, 64)        256       
_________________________________________________________________
leaky_re_lu_5 (LeakyReLU)    (None, 1, 600, 64)        0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 1, 600, 128)       24704     
_________________________________________________________________
leaky_re_lu_6 (LeakyReLU)    (None, 1, 600, 128)       0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 76800)             0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               39322112  
_________________________________________________________________
leaky_re_lu_7 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 512)               262656    
_________________________________________________________________
leaky_re_lu_8 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
D_A_Out (Dense)              (None, 1)                 513       
=================================================================
Total params: 79,220,482
Trainable params: 39,610,241
Non-trainable params: 39,610,241
_________________________________________________________________
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 1, 600, 1)         0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 1, 600, 64)        256       
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU)    (None, 1, 600, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 1, 600, 128)       24704     
_________________________________________________________________
leaky_re_lu_2 (LeakyReLU)    (None, 1, 600, 128)       0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 76800)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               39322112  
_________________________________________________________________
leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
D_B_Out (Dense)              (None, 1)                 513       
=================================================================
Total params: 79,220,482
Trainable params: 39,610,241
Non-trainable params: 39,610,241
_________________________________________________________________
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
ivec_b (InputLayer)             (None, 1, 600, 1)    0                                            
__________________________________________________________________________________________________
ivec_a (InputLayer)             (None, 1, 600, 1)    0                                            
__________________________________________________________________________________________________
model_4 (Model)                 (None, 1, 600, 1)    653345      ivec_b[0][0]                     
                                                                 model_3[1][0]                    
                                                                 ivec_a[0][0]                     
__________________________________________________________________________________________________
model_3 (Model)                 (None, 1, 600, 1)    653345      ivec_a[0][0]                     
                                                                 model_4[1][0]                    
                                                                 ivec_b[0][0]                     
__________________________________________________________________________________________________
model_2 (Model)                 (None, 1)            39610241    model_4[1][0]                    
__________________________________________________________________________________________________
model_1 (Model)                 (None, 1)            39610241    model_3[1][0]                    
==================================================================================================
Total params: 80,527,172
Trainable params: 1,306,690
Non-trainable params: 79,220,482
__________________________________________________________________________________________________
Start CycleGAN training.
[Epoch 1/30] [Batch 0/516] [D loss: 12.087490, acc:   0%] [G loss: 21.407955, adv: 0.469577, cyc: 0.931445, id: 0.956667] time: 0:02:08.130501 
[Epoch 1/30] [Batch 100/516] [D loss: 0.084789, acc:  89%] [G loss: 14.152546, adv: 0.992498, cyc: 0.563034, id: 0.484764] time: 0:03:17.097184 
[Epoch 1/30] [Batch 200/516] [D loss: 0.027571, acc:  98%] [G loss: 12.549586, adv: 0.940773, cyc: 0.493646, id: 0.442513] time: 0:04:26.073231 
[Epoch 1/30] [Batch 300/516] [D loss: 0.024443, acc:  98%] [G loss: 11.874769, adv: 0.874628, cyc: 0.468232, id: 0.412912] time: 0:05:35.050730 
[Epoch 1/30] [Batch 400/516] [D loss: 0.012135, acc: 100%] [G loss: 11.441067, adv: 0.879110, cyc: 0.447146, id: 0.404454] time: 0:06:44.029536 
[Epoch 1/30] [Batch 500/516] [D loss: 0.034097, acc:  98%] [G loss: 11.325181, adv: 1.083108, cyc: 0.422559, id: 0.381108] time: 0:07:53.002706 
[Epoch 2/30] [Batch 0/516] [D loss: 0.020149, acc:  99%] [G loss: 10.760249, adv: 0.865096, cyc: 0.417086, id: 0.385533] time: 0:08:04.001978 
[Epoch 2/30] [Batch 100/516] [D loss: 0.008158, acc:  99%] [G loss: 10.572639, adv: 0.896986, cyc: 0.404771, id: 0.373439] time: 0:09:12.949315 
[Epoch 2/30] [Batch 200/516] [D loss: 0.007033, acc: 100%] [G loss: 10.414554, adv: 0.924510, cyc: 0.395023, id: 0.383932] time: 0:10:21.895586 
[Epoch 2/30] [Batch 300/516] [D loss: 0.003336, acc: 100%] [G loss: 10.154836, adv: 0.874762, cyc: 0.387356, id: 0.364074] time: 0:11:30.833313 
[Epoch 2/30] [Batch 400/516] [D loss: 0.009571, acc:  99%] [G loss: 10.372703, adv: 1.052815, cyc: 0.381285, id: 0.358914] time: 0:12:39.756590 
[Epoch 2/30] [Batch 500/516] [D loss: 0.032176, acc:  96%] [G loss: 9.948561, adv: 0.961362, cyc: 0.369442, id: 0.348130] time: 0:13:48.695771 
[Epoch 3/30] [Batch 0/516] [D loss: 0.011504, acc: 100%] [G loss: 9.647228, adv: 0.873812, cyc: 0.363853, id: 0.355149] time: 0:13:59.691433 
[Epoch 3/30] [Batch 100/516] [D loss: 0.010009, acc: 100%] [G loss: 9.929274, adv: 1.078521, cyc: 0.358001, id: 0.327595] time: 0:15:08.610961 
[Epoch 3/30] [Batch 200/516] [D loss: 0.004219, acc: 100%] [G loss: 9.640464, adv: 0.970834, cyc: 0.355352, id: 0.336399] time: 0:16:17.516941 
[Epoch 3/30] [Batch 300/516] [D loss: 0.003786, acc: 100%] [G loss: 9.608819, adv: 0.988915, cyc: 0.351638, id: 0.332575] time: 0:17:26.416254 
[Epoch 3/30] [Batch 400/516] [D loss: 0.006689, acc: 100%] [G loss: 9.592131, adv: 0.962056, cyc: 0.352467, id: 0.346336] time: 0:18:35.132716 
[Epoch 3/30] [Batch 500/516] [D loss: 0.004711, acc: 100%] [G loss: 9.335915, adv: 0.927145, cyc: 0.344389, id: 0.325837] time: 0:19:43.889013 
[Epoch 4/30] [Batch 0/516] [D loss: 0.006988, acc: 100%] [G loss: 9.241604, adv: 0.930546, cyc: 0.340465, id: 0.320000] time: 0:19:54.880834 
[Epoch 4/30] [Batch 100/516] [D loss: 0.003191, acc: 100%] [G loss: 9.179055, adv: 0.934756, cyc: 0.336418, id: 0.312772] time: 0:21:03.580412 
[Epoch 4/30] [Batch 200/516] [D loss: 0.002069, acc: 100%] [G loss: 9.002758, adv: 0.893508, cyc: 0.332370, id: 0.326639] time: 0:22:12.441644 
[Epoch 4/30] [Batch 300/516] [D loss: 0.001977, acc: 100%] [G loss: 9.072623, adv: 0.901463, cyc: 0.335037, id: 0.317426] time: 0:23:21.468265 
[Epoch 4/30] [Batch 400/516] [D loss: 0.002539, acc: 100%] [G loss: 9.093891, adv: 0.906909, cyc: 0.334932, id: 0.327714] time: 0:24:30.453936 
[Epoch 4/30] [Batch 500/516] [D loss: 0.003416, acc: 100%] [G loss: 9.145824, adv: 1.020465, cyc: 0.327135, id: 0.311595] time: 0:25:39.315650 
[Epoch 5/30] [Batch 0/516] [D loss: 0.002052, acc: 100%] [G loss: 8.969590, adv: 0.983749, cyc: 0.322784, id: 0.310001] time: 0:25:50.337697 
[Epoch 5/30] [Batch 100/516] [D loss: 0.004069, acc: 100%] [G loss: 8.837875, adv: 0.932984, cyc: 0.320970, id: 0.300177] time: 0:26:59.224907 
[Epoch 5/30] [Batch 200/516] [D loss: 0.002490, acc: 100%] [G loss: 8.759386, adv: 0.917901, cyc: 0.319047, id: 0.316078] time: 0:28:08.108892 
[Epoch 5/30] [Batch 300/516] [D loss: 0.001531, acc: 100%] [G loss: 8.941888, adv: 0.994708, cyc: 0.320062, id: 0.309544] time: 0:29:17.010140 
[Epoch 5/30] [Batch 400/516] [D loss: 0.001963, acc: 100%] [G loss: 9.059930, adv: 0.990375, cyc: 0.325284, id: 0.322040] time: 0:30:25.916579 
[Epoch 5/30] [Batch 500/516] [D loss: 0.003302, acc: 100%] [G loss: 8.932055, adv: 1.027756, cyc: 0.316026, id: 0.303375] time: 0:31:34.761763 
[Epoch 6/30] [Batch 0/516] [D loss: 0.003050, acc: 100%] [G loss: 8.754758, adv: 1.000103, cyc: 0.311313, id: 0.300759] time: 0:31:51.728893 
[Epoch 6/30] [Batch 100/516] [D loss: 0.003193, acc: 100%] [G loss: 8.739732, adv: 0.975550, cyc: 0.312341, id: 0.298902] time: 0:33:00.624599 
[Epoch 6/30] [Batch 200/516] [D loss: 0.001593, acc: 100%] [G loss: 8.732044, adv: 0.997567, cyc: 0.310273, id: 0.310878] time: 0:34:09.512445 
[Epoch 6/30] [Batch 300/516] [D loss: 0.001365, acc: 100%] [G loss: 8.616084, adv: 0.924333, cyc: 0.311515, id: 0.302152] time: 0:35:18.387387 
[Epoch 6/30] [Batch 400/516] [D loss: 0.001897, acc: 100%] [G loss: 8.712721, adv: 0.909926, cyc: 0.316829, id: 0.320385] time: 0:36:27.272582 
[Epoch 6/30] [Batch 500/516] [D loss: 0.002070, acc: 100%] [G loss: 8.561793, adv: 0.960946, cyc: 0.305348, id: 0.298996] time: 0:37:36.163714 
[Epoch 7/30] [Batch 0/516] [D loss: 0.003820, acc: 100%] [G loss: 8.436705, adv: 0.908151, cyc: 0.304908, id: 0.300659] time: 0:37:47.196171 
[Epoch 7/30] [Batch 100/516] [D loss: 0.001799, acc: 100%] [G loss: 8.595181, adv: 0.990370, cyc: 0.304196, id: 0.293553] time: 0:38:56.085196 
[Epoch 7/30] [Batch 200/516] [D loss: 0.000406, acc: 100%] [G loss: 8.567961, adv: 0.982923, cyc: 0.303805, id: 0.311425] time: 0:40:04.975389 
[Epoch 7/30] [Batch 300/516] [D loss: 0.001708, acc: 100%] [G loss: 8.650896, adv: 1.000527, cyc: 0.305996, id: 0.299755] time: 0:41:13.859500 
[Epoch 7/30] [Batch 400/516] [D loss: 0.001259, acc: 100%] [G loss: 8.718495, adv: 1.011708, cyc: 0.307549, id: 0.315280] time: 0:42:22.559869 
[Epoch 7/30] [Batch 500/516] [D loss: 0.001939, acc: 100%] [G loss: 8.363942, adv: 0.928308, cyc: 0.299205, id: 0.296259] time: 0:43:31.321001 
[Epoch 8/30] [Batch 0/516] [D loss: 0.003508, acc: 100%] [G loss: 8.377131, adv: 0.952051, cyc: 0.297961, id: 0.296230] time: 0:43:42.313819 
[Epoch 8/30] [Batch 100/516] [D loss: 0.001853, acc: 100%] [G loss: 8.376329, adv: 0.940212, cyc: 0.298929, id: 0.284420] time: 0:44:51.019641 
[Epoch 8/30] [Batch 200/516] [D loss: 0.000893, acc: 100%] [G loss: 8.394243, adv: 0.963692, cyc: 0.297653, id: 0.303952] time: 0:45:59.874120 
[Epoch 8/30] [Batch 300/516] [D loss: 0.001034, acc: 100%] [G loss: 8.347899, adv: 0.927535, cyc: 0.298416, id: 0.298240] time: 0:47:08.923636 
[Epoch 8/30] [Batch 400/516] [D loss: 0.007734, acc: 100%] [G loss: 8.663196, adv: 1.040106, cyc: 0.302070, id: 0.316249] time: 0:48:17.893617 
[Epoch 8/30] [Batch 500/516] [D loss: 0.001604, acc: 100%] [G loss: 8.352669, adv: 0.969261, cyc: 0.294933, id: 0.291628] time: 0:49:26.747804 
[Epoch 9/30] [Batch 0/516] [D loss: 0.000773, acc: 100%] [G loss: 8.178119, adv: 0.922301, cyc: 0.291431, id: 0.293091] time: 0:49:37.772408 
[Epoch 9/30] [Batch 100/516] [D loss: 0.001327, acc: 100%] [G loss: 8.351418, adv: 0.995975, cyc: 0.292421, id: 0.286674] time: 0:50:46.644708 
[Epoch 9/30] [Batch 200/516] [D loss: 0.001116, acc: 100%] [G loss: 8.346257, adv: 0.980429, cyc: 0.293692, id: 0.304833] time: 0:51:55.534852 
[Epoch 9/30] [Batch 300/516] [D loss: 0.001608, acc: 100%] [G loss: 8.454823, adv: 1.030661, cyc: 0.293896, id: 0.298022] time: 0:53:04.430244 
[Epoch 9/30] [Batch 400/516] [D loss: 0.001007, acc: 100%] [G loss: 8.411264, adv: 0.967818, cyc: 0.297179, id: 0.310818] time: 0:54:13.318357 
[Epoch 9/30] [Batch 500/516] [D loss: 0.000615, acc: 100%] [G loss: 8.261517, adv: 0.955698, cyc: 0.291542, id: 0.292722] time: 0:55:22.216295 
[Epoch 10/30] [Batch 0/516] [D loss: 0.001581, acc: 100%] [G loss: 8.243609, adv: 1.001831, cyc: 0.287055, id: 0.291378] time: 0:55:33.217289 
[Epoch 10/30] [Batch 100/516] [D loss: 0.000328, acc: 100%] [G loss: 8.188389, adv: 0.971183, cyc: 0.286842, id: 0.287492] time: 0:56:42.114063 
[Epoch 10/30] [Batch 200/516] [D loss: 0.000548, acc: 100%] [G loss: 8.255304, adv: 0.999731, cyc: 0.287387, id: 0.306337] time: 0:57:51.001466 
[Epoch 10/30] [Batch 300/516] [D loss: 0.000203, acc: 100%] [G loss: 8.243670, adv: 0.970901, cyc: 0.289466, id: 0.296591] time: 0:58:59.896303 
[Epoch 10/30] [Batch 400/516] [D loss: 0.000652, acc: 100%] [G loss: 8.318007, adv: 0.968218, cyc: 0.292867, id: 0.306611] time: 1:00:08.791846 
[Epoch 10/30] [Batch 500/516] [D loss: 0.000697, acc: 100%] [G loss: 8.187563, adv: 0.991980, cyc: 0.284749, id: 0.290279] time: 1:01:17.679028 
[Epoch 11/30] [Batch 0/516] [D loss: 0.000467, acc: 100%] [G loss: 8.061945, adv: 0.945034, cyc: 0.283770, id: 0.290739] time: 1:01:29.810810 
[Epoch 11/30] [Batch 100/516] [D loss: 0.000940, acc: 100%] [G loss: 8.033678, adv: 0.935490, cyc: 0.283179, id: 0.278781] time: 1:02:38.619116 
[Epoch 11/30] [Batch 200/516] [D loss: 0.000852, acc: 100%] [G loss: 8.101072, adv: 0.959197, cyc: 0.283915, id: 0.303922] time: 1:03:47.378436 
[Epoch 11/30] [Batch 300/516] [D loss: 0.000794, acc: 100%] [G loss: 8.076289, adv: 0.926043, cyc: 0.286089, id: 0.288766] time: 1:04:56.094159 
[Epoch 11/30] [Batch 400/516] [D loss: 0.001626, acc: 100%] [G loss: 8.291513, adv: 0.980672, cyc: 0.290268, id: 0.308709] time: 1:06:04.777688 
[Epoch 11/30] [Batch 500/516] [D loss: 0.000533, acc: 100%] [G loss: 8.011910, adv: 0.941989, cyc: 0.281382, id: 0.287068] time: 1:07:13.789296 
[Epoch 12/30] [Batch 0/516] [D loss: 0.000759, acc: 100%] [G loss: 8.118154, adv: 1.002593, cyc: 0.281127, id: 0.284863] time: 1:07:24.807180 
[Epoch 12/30] [Batch 100/516] [D loss: 0.000391, acc: 100%] [G loss: 8.096439, adv: 0.970823, cyc: 0.282629, id: 0.285200] time: 1:08:33.653500 
[Epoch 12/30] [Batch 200/516] [D loss: 0.000792, acc: 100%] [G loss: 8.024456, adv: 0.956725, cyc: 0.280903, id: 0.292762] time: 1:09:42.635304 
[Epoch 12/30] [Batch 300/516] [D loss: 0.001596, acc: 100%] [G loss: 8.248833, adv: 1.030761, cyc: 0.284250, id: 0.287894] time: 1:10:51.448978 
[Epoch 12/30] [Batch 400/516] [D loss: 0.001185, acc: 100%] [G loss: 8.235335, adv: 1.000667, cyc: 0.285997, id: 0.297992] time: 1:12:00.202010 
[Epoch 12/30] [Batch 500/516] [D loss: 0.002387, acc: 100%] [G loss: 7.971101, adv: 0.957656, cyc: 0.277975, id: 0.282379] time: 1:13:08.980461 
[Epoch 13/30] [Batch 0/516] [D loss: 0.000765, acc: 100%] [G loss: 7.936980, adv: 0.964880, cyc: 0.276277, id: 0.281765] time: 1:13:19.978173 
[Epoch 13/30] [Batch 100/516] [D loss: 0.001002, acc: 100%] [G loss: 7.911647, adv: 0.935930, cyc: 0.277324, id: 0.279040] time: 1:14:28.760947 
[Epoch 13/30] [Batch 200/516] [D loss: 0.000485, acc: 100%] [G loss: 8.032943, adv: 0.989375, cyc: 0.278184, id: 0.294927] time: 1:15:37.550472 
[Epoch 13/30] [Batch 300/516] [D loss: 0.000578, acc: 100%] [G loss: 8.092268, adv: 1.004670, cyc: 0.278902, id: 0.294664] time: 1:16:46.354001 
[Epoch 13/30] [Batch 400/516] [D loss: 0.001066, acc: 100%] [G loss: 8.022455, adv: 0.929146, cyc: 0.282931, id: 0.294819] time: 1:17:55.168941 
[Epoch 13/30] [Batch 500/516] [D loss: 0.000647, acc: 100%] [G loss: 7.905498, adv: 0.942820, cyc: 0.276119, id: 0.286162] time: 1:19:03.961380 
[Epoch 14/30] [Batch 0/516] [D loss: 0.000585, acc: 100%] [G loss: 8.007249, adv: 1.013753, cyc: 0.274747, id: 0.285528] time: 1:19:14.945195 
[Epoch 14/30] [Batch 100/516] [D loss: 0.002762, acc: 100%] [G loss: 7.985055, adv: 0.996862, cyc: 0.274640, id: 0.284418] time: 1:20:23.747060 
[Epoch 14/30] [Batch 200/516] [D loss: 0.000320, acc: 100%] [G loss: 7.922313, adv: 0.955659, cyc: 0.275889, id: 0.294500] time: 1:21:32.547283 
[Epoch 14/30] [Batch 300/516] [D loss: 0.000485, acc: 100%] [G loss: 7.931488, adv: 0.947389, cyc: 0.276758, id: 0.289077] time: 1:22:41.342932 
[Epoch 14/30] [Batch 400/516] [D loss: 0.000273, acc: 100%] [G loss: 8.082262, adv: 0.966106, cyc: 0.281881, id: 0.301608] time: 1:23:50.119900 
[Epoch 14/30] [Batch 500/516] [D loss: 0.000246, acc: 100%] [G loss: 7.888971, adv: 0.970212, cyc: 0.272750, id: 0.277517] time: 1:24:58.910473 
[Epoch 15/30] [Batch 0/516] [D loss: 0.000433, acc: 100%] [G loss: 7.797812, adv: 0.944653, cyc: 0.271335, id: 0.282633] time: 1:25:09.886962 
[Epoch 15/30] [Batch 100/516] [D loss: 0.000583, acc: 100%] [G loss: 7.871923, adv: 0.969836, cyc: 0.272231, id: 0.275067] time: 1:26:18.664697 
[Epoch 15/30] [Batch 200/516] [D loss: 0.000159, acc: 100%] [G loss: 7.890163, adv: 0.987947, cyc: 0.271165, id: 0.294946] time: 1:27:27.283035 
[Epoch 15/30] [Batch 300/516] [D loss: 0.000368, acc: 100%] [G loss: 7.874150, adv: 0.936558, cyc: 0.275341, id: 0.285208] time: 1:28:35.960919 
[Epoch 15/30] [Batch 400/516] [D loss: 0.000213, acc: 100%] [G loss: 7.945855, adv: 0.963510, cyc: 0.275793, id: 0.293672] time: 1:29:44.570330 
[Epoch 15/30] [Batch 500/516] [D loss: 0.000472, acc: 100%] [G loss: 7.834753, adv: 0.962899, cyc: 0.270858, id: 0.282803] time: 1:30:53.210527 
[Epoch 16/30] [Batch 0/516] [D loss: 0.000219, acc: 100%] [G loss: 7.810072, adv: 0.980094, cyc: 0.268499, id: 0.281623] time: 1:31:05.524867 
[Epoch 16/30] [Batch 100/516] [D loss: 0.000661, acc: 100%] [G loss: 7.835402, adv: 0.964600, cyc: 0.270912, id: 0.273650] time: 1:32:14.441794 
[Epoch 16/30] [Batch 200/516] [D loss: 0.000167, acc: 100%] [G loss: 7.776984, adv: 0.955050, cyc: 0.268921, id: 0.292514] time: 1:33:23.303276 
[Epoch 16/30] [Batch 300/516] [D loss: 0.000409, acc: 100%] [G loss: 7.883506, adv: 0.959927, cyc: 0.273472, id: 0.285573] time: 1:34:32.056896 
[Epoch 16/30] [Batch 400/516] [D loss: 0.000337, acc: 100%] [G loss: 7.954278, adv: 0.981475, cyc: 0.274342, id: 0.295109] time: 1:35:40.835654 
[Epoch 16/30] [Batch 500/516] [D loss: 0.000375, acc: 100%] [G loss: 7.804997, adv: 0.962649, cyc: 0.269559, id: 0.274345] time: 1:36:49.625109 
[Epoch 17/30] [Batch 0/516] [D loss: 0.000425, acc: 100%] [G loss: 7.785283, adv: 0.977913, cyc: 0.267809, id: 0.275266] time: 1:37:00.610876 
[Epoch 17/30] [Batch 100/516] [D loss: 0.000951, acc: 100%] [G loss: 7.811877, adv: 0.977689, cyc: 0.268251, id: 0.279735] time: 1:38:09.394053 
[Epoch 17/30] [Batch 200/516] [D loss: 0.000183, acc: 100%] [G loss: 7.812061, adv: 0.965088, cyc: 0.269452, id: 0.299429] time: 1:39:18.189372 
[Epoch 17/30] [Batch 300/516] [D loss: 0.000699, acc: 100%] [G loss: 7.852735, adv: 0.969542, cyc: 0.270857, id: 0.283664] time: 1:40:26.973708 
[Epoch 17/30] [Batch 400/516] [D loss: 0.000338, acc: 100%] [G loss: 7.939955, adv: 0.977178, cyc: 0.273958, id: 0.299001] time: 1:41:35.766949 
[Epoch 17/30] [Batch 500/516] [D loss: 0.000542, acc: 100%] [G loss: 7.746365, adv: 0.960253, cyc: 0.267097, id: 0.276269] time: 1:42:44.555365 
[Epoch 18/30] [Batch 0/516] [D loss: 0.000140, acc: 100%] [G loss: 7.683842, adv: 0.945577, cyc: 0.265353, id: 0.291590] time: 1:42:55.541160 
[Epoch 18/30] [Batch 100/516] [D loss: 0.000228, acc: 100%] [G loss: 7.753601, adv: 0.969455, cyc: 0.266341, id: 0.274076] time: 1:44:04.327344 
[Epoch 18/30] [Batch 200/516] [D loss: 0.000140, acc: 100%] [G loss: 7.750946, adv: 0.964073, cyc: 0.266738, id: 0.292340] time: 1:45:13.119799 
[Epoch 18/30] [Batch 300/516] [D loss: 0.000431, acc: 100%] [G loss: 7.763118, adv: 0.953617, cyc: 0.268421, id: 0.281787] time: 1:46:21.917066 
[Epoch 18/30] [Batch 400/516] [D loss: 0.000372, acc: 100%] [G loss: 7.868337, adv: 0.981582, cyc: 0.269999, id: 0.297971] time: 1:47:30.642702 
[Epoch 18/30] [Batch 500/516] [D loss: 0.000519, acc: 100%] [G loss: 7.743018, adv: 1.004956, cyc: 0.262417, id: 0.277664] time: 1:48:39.310735 
[Epoch 19/30] [Batch 0/516] [D loss: 0.000677, acc: 100%] [G loss: 7.753009, adv: 1.012010, cyc: 0.262562, id: 0.280819] time: 1:48:50.285375 
[Epoch 19/30] [Batch 100/516] [D loss: 0.000347, acc: 100%] [G loss: 7.709864, adv: 0.972265, cyc: 0.263951, id: 0.275209] time: 1:49:58.924147 
[Epoch 19/30] [Batch 200/516] [D loss: 0.000453, acc: 100%] [G loss: 7.712134, adv: 0.980802, cyc: 0.263536, id: 0.285685] time: 1:51:07.546847 
[Epoch 19/30] [Batch 300/516] [D loss: 0.000447, acc: 100%] [G loss: 7.803464, adv: 0.980298, cyc: 0.267548, id: 0.284957] time: 1:52:16.459387 
[Epoch 19/30] [Batch 400/516] [D loss: 0.000551, acc: 100%] [G loss: 7.765618, adv: 0.924421, cyc: 0.270617, id: 0.298610] time: 1:53:25.243993 
[Epoch 19/30] [Batch 500/516] [D loss: 0.000745, acc: 100%] [G loss: 7.625762, adv: 0.940605, cyc: 0.263370, id: 0.264933] time: 1:54:34.144517 
[Epoch 20/30] [Batch 0/516] [D loss: 0.000367, acc: 100%] [G loss: 7.686603, adv: 1.012480, cyc: 0.258858, id: 0.286936] time: 1:54:45.130347 
[Epoch 20/30] [Batch 100/516] [D loss: 0.000488, acc: 100%] [G loss: 7.632672, adv: 0.952464, cyc: 0.262053, id: 0.271410] time: 1:55:53.918801 
[Epoch 20/30] [Batch 200/516] [D loss: 0.000159, acc: 100%] [G loss: 7.694924, adv: 0.990376, cyc: 0.261394, id: 0.291037] time: 1:57:02.714044 
[Epoch 20/30] [Batch 300/516] [D loss: 0.000190, acc: 100%] [G loss: 7.711432, adv: 0.971362, cyc: 0.263705, id: 0.291095] time: 1:58:11.456118 
[Epoch 20/30] [Batch 400/516] [D loss: 0.001527, acc: 100%] [G loss: 7.752696, adv: 0.967584, cyc: 0.266234, id: 0.291806] time: 1:59:20.190674 
[Epoch 20/30] [Batch 500/516] [D loss: 0.000265, acc: 100%] [G loss: 7.610388, adv: 0.959533, cyc: 0.259954, id: 0.284450] time: 2:00:28.977452 
[Epoch 21/30] [Batch 0/516] [D loss: 0.000359, acc: 100%] [G loss: 7.567045, adv: 0.940317, cyc: 0.260727, id: 0.279752] time: 2:00:41.210982 
[Epoch 21/30] [Batch 100/516] [D loss: 0.000479, acc: 100%] [G loss: 7.638974, adv: 0.972181, cyc: 0.260357, id: 0.274712] time: 2:01:49.990055 
[Epoch 21/30] [Batch 200/516] [D loss: 0.000377, acc: 100%] [G loss: 7.641158, adv: 0.986293, cyc: 0.259181, id: 0.295480] time: 2:02:58.777985 
[Epoch 21/30] [Batch 300/516] [D loss: 0.000158, acc: 100%] [G loss: 7.788156, adv: 0.994524, cyc: 0.265017, id: 0.293036] time: 2:04:07.574242 
[Epoch 21/30] [Batch 400/516] [D loss: 0.000271, acc: 100%] [G loss: 7.773518, adv: 0.956396, cyc: 0.268034, id: 0.294171] time: 2:05:16.382375 
[Epoch 21/30] [Batch 500/516] [D loss: 0.000168, acc: 100%] [G loss: 7.626269, adv: 0.962609, cyc: 0.260654, id: 0.276551] time: 2:06:25.214107 
[Epoch 22/30] [Batch 0/516] [D loss: 0.000426, acc: 100%] [G loss: 7.544466, adv: 0.955360, cyc: 0.258124, id: 0.277901] time: 2:06:36.212709 
[Epoch 22/30] [Batch 100/516] [D loss: 0.000539, acc: 100%] [G loss: 7.593812, adv: 0.960930, cyc: 0.259534, id: 0.267158] time: 2:07:44.833784 
[Epoch 22/30] [Batch 200/516] [D loss: 0.000650, acc: 100%] [G loss: 7.604200, adv: 0.984049, cyc: 0.257701, id: 0.294439] time: 2:08:53.508817 
[Epoch 22/30] [Batch 300/516] [D loss: 0.000461, acc: 100%] [G loss: 7.603309, adv: 0.942313, cyc: 0.261675, id: 0.272831] time: 2:10:02.137786 
[Epoch 22/30] [Batch 400/516] [D loss: 0.000214, acc: 100%] [G loss: 7.719992, adv: 0.974525, cyc: 0.263417, id: 0.301167] time: 2:11:10.785960 
[Epoch 22/30] [Batch 500/516] [D loss: 0.000373, acc: 100%] [G loss: 7.655951, adv: 0.989665, cyc: 0.259261, id: 0.285034] time: 2:12:19.794738 
[Epoch 23/30] [Batch 0/516] [D loss: 0.000154, acc: 100%] [G loss: 7.481745, adv: 0.950209, cyc: 0.255656, id: 0.271424] time: 2:12:30.779880 
[Epoch 23/30] [Batch 100/516] [D loss: 0.000177, acc: 100%] [G loss: 7.603773, adv: 0.988980, cyc: 0.257139, id: 0.273843] time: 2:13:39.602499 
[Epoch 23/30] [Batch 200/516] [D loss: 0.000531, acc: 100%] [G loss: 7.597895, adv: 0.989765, cyc: 0.256365, id: 0.303978] time: 2:14:48.393109 
[Epoch 23/30] [Batch 300/516] [D loss: 0.000111, acc: 100%] [G loss: 7.590415, adv: 0.949935, cyc: 0.259903, id: 0.286872] time: 2:15:57.186095 
[Epoch 23/30] [Batch 400/516] [D loss: 0.000442, acc: 100%] [G loss: 7.808468, adv: 1.020820, cyc: 0.263396, id: 0.286177] time: 2:17:05.985225 
[Epoch 23/30] [Batch 500/516] [D loss: 0.000205, acc: 100%] [G loss: 7.546297, adv: 0.952869, cyc: 0.257570, id: 0.280207] time: 2:18:14.779425 
[Epoch 24/30] [Batch 0/516] [D loss: 0.000187, acc: 100%] [G loss: 7.516488, adv: 0.991712, cyc: 0.253354, id: 0.272734] time: 2:18:25.759804 
[Epoch 24/30] [Batch 100/516] [D loss: 0.000177, acc: 100%] [G loss: 7.471874, adv: 0.934536, cyc: 0.256038, id: 0.269799] time: 2:19:34.553871 
[Epoch 24/30] [Batch 200/516] [D loss: 0.000196, acc: 100%] [G loss: 7.479830, adv: 0.939528, cyc: 0.256088, id: 0.289376] time: 2:20:43.346765 
[Epoch 24/30] [Batch 300/516] [D loss: 0.000139, acc: 100%] [G loss: 7.579022, adv: 0.964540, cyc: 0.257795, id: 0.287703] time: 2:21:52.136365 
[Epoch 24/30] [Batch 400/516] [D loss: 0.000299, acc: 100%] [G loss: 7.627087, adv: 0.923039, cyc: 0.263793, id: 0.288168] time: 2:23:00.920157 
[Epoch 24/30] [Batch 500/516] [D loss: 0.000309, acc: 100%] [G loss: 7.498503, adv: 0.940159, cyc: 0.256550, id: 0.275519] time: 2:24:09.698308 
[Epoch 25/30] [Batch 0/516] [D loss: 0.000154, acc: 100%] [G loss: 7.522738, adv: 0.981680, cyc: 0.254219, id: 0.271567] time: 2:24:20.684985 
[Epoch 25/30] [Batch 100/516] [D loss: 0.000135, acc: 100%] [G loss: 7.541608, adv: 0.977838, cyc: 0.255207, id: 0.268544] time: 2:25:29.466968 
[Epoch 25/30] [Batch 200/516] [D loss: 0.000041, acc: 100%] [G loss: 7.533788, adv: 0.971138, cyc: 0.255735, id: 0.278374] time: 2:26:38.241005 
[Epoch 25/30] [Batch 300/516] [D loss: 0.000144, acc: 100%] [G loss: 7.578799, adv: 0.978531, cyc: 0.256744, id: 0.276869] time: 2:27:46.862199 
[Epoch 25/30] [Batch 400/516] [D loss: 0.000059, acc: 100%] [G loss: 7.643150, adv: 0.972882, cyc: 0.260075, id: 0.285876] time: 2:28:55.513543 
[Epoch 25/30] [Batch 500/516] [D loss: 0.000109, acc: 100%] [G loss: 7.527608, adv: 0.971194, cyc: 0.254774, id: 0.275016] time: 2:30:04.129353 
[Epoch 26/30] [Batch 0/516] [D loss: 0.000148, acc: 100%] [G loss: 7.369719, adv: 0.934384, cyc: 0.251053, id: 0.286709] time: 2:30:16.194361 
[Epoch 26/30] [Batch 100/516] [D loss: 0.004499, acc:  99%] [G loss: 7.457703, adv: 0.952546, cyc: 0.253828, id: 0.263878] time: 2:31:25.018170 
[Epoch 26/30] [Batch 200/516] [D loss: 0.000090, acc: 100%] [G loss: 7.420557, adv: 0.921425, cyc: 0.254587, id: 0.282521] time: 2:32:33.850779 
[Epoch 26/30] [Batch 300/516] [D loss: 0.000121, acc: 100%] [G loss: 7.506321, adv: 0.955273, cyc: 0.255368, id: 0.272930] time: 2:33:42.756915 
[Epoch 26/30] [Batch 400/516] [D loss: 0.000207, acc: 100%] [G loss: 7.654218, adv: 0.958910, cyc: 0.261469, id: 0.298536] time: 2:34:51.483469 
[Epoch 26/30] [Batch 500/516] [D loss: 0.000226, acc: 100%] [G loss: 7.403855, adv: 0.929325, cyc: 0.253246, id: 0.276620] time: 2:36:00.269557 
[Epoch 27/30] [Batch 0/516] [D loss: 0.000135, acc: 100%] [G loss: 7.370517, adv: 0.933777, cyc: 0.251483, id: 0.272508] time: 2:36:11.271205 
[Epoch 27/30] [Batch 100/516] [D loss: 0.000317, acc: 100%] [G loss: 7.466906, adv: 0.975892, cyc: 0.251646, id: 0.278610] time: 2:37:20.049652 
[Epoch 27/30] [Batch 200/516] [D loss: 0.000115, acc: 100%] [G loss: 7.466392, adv: 0.979482, cyc: 0.251765, id: 0.283716] time: 2:38:28.831371 
[Epoch 27/30] [Batch 300/516] [D loss: 0.000166, acc: 100%] [G loss: 7.510983, adv: 0.951254, cyc: 0.256449, id: 0.273933] time: 2:39:37.602160 
[Epoch 27/30] [Batch 400/516] [D loss: 0.000256, acc: 100%] [G loss: 7.448225, adv: 0.872186, cyc: 0.259911, id: 0.300984] time: 2:40:46.389069 
[Epoch 27/30] [Batch 500/516] [D loss: 0.000185, acc: 100%] [G loss: 7.453045, adv: 0.965727, cyc: 0.251984, id: 0.271992] time: 2:41:55.172363 
[Epoch 28/30] [Batch 0/516] [D loss: 0.000190, acc: 100%] [G loss: 7.331897, adv: 0.943052, cyc: 0.248557, id: 0.273273] time: 2:42:06.175137 
[Epoch 28/30] [Batch 100/516] [D loss: 0.000203, acc: 100%] [G loss: 7.438062, adv: 0.991772, cyc: 0.248868, id: 0.266223] time: 2:43:14.965026 
[Epoch 28/30] [Batch 200/516] [D loss: 0.000068, acc: 100%] [G loss: 7.504634, adv: 0.995817, cyc: 0.251339, id: 0.295091] time: 2:44:23.777809 
[Epoch 28/30] [Batch 300/516] [D loss: 0.000064, acc: 100%] [G loss: 7.509051, adv: 0.976067, cyc: 0.253860, id: 0.272912] time: 2:45:32.600456 
[Epoch 28/30] [Batch 400/516] [D loss: 0.000338, acc: 100%] [G loss: 7.703425, adv: 1.004812, cyc: 0.259429, id: 0.291285] time: 2:46:41.246383 
[Epoch 28/30] [Batch 500/516] [D loss: 0.000262, acc: 100%] [G loss: 7.355344, adv: 0.935157, cyc: 0.250100, id: 0.267532] time: 2:47:49.910612 
[Epoch 29/30] [Batch 0/516] [D loss: 0.000144, acc: 100%] [G loss: 7.433994, adv: 0.984521, cyc: 0.248735, id: 0.294624] time: 2:48:00.908892 
[Epoch 29/30] [Batch 100/516] [D loss: 0.000087, acc: 100%] [G loss: 7.420679, adv: 0.959303, cyc: 0.250794, id: 0.270273] time: 2:49:09.528200 
[Epoch 29/30] [Batch 200/516] [D loss: 0.000240, acc: 100%] [G loss: 7.392698, adv: 0.924939, cyc: 0.253308, id: 0.291081] time: 2:50:18.154190 
[Epoch 29/30] [Batch 300/516] [D loss: 0.000145, acc: 100%] [G loss: 7.481534, adv: 0.992354, cyc: 0.250927, id: 0.279806] time: 2:51:27.112791 
[Epoch 29/30] [Batch 400/516] [D loss: 0.000060, acc: 100%] [G loss: 7.548836, adv: 0.976290, cyc: 0.255092, id: 0.294729] time: 2:52:35.863923 
[Epoch 29/30] [Batch 500/516] [D loss: 0.000114, acc: 100%] [G loss: 7.435453, adv: 0.983905, cyc: 0.249371, id: 0.267435] time: 2:53:44.753514 
[Epoch 30/30] [Batch 0/516] [D loss: 0.000242, acc: 100%] [G loss: 7.215390, adv: 0.912550, cyc: 0.246555, id: 0.263147] time: 2:53:55.740663 
[Epoch 30/30] [Batch 100/516] [D loss: 0.000320, acc: 100%] [G loss: 7.480192, adv: 1.012822, cyc: 0.248309, id: 0.275743] time: 2:55:04.551472 
[Epoch 30/30] [Batch 200/516] [D loss: 0.000050, acc: 100%] [G loss: 7.382656, adv: 0.973043, cyc: 0.248512, id: 0.278718] time: 2:56:13.410853 
[Epoch 30/30] [Batch 300/516] [D loss: 0.000247, acc: 100%] [G loss: 7.526979, adv: 0.998079, cyc: 0.252803, id: 0.269778] time: 2:57:22.167482 
[Epoch 30/30] [Batch 400/516] [D loss: 0.000061, acc: 100%] [G loss: 7.620615, adv: 0.996699, cyc: 0.256813, id: 0.285909] time: 2:58:30.914444 
[Epoch 30/30] [Batch 500/516] [D loss: 0.000251, acc: 100%] [G loss: 7.339242, adv: 0.955287, cyc: 0.248088, id: 0.263387] time: 2:59:39.687931 
Train Finished.
