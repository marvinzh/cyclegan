_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         (None, 1, 600, 1)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 1, 600, 64)        256       
_________________________________________________________________
leaky_re_lu_5 (LeakyReLU)    (None, 1, 600, 64)        0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 1, 600, 128)       24704     
_________________________________________________________________
leaky_re_lu_6 (LeakyReLU)    (None, 1, 600, 128)       0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 76800)             0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               39322112  
_________________________________________________________________
leaky_re_lu_7 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 512)               262656    
_________________________________________________________________
leaky_re_lu_8 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
D_A_Out (Dense)              (None, 1)                 513       
=================================================================
Total params: 79,220,482
Trainable params: 39,610,241
Non-trainable params: 39,610,241
_________________________________________________________________
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 1, 600, 1)         0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 1, 600, 64)        256       
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU)    (None, 1, 600, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 1, 600, 128)       24704     
_________________________________________________________________
leaky_re_lu_2 (LeakyReLU)    (None, 1, 600, 128)       0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 76800)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               39322112  
_________________________________________________________________
leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
D_B_Out (Dense)              (None, 1)                 513       
=================================================================
Total params: 79,220,482
Trainable params: 39,610,241
Non-trainable params: 39,610,241
_________________________________________________________________
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
ivec_b (InputLayer)             (None, 1, 600, 1)    0                                            
__________________________________________________________________________________________________
ivec_a (InputLayer)             (None, 1, 600, 1)    0                                            
__________________________________________________________________________________________________
model_4 (Model)                 (None, 1, 600, 1)    653345      ivec_b[0][0]                     
                                                                 model_3[1][0]                    
                                                                 ivec_a[0][0]                     
__________________________________________________________________________________________________
model_3 (Model)                 (None, 1, 600, 1)    653345      ivec_a[0][0]                     
                                                                 model_4[1][0]                    
                                                                 ivec_b[0][0]                     
__________________________________________________________________________________________________
model_2 (Model)                 (None, 1)            39610241    model_4[1][0]                    
__________________________________________________________________________________________________
model_1 (Model)                 (None, 1)            39610241    model_3[1][0]                    
==================================================================================================
Total params: 80,527,172
Trainable params: 1,306,690
Non-trainable params: 79,220,482
__________________________________________________________________________________________________
Start CycleGAN training.
[Epoch 1/30] [Batch 0/516] [D loss: 0.549334, acc:  50%] [G loss: 22.416098, adv: 1.050290, cyc: 0.924832, id: 0.940378] time: 0:02:00.991329 
[Epoch 1/30] [Batch 100/516] [D loss: 0.192354, acc:  75%] [G loss: 21.142626, adv: 0.383536, cyc: 0.927652, id: 0.943381] time: 0:03:07.501572 
[Epoch 1/30] [Batch 200/516] [D loss: 0.171039, acc:  92%] [G loss: 20.660698, adv: 0.296873, cyc: 0.913532, id: 0.936937] time: 0:04:14.014676 
[Epoch 1/30] [Batch 300/516] [D loss: 0.167361, acc:  92%] [G loss: 20.545126, adv: 0.284628, cyc: 0.909407, id: 0.923834] time: 0:05:20.515054 
[Epoch 1/30] [Batch 400/516] [D loss: 0.153462, acc:  96%] [G loss: 20.221432, adv: 0.282450, cyc: 0.894680, id: 0.906069] time: 0:06:27.008212 
[Epoch 1/30] [Batch 500/516] [D loss: 0.146024, acc:  95%] [G loss: 20.354563, adv: 0.279199, cyc: 0.901282, id: 0.929071] time: 0:07:33.504137 
[Epoch 2/30] [Batch 0/516] [D loss: 0.140276, acc:  97%] [G loss: 20.209415, adv: 0.280717, cyc: 0.894951, id: 0.906680] time: 0:07:44.138456 
[Epoch 2/30] [Batch 100/516] [D loss: 0.128971, acc:  98%] [G loss: 20.310144, adv: 0.287682, cyc: 0.899018, id: 0.907628] time: 0:08:50.627009 
[Epoch 2/30] [Batch 200/516] [D loss: 0.122468, acc:  98%] [G loss: 20.034277, adv: 0.289550, cyc: 0.886015, id: 0.907876] time: 0:09:57.122831 
[Epoch 2/30] [Batch 300/516] [D loss: 0.119808, acc:  98%] [G loss: 20.003559, adv: 0.286273, cyc: 0.885206, id: 0.900394] time: 0:11:03.613485 
[Epoch 2/30] [Batch 400/516] [D loss: 0.110312, acc:  99%] [G loss: 19.659008, adv: 0.297555, cyc: 0.868668, id: 0.874868] time: 0:12:10.103942 
[Epoch 2/30] [Batch 500/516] [D loss: 0.110301, acc:  99%] [G loss: 19.825983, adv: 0.305004, cyc: 0.876119, id: 0.895494] time: 0:13:16.589240 
[Epoch 3/30] [Batch 0/516] [D loss: 0.105304, acc:  99%] [G loss: 19.723553, adv: 0.297425, cyc: 0.871993, id: 0.882591] time: 0:13:27.224847 
[Epoch 3/30] [Batch 100/516] [D loss: 0.101838, acc:  99%] [G loss: 19.804144, adv: 0.317142, cyc: 0.874027, id: 0.879829] time: 0:14:33.719371 
[Epoch 3/30] [Batch 200/516] [D loss: 0.100865, acc:  99%] [G loss: 19.630398, adv: 0.333385, cyc: 0.864803, id: 0.882649] time: 0:15:40.219315 
[Epoch 3/30] [Batch 300/516] [D loss: 0.097674, acc:  99%] [G loss: 19.534988, adv: 0.352041, cyc: 0.859083, id: 0.863771] time: 0:16:46.704421 
[Epoch 3/30] [Batch 400/516] [D loss: 0.085290, acc:  99%] [G loss: 19.252577, adv: 0.364645, cyc: 0.844920, id: 0.847249] time: 0:17:53.188962 
[Epoch 3/30] [Batch 500/516] [D loss: 0.081193, acc:  99%] [G loss: 19.324268, adv: 0.379974, cyc: 0.846345, id: 0.871445] time: 0:18:59.685496 
[Epoch 4/30] [Batch 0/516] [D loss: 0.076211, acc: 100%] [G loss: 19.283503, adv: 0.393960, cyc: 0.843630, id: 0.855658] time: 0:19:10.322124 
[Epoch 4/30] [Batch 100/516] [D loss: 0.069828, acc:  99%] [G loss: 19.367918, adv: 0.417160, cyc: 0.845357, id: 0.858214] time: 0:20:16.816141 
[Epoch 4/30] [Batch 200/516] [D loss: 0.069165, acc:  99%] [G loss: 19.160341, adv: 0.428400, cyc: 0.835174, id: 0.852650] time: 0:21:23.309178 
[Epoch 4/30] [Batch 300/516] [D loss: 0.064891, acc:  99%] [G loss: 19.167591, adv: 0.461048, cyc: 0.832747, id: 0.839622] time: 0:22:29.798902 
[Epoch 4/30] [Batch 400/516] [D loss: 0.055046, acc:  99%] [G loss: 18.906843, adv: 0.472426, cyc: 0.819814, id: 0.822222] time: 0:23:36.287612 
[Epoch 4/30] [Batch 500/516] [D loss: 0.054289, acc:  99%] [G loss: 19.051182, adv: 0.492481, cyc: 0.824721, id: 0.844711] time: 0:24:42.791727 
[Epoch 5/30] [Batch 0/516] [D loss: 0.050314, acc: 100%] [G loss: 18.912851, adv: 0.504561, cyc: 0.817401, id: 0.828279] time: 0:24:53.430183 
[Epoch 5/30] [Batch 100/516] [D loss: 0.046469, acc:  99%] [G loss: 19.036736, adv: 0.520074, cyc: 0.821325, id: 0.834533] time: 0:25:59.894104 
[Epoch 5/30] [Batch 200/516] [D loss: 0.049270, acc:  99%] [G loss: 18.943277, adv: 0.548412, cyc: 0.814981, id: 0.831143] time: 0:27:06.373090 
[Epoch 5/30] [Batch 300/516] [D loss: 0.046615, acc:  99%] [G loss: 18.788498, adv: 0.558943, cyc: 0.806594, id: 0.822194] time: 0:28:12.849591 
[Epoch 5/30] [Batch 400/516] [D loss: 0.039301, acc: 100%] [G loss: 18.577623, adv: 0.584209, cyc: 0.794858, id: 0.802778] time: 0:29:19.331652 
[Epoch 5/30] [Batch 500/516] [D loss: 0.040770, acc:  99%] [G loss: 18.752857, adv: 0.597151, cyc: 0.801831, id: 0.823038] time: 0:30:25.823090 
[Epoch 6/30] [Batch 0/516] [D loss: 0.037206, acc: 100%] [G loss: 18.621017, adv: 0.593206, cyc: 0.796250, id: 0.810306] time: 0:30:42.674108 
[Epoch 6/30] [Batch 100/516] [D loss: 0.034921, acc: 100%] [G loss: 18.774483, adv: 0.619336, cyc: 0.801073, id: 0.809558] time: 0:31:49.164897 
[Epoch 6/30] [Batch 200/516] [D loss: 0.039085, acc:  99%] [G loss: 18.611006, adv: 0.636495, cyc: 0.791902, id: 0.813727] time: 0:32:55.650114 
[Epoch 6/30] [Batch 300/516] [D loss: 0.037410, acc:  99%] [G loss: 18.518295, adv: 0.636657, cyc: 0.787574, id: 0.801166] time: 0:34:02.147163 
[Epoch 6/30] [Batch 400/516] [D loss: 0.031873, acc: 100%] [G loss: 18.340618, adv: 0.659450, cyc: 0.777418, id: 0.789412] time: 0:35:08.640884 
[Epoch 6/30] [Batch 500/516] [D loss: 0.034841, acc:  99%] [G loss: 18.551788, adv: 0.674403, cyc: 0.785915, id: 0.808521] time: 0:36:15.121481 
[Epoch 7/30] [Batch 0/516] [D loss: 0.031350, acc: 100%] [G loss: 18.444241, adv: 0.688468, cyc: 0.779889, id: 0.790051] time: 0:36:25.758974 
[Epoch 7/30] [Batch 100/516] [D loss: 0.030322, acc: 100%] [G loss: 18.602278, adv: 0.699874, cyc: 0.785847, id: 0.795702] time: 0:37:32.250699 
[Epoch 7/30] [Batch 200/516] [D loss: 0.035418, acc:  99%] [G loss: 18.424593, adv: 0.701746, cyc: 0.777939, id: 0.796218] time: 0:38:38.737042 
[Epoch 7/30] [Batch 300/516] [D loss: 0.034846, acc:  99%] [G loss: 18.375402, adv: 0.724439, cyc: 0.773645, id: 0.781671] time: 0:39:45.231668 
[Epoch 7/30] [Batch 400/516] [D loss: 0.030372, acc: 100%] [G loss: 18.212410, adv: 0.734644, cyc: 0.765288, id: 0.769734] time: 0:40:51.713041 
[Epoch 7/30] [Batch 500/516] [D loss: 0.034669, acc:  99%] [G loss: 18.412840, adv: 0.736969, cyc: 0.774575, id: 0.790022] time: 0:41:58.191799 
[Epoch 8/30] [Batch 0/516] [D loss: 0.030950, acc: 100%] [G loss: 18.235458, adv: 0.750733, cyc: 0.764850, id: 0.774436] time: 0:42:08.826558 
[Epoch 8/30] [Batch 100/516] [D loss: 0.031356, acc: 100%] [G loss: 18.444353, adv: 0.747724, cyc: 0.774983, id: 0.778550] time: 0:43:15.297921 
[Epoch 8/30] [Batch 200/516] [D loss: 0.037586, acc:  98%] [G loss: 18.233831, adv: 0.746284, cyc: 0.765484, id: 0.780804] time: 0:44:21.838219 
[Epoch 8/30] [Batch 300/516] [D loss: 0.038529, acc:  98%] [G loss: 18.256798, adv: 0.769736, cyc: 0.764542, id: 0.767400] time: 0:45:28.671826 
[Epoch 8/30] [Batch 400/516] [D loss: 0.035013, acc: 100%] [G loss: 18.008739, adv: 0.781047, cyc: 0.752219, id: 0.750658] time: 0:46:35.458638 
[Epoch 8/30] [Batch 500/516] [D loss: 0.041047, acc:  99%] [G loss: 18.279676, adv: 0.797089, cyc: 0.763223, id: 0.776616] time: 0:47:42.244409 
[Epoch 9/30] [Batch 0/516] [D loss: 0.036851, acc:  99%] [G loss: 18.081694, adv: 0.780874, cyc: 0.755591, id: 0.760892] time: 0:47:52.929737 
[Epoch 9/30] [Batch 100/516] [D loss: 0.039046, acc:  99%] [G loss: 18.262329, adv: 0.788323, cyc: 0.763204, id: 0.764647] time: 0:48:59.669741 
[Epoch 9/30] [Batch 200/516] [D loss: 0.046250, acc:  99%] [G loss: 18.034735, adv: 0.791786, cyc: 0.752421, id: 0.763413] time: 0:50:06.395286 
[Epoch 9/30] [Batch 300/516] [D loss: 0.047797, acc:  98%] [G loss: 18.016382, adv: 0.802863, cyc: 0.750754, id: 0.751107] time: 0:51:13.129640 
[Epoch 9/30] [Batch 400/516] [D loss: 0.043677, acc:  99%] [G loss: 17.856899, adv: 0.809641, cyc: 0.742939, id: 0.739755] time: 0:52:19.855087 
[Epoch 9/30] [Batch 500/516] [D loss: 0.048822, acc:  98%] [G loss: 18.058111, adv: 0.831994, cyc: 0.749944, id: 0.761048] time: 0:53:26.585039 
[Epoch 10/30] [Batch 0/516] [D loss: 0.044029, acc:  99%] [G loss: 17.900375, adv: 0.839538, cyc: 0.742083, id: 0.745177] time: 0:53:37.253972 
[Epoch 10/30] [Batch 100/516] [D loss: 0.044282, acc:  99%] [G loss: 18.071009, adv: 0.839679, cyc: 0.749549, id: 0.754134] time: 0:54:43.967955 
[Epoch 10/30] [Batch 200/516] [D loss: 0.049062, acc:  98%] [G loss: 17.967350, adv: 0.869937, cyc: 0.742211, id: 0.753624] time: 0:55:50.664935 
[Epoch 10/30] [Batch 300/516] [D loss: 0.047318, acc:  98%] [G loss: 17.957859, adv: 0.897556, cyc: 0.739567, id: 0.737420] time: 0:56:57.354835 
[Epoch 10/30] [Batch 400/516] [D loss: 0.041133, acc:  99%] [G loss: 17.764236, adv: 0.891750, cyc: 0.731597, id: 0.720942] time: 0:58:04.052544 
[Epoch 10/30] [Batch 500/516] [D loss: 0.043704, acc:  98%] [G loss: 17.971951, adv: 0.932770, cyc: 0.736950, id: 0.747248] time: 0:59:10.736764 
[Epoch 11/30] [Batch 0/516] [D loss: 0.039005, acc:  99%] [G loss: 17.840189, adv: 0.925236, cyc: 0.731803, id: 0.732630] time: 0:59:23.530828 
[Epoch 11/30] [Batch 100/516] [D loss: 0.036563, acc:  99%] [G loss: 17.978981, adv: 0.948931, cyc: 0.735794, id: 0.734511] time: 1:00:30.203915 
[Epoch 11/30] [Batch 200/516] [D loss: 0.040449, acc:  98%] [G loss: 17.801014, adv: 0.946178, cyc: 0.728279, id: 0.729387] time: 1:01:36.857758 
[Epoch 11/30] [Batch 300/516] [D loss: 0.038788, acc:  98%] [G loss: 17.839964, adv: 0.987591, cyc: 0.726611, id: 0.717773] time: 1:02:43.545370 
[Epoch 11/30] [Batch 400/516] [D loss: 0.033598, acc:  99%] [G loss: 17.578735, adv: 0.963277, cyc: 0.717070, id: 0.698887] time: 1:03:50.173197 
[Epoch 11/30] [Batch 500/516] [D loss: 0.037246, acc:  98%] [G loss: 17.833284, adv: 1.009089, cyc: 0.724785, id: 0.719363] time: 1:04:56.805221 
[Epoch 12/30] [Batch 0/516] [D loss: 0.032994, acc:  98%] [G loss: 17.696779, adv: 1.003890, cyc: 0.719166, id: 0.703971] time: 1:05:07.464892 
[Epoch 12/30] [Batch 100/516] [D loss: 0.030847, acc: 100%] [G loss: 17.825237, adv: 1.002489, cyc: 0.725196, id: 0.706168] time: 1:06:14.089328 
[Epoch 12/30] [Batch 200/516] [D loss: 0.035382, acc:  98%] [G loss: 17.643730, adv: 1.003129, cyc: 0.717164, id: 0.704029] time: 1:07:20.700824 
[Epoch 12/30] [Batch 300/516] [D loss: 0.034978, acc:  98%] [G loss: 17.661968, adv: 1.035583, cyc: 0.715447, id: 0.688731] time: 1:08:27.309901 
[Epoch 12/30] [Batch 400/516] [D loss: 0.030892, acc: 100%] [G loss: 17.443989, adv: 1.031923, cyc: 0.706141, id: 0.673084] time: 1:09:33.913413 
[Epoch 12/30] [Batch 500/516] [D loss: 0.035906, acc:  98%] [G loss: 17.521576, adv: 1.026633, cyc: 0.710018, id: 0.693170] time: 1:10:40.515758 
[Epoch 13/30] [Batch 0/516] [D loss: 0.031732, acc:  98%] [G loss: 17.440512, adv: 1.031763, cyc: 0.706335, id: 0.673118] time: 1:10:51.175838 
[Epoch 13/30] [Batch 100/516] [D loss: 0.029932, acc: 100%] [G loss: 17.567617, adv: 1.027208, cyc: 0.712790, id: 0.671842] time: 1:11:57.798156 
[Epoch 13/30] [Batch 200/516] [D loss: 0.035289, acc:  98%] [G loss: 17.323454, adv: 1.023179, cyc: 0.701939, id: 0.672714] time: 1:13:04.380541 
[Epoch 13/30] [Batch 300/516] [D loss: 0.035694, acc:  98%] [G loss: 17.253229, adv: 1.015851, cyc: 0.699774, id: 0.655748] time: 1:14:10.972377 
[Epoch 13/30] [Batch 400/516] [D loss: 0.032563, acc:  99%] [G loss: 17.041636, adv: 1.021775, cyc: 0.689690, id: 0.641677] time: 1:15:17.565177 
[Epoch 13/30] [Batch 500/516] [D loss: 0.039264, acc:  98%] [G loss: 17.170221, adv: 1.009618, cyc: 0.696869, id: 0.661699] time: 1:16:24.153652 
[Epoch 14/30] [Batch 0/516] [D loss: 0.034861, acc:  98%] [G loss: 17.090940, adv: 1.044729, cyc: 0.690062, id: 0.643883] time: 1:16:34.807449 
[Epoch 14/30] [Batch 100/516] [D loss: 0.033452, acc: 100%] [G loss: 17.157829, adv: 1.000765, cyc: 0.697276, id: 0.648290] time: 1:17:41.317287 
[Epoch 14/30] [Batch 200/516] [D loss: 0.040463, acc:  98%] [G loss: 16.970528, adv: 1.015830, cyc: 0.687525, id: 0.645421] time: 1:18:47.818049 
[Epoch 14/30] [Batch 300/516] [D loss: 0.041825, acc:  98%] [G loss: 16.879635, adv: 1.005478, cyc: 0.684513, id: 0.631588] time: 1:19:54.329491 
[Epoch 14/30] [Batch 400/516] [D loss: 0.039512, acc:  99%] [G loss: 16.670891, adv: 1.004366, cyc: 0.675370, id: 0.614530] time: 1:21:00.824165 
[Epoch 14/30] [Batch 500/516] [D loss: 0.047815, acc:  97%] [G loss: 16.759506, adv: 0.992950, cyc: 0.680424, id: 0.633881] time: 1:22:07.321361 
[Epoch 15/30] [Batch 0/516] [D loss: 0.042803, acc:  99%] [G loss: 16.651773, adv: 1.006418, cyc: 0.674200, id: 0.620441] time: 1:22:17.962322 
[Epoch 15/30] [Batch 100/516] [D loss: 0.041171, acc:  99%] [G loss: 16.754250, adv: 0.986035, cyc: 0.681078, id: 0.621032] time: 1:23:24.455003 
[Epoch 15/30] [Batch 200/516] [D loss: 0.049355, acc:  98%] [G loss: 16.612066, adv: 0.995721, cyc: 0.673829, id: 0.620990] time: 1:24:30.990697 
[Epoch 15/30] [Batch 300/516] [D loss: 0.050896, acc:  98%] [G loss: 16.577833, adv: 0.985888, cyc: 0.673550, id: 0.607628] time: 1:25:37.623986 
[Epoch 15/30] [Batch 400/516] [D loss: 0.046777, acc:  98%] [G loss: 16.359924, adv: 0.985575, cyc: 0.663960, id: 0.589045] time: 1:26:44.292697 
[Epoch 15/30] [Batch 500/516] [D loss: 0.054842, acc:  96%] [G loss: 16.541958, adv: 1.031349, cyc: 0.667903, id: 0.609853] time: 1:27:50.995335 
[Epoch 16/30] [Batch 0/516] [D loss: 0.049531, acc:  98%] [G loss: 16.334524, adv: 1.003363, cyc: 0.660765, id: 0.598367] time: 1:28:04.867396 
[Epoch 16/30] [Batch 100/516] [D loss: 0.045875, acc:  98%] [G loss: 16.459087, adv: 0.998310, cyc: 0.667256, id: 0.596306] time: 1:29:11.577990 
[Epoch 16/30] [Batch 200/516] [D loss: 0.053830, acc:  98%] [G loss: 16.265982, adv: 0.978047, cyc: 0.660279, id: 0.598965] time: 1:30:18.227648 
[Epoch 16/30] [Batch 300/516] [D loss: 0.055350, acc:  97%] [G loss: 16.233404, adv: 0.998846, cyc: 0.657267, id: 0.584781] time: 1:31:24.877295 
[Epoch 16/30] [Batch 400/516] [D loss: 0.049822, acc:  98%] [G loss: 15.984665, adv: 1.004382, cyc: 0.645194, id: 0.572230] time: 1:32:31.522042 
[Epoch 16/30] [Batch 500/516] [D loss: 0.057895, acc:  95%] [G loss: 16.152000, adv: 0.993399, cyc: 0.654268, id: 0.589274] time: 1:33:38.157799 
[Epoch 17/30] [Batch 0/516] [D loss: 0.052646, acc:  98%] [G loss: 15.990790, adv: 0.994845, cyc: 0.646619, id: 0.571599] time: 1:33:48.821941 
[Epoch 17/30] [Batch 100/516] [D loss: 0.047811, acc:  98%] [G loss: 16.081995, adv: 0.982134, cyc: 0.652092, id: 0.574913] time: 1:34:55.461858 
[Epoch 17/30] [Batch 200/516] [D loss: 0.056431, acc:  97%] [G loss: 15.928378, adv: 0.997460, cyc: 0.643856, id: 0.573818] time: 1:36:02.095563 
[Epoch 17/30] [Batch 300/516] [D loss: 0.059157, acc:  96%] [G loss: 15.816889, adv: 0.984184, cyc: 0.639922, id: 0.561986] time: 1:37:08.711256 
[Epoch 17/30] [Batch 400/516] [D loss: 0.053788, acc:  97%] [G loss: 15.636254, adv: 1.005600, cyc: 0.629794, id: 0.549021] time: 1:38:15.347474 
[Epoch 17/30] [Batch 500/516] [D loss: 0.063057, acc:  95%] [G loss: 15.805137, adv: 0.990281, cyc: 0.639236, id: 0.569114] time: 1:39:21.985636 
[Epoch 18/30] [Batch 0/516] [D loss: 0.057802, acc:  97%] [G loss: 15.569592, adv: 0.985460, cyc: 0.628568, id: 0.553185] time: 1:39:32.642272 
[Epoch 18/30] [Batch 100/516] [D loss: 0.052731, acc:  98%] [G loss: 15.676494, adv: 0.966533, cyc: 0.635509, id: 0.551406] time: 1:40:39.247901 
[Epoch 18/30] [Batch 200/516] [D loss: 0.062396, acc:  96%] [G loss: 15.410320, adv: 0.950185, cyc: 0.624799, id: 0.552537] time: 1:41:45.842649 
[Epoch 18/30] [Batch 300/516] [D loss: 0.067356, acc:  95%] [G loss: 15.438702, adv: 0.979887, cyc: 0.623623, id: 0.540101] time: 1:42:52.432524 
[Epoch 18/30] [Batch 400/516] [D loss: 0.061966, acc:  95%] [G loss: 15.182392, adv: 0.979773, cyc: 0.611675, id: 0.529653] time: 1:43:59.037599 
[Epoch 18/30] [Batch 500/516] [D loss: 0.072558, acc:  94%] [G loss: 15.275591, adv: 0.967287, cyc: 0.616948, id: 0.549968] time: 1:45:05.641211 
[Epoch 19/30] [Batch 0/516] [D loss: 0.067445, acc:  94%] [G loss: 15.137167, adv: 0.959670, cyc: 0.611375, id: 0.535490] time: 1:45:16.294023 
[Epoch 19/30] [Batch 100/516] [D loss: 0.062003, acc:  94%] [G loss: 15.214951, adv: 0.944670, cyc: 0.616585, id: 0.531997] time: 1:46:22.855747 
[Epoch 19/30] [Batch 200/516] [D loss: 0.072180, acc:  94%] [G loss: 15.024075, adv: 0.912016, cyc: 0.610995, id: 0.535183] time: 1:47:29.410467 
[Epoch 19/30] [Batch 300/516] [D loss: 0.078985, acc:  94%] [G loss: 14.987477, adv: 0.939914, cyc: 0.606684, id: 0.525363] time: 1:48:35.972432 
[Epoch 19/30] [Batch 400/516] [D loss: 0.072241, acc:  94%] [G loss: 14.732389, adv: 0.913985, cyc: 0.597256, id: 0.512914] time: 1:49:42.537343 
[Epoch 19/30] [Batch 500/516] [D loss: 0.082919, acc:  92%] [G loss: 14.842691, adv: 0.915803, cyc: 0.602147, id: 0.532932] time: 1:50:49.091596 
[Epoch 20/30] [Batch 0/516] [D loss: 0.077582, acc:  92%] [G loss: 14.804731, adv: 0.920585, cyc: 0.600212, id: 0.518867] time: 1:50:59.743129 
[Epoch 20/30] [Batch 100/516] [D loss: 0.070897, acc:  93%] [G loss: 14.847710, adv: 0.914820, cyc: 0.602380, id: 0.523491] time: 1:52:06.316764 
[Epoch 20/30] [Batch 200/516] [D loss: 0.080302, acc:  93%] [G loss: 14.650504, adv: 0.883305, cyc: 0.596470, id: 0.522674] time: 1:53:12.853174 
[Epoch 20/30] [Batch 300/516] [D loss: 0.086698, acc:  93%] [G loss: 14.663471, adv: 0.916949, cyc: 0.593941, id: 0.516767] time: 1:54:19.375447 
[Epoch 20/30] [Batch 400/516] [D loss: 0.077978, acc:  93%] [G loss: 14.437128, adv: 0.906423, cyc: 0.584482, id: 0.505360] time: 1:55:25.901644 
[Epoch 20/30] [Batch 500/516] [D loss: 0.087609, acc:  91%] [G loss: 14.661841, adv: 0.928577, cyc: 0.592676, id: 0.526528] time: 1:56:32.427626 
[Epoch 21/30] [Batch 0/516] [D loss: 0.082023, acc:  92%] [G loss: 14.495592, adv: 0.898999, cyc: 0.587963, id: 0.512624] time: 1:56:45.766493 
[Epoch 21/30] [Batch 100/516] [D loss: 0.073928, acc:  91%] [G loss: 14.621695, adv: 0.916862, cyc: 0.591800, id: 0.515619] time: 1:57:52.262898 
[Epoch 21/30] [Batch 200/516] [D loss: 0.081584, acc:  92%] [G loss: 14.463126, adv: 0.890571, cyc: 0.587011, id: 0.519408] time: 1:58:58.705576 
[Epoch 21/30] [Batch 300/516] [D loss: 0.087621, acc:  93%] [G loss: 14.539476, adv: 0.939234, cyc: 0.586322, id: 0.510991] time: 2:00:05.149476 
[Epoch 21/30] [Batch 400/516] [D loss: 0.077763, acc:  92%] [G loss: 14.285993, adv: 0.913535, cyc: 0.576859, id: 0.498052] time: 2:01:11.613336 
[Epoch 21/30] [Batch 500/516] [D loss: 0.085930, acc:  92%] [G loss: 14.495518, adv: 0.919646, cyc: 0.586148, id: 0.519562] time: 2:02:18.054572 
[Epoch 22/30] [Batch 0/516] [D loss: 0.080694, acc:  91%] [G loss: 14.367199, adv: 0.931968, cyc: 0.578981, id: 0.505433] time: 2:02:28.690862 
[Epoch 22/30] [Batch 100/516] [D loss: 0.071615, acc:  92%] [G loss: 14.491974, adv: 0.933926, cyc: 0.584663, id: 0.506006] time: 2:03:35.150519 
[Epoch 22/30] [Batch 200/516] [D loss: 0.078172, acc:  93%] [G loss: 14.241490, adv: 0.876267, cyc: 0.578366, id: 0.509455] time: 2:04:41.610220 
[Epoch 22/30] [Batch 300/516] [D loss: 0.084288, acc:  93%] [G loss: 14.336884, adv: 0.929962, cyc: 0.577824, id: 0.504798] time: 2:05:48.164993 
[Epoch 22/30] [Batch 400/516] [D loss: 0.074529, acc:  93%] [G loss: 14.102175, adv: 0.917189, cyc: 0.568080, id: 0.491913] time: 2:06:54.832979 
[Epoch 22/30] [Batch 500/516] [D loss: 0.081537, acc:  92%] [G loss: 14.274992, adv: 0.911400, cyc: 0.576615, id: 0.511263] time: 2:08:01.564265 
[Epoch 23/30] [Batch 0/516] [D loss: 0.076673, acc:  91%] [G loss: 14.101326, adv: 0.907054, cyc: 0.568776, id: 0.501960] time: 2:08:12.237988 
[Epoch 23/30] [Batch 100/516] [D loss: 0.066934, acc:  93%] [G loss: 14.304438, adv: 0.917732, cyc: 0.577581, id: 0.500137] time: 2:09:18.952889 
[Epoch 23/30] [Batch 200/516] [D loss: 0.073119, acc:  94%] [G loss: 14.114854, adv: 0.904841, cyc: 0.569787, id: 0.505163] time: 2:10:25.654564 
[Epoch 23/30] [Batch 300/516] [D loss: 0.079579, acc:  93%] [G loss: 14.164970, adv: 0.942391, cyc: 0.568687, id: 0.497054] time: 2:11:32.294733 
[Epoch 23/30] [Batch 400/516] [D loss: 0.070532, acc:  93%] [G loss: 13.981113, adv: 0.945091, cyc: 0.559954, id: 0.484926] time: 2:12:38.931059 
[Epoch 23/30] [Batch 500/516] [D loss: 0.076901, acc:  93%] [G loss: 14.220006, adv: 0.939580, cyc: 0.571692, id: 0.504076] time: 2:13:45.571969 
[Epoch 24/30] [Batch 0/516] [D loss: 0.072238, acc:  93%] [G loss: 14.106657, adv: 0.954880, cyc: 0.565075, id: 0.490856] time: 2:13:56.236447 
[Epoch 24/30] [Batch 100/516] [D loss: 0.061977, acc:  94%] [G loss: 14.147410, adv: 0.952788, cyc: 0.566894, id: 0.493616] time: 2:15:02.910301 
[Epoch 24/30] [Batch 200/516] [D loss: 0.068124, acc:  95%] [G loss: 13.996833, adv: 0.924230, cyc: 0.562718, id: 0.495558] time: 2:16:09.588078 
[Epoch 24/30] [Batch 300/516] [D loss: 0.074711, acc:  94%] [G loss: 14.031124, adv: 0.945079, cyc: 0.562502, id: 0.487886] time: 2:17:16.202367 
[Epoch 24/30] [Batch 400/516] [D loss: 0.066694, acc:  93%] [G loss: 13.818045, adv: 0.934745, cyc: 0.553460, id: 0.478797] time: 2:18:22.818845 
[Epoch 24/30] [Batch 500/516] [D loss: 0.072666, acc:  93%] [G loss: 14.036432, adv: 0.957642, cyc: 0.561408, id: 0.498244] time: 2:19:29.444783 
[Epoch 25/30] [Batch 0/516] [D loss: 0.068083, acc:  93%] [G loss: 13.886474, adv: 0.940728, cyc: 0.556036, id: 0.487368] time: 2:19:40.104770 
[Epoch 25/30] [Batch 100/516] [D loss: 0.057374, acc:  94%] [G loss: 13.965928, adv: 0.947447, cyc: 0.558908, id: 0.488481] time: 2:20:46.728365 
[Epoch 25/30] [Batch 200/516] [D loss: 0.063679, acc:  96%] [G loss: 13.868599, adv: 0.932728, cyc: 0.555925, id: 0.492398] time: 2:21:53.333746 
[Epoch 25/30] [Batch 300/516] [D loss: 0.070307, acc:  95%] [G loss: 13.827703, adv: 0.959425, cyc: 0.551270, id: 0.484883] time: 2:22:59.927890 
[Epoch 25/30] [Batch 400/516] [D loss: 0.063335, acc:  94%] [G loss: 13.724503, adv: 0.964783, cyc: 0.546263, id: 0.472692] time: 2:24:06.533111 
[Epoch 25/30] [Batch 500/516] [D loss: 0.069070, acc:  93%] [G loss: 13.909032, adv: 0.980411, cyc: 0.553316, id: 0.491311] time: 2:25:13.125535 
[Epoch 26/30] [Batch 0/516] [D loss: 0.064560, acc:  94%] [G loss: 13.788386, adv: 0.962570, cyc: 0.549501, id: 0.480390] time: 2:25:27.649143 
[Epoch 26/30] [Batch 100/516] [D loss: 0.053464, acc:  95%] [G loss: 13.919036, adv: 0.973845, cyc: 0.554332, id: 0.481344] time: 2:26:34.216883 
[Epoch 26/30] [Batch 200/516] [D loss: 0.059971, acc:  96%] [G loss: 13.711658, adv: 0.933851, cyc: 0.548554, id: 0.485467] time: 2:27:40.779761 
[Epoch 26/30] [Batch 300/516] [D loss: 0.066454, acc:  96%] [G loss: 13.706206, adv: 0.952372, cyc: 0.546525, id: 0.479676] time: 2:28:47.348583 
[Epoch 26/30] [Batch 400/516] [D loss: 0.060436, acc:  94%] [G loss: 13.584898, adv: 0.978472, cyc: 0.538592, id: 0.466676] time: 2:29:53.925535 
[Epoch 26/30] [Batch 500/516] [D loss: 0.065972, acc:  94%] [G loss: 13.776636, adv: 0.976315, cyc: 0.547402, id: 0.488491] time: 2:31:00.498588 
[Epoch 27/30] [Batch 0/516] [D loss: 0.061583, acc:  94%] [G loss: 13.568416, adv: 0.939784, cyc: 0.541257, id: 0.475305] time: 2:31:11.147997 
[Epoch 27/30] [Batch 100/516] [D loss: 0.050197, acc:  96%] [G loss: 13.780724, adv: 0.981606, cyc: 0.547233, id: 0.478049] time: 2:32:17.701208 
[Epoch 27/30] [Batch 200/516] [D loss: 0.056794, acc:  96%] [G loss: 13.632499, adv: 0.952420, cyc: 0.542974, id: 0.483363] time: 2:33:24.263537 
[Epoch 27/30] [Batch 300/516] [D loss: 0.063177, acc:  96%] [G loss: 13.674383, adv: 0.986151, cyc: 0.542013, id: 0.470933] time: 2:34:30.795945 
[Epoch 27/30] [Batch 400/516] [D loss: 0.057963, acc:  95%] [G loss: 13.424974, adv: 0.946216, cyc: 0.534362, id: 0.459350] time: 2:35:37.314727 
[Epoch 27/30] [Batch 500/516] [D loss: 0.063328, acc:  95%] [G loss: 13.607950, adv: 0.950820, cyc: 0.542193, id: 0.480937] time: 2:36:43.846477 
[Epoch 28/30] [Batch 0/516] [D loss: 0.059097, acc:  95%] [G loss: 13.520683, adv: 0.947108, cyc: 0.538640, id: 0.468457] time: 2:36:54.490945 
[Epoch 28/30] [Batch 100/516] [D loss: 0.047441, acc:  96%] [G loss: 13.609039, adv: 0.962311, cyc: 0.541045, id: 0.470411] time: 2:38:01.021990 
[Epoch 28/30] [Batch 200/516] [D loss: 0.054114, acc:  98%] [G loss: 13.480422, adv: 0.947519, cyc: 0.536440, id: 0.476013] time: 2:39:07.560370 
[Epoch 28/30] [Batch 300/516] [D loss: 0.060349, acc:  96%] [G loss: 13.446568, adv: 0.953080, cyc: 0.534455, id: 0.465769] time: 2:40:13.999964 
[Epoch 28/30] [Batch 400/516] [D loss: 0.055848, acc:  95%] [G loss: 13.326967, adv: 0.956106, cyc: 0.528753, id: 0.459063] time: 2:41:20.435043 
[Epoch 28/30] [Batch 500/516] [D loss: 0.061018, acc:  96%] [G loss: 13.412147, adv: 0.911358, cyc: 0.536757, id: 0.475466] time: 2:42:26.871544 
[Epoch 29/30] [Batch 0/516] [D loss: 0.056945, acc:  95%] [G loss: 13.397686, adv: 0.973592, cyc: 0.530457, id: 0.461195] time: 2:42:37.502639 
[Epoch 29/30] [Batch 100/516] [D loss: 0.045145, acc:  97%] [G loss: 13.457095, adv: 0.950946, cyc: 0.534784, id: 0.468526] time: 2:43:43.941358 
[Epoch 29/30] [Batch 200/516] [D loss: 0.051795, acc:  98%] [G loss: 13.343891, adv: 0.947045, cyc: 0.530105, id: 0.471145] time: 2:44:50.387366 
[Epoch 29/30] [Batch 300/516] [D loss: 0.057887, acc:  96%] [G loss: 13.341410, adv: 0.940892, cyc: 0.530682, id: 0.465344] time: 2:45:56.853830 
[Epoch 29/30] [Batch 400/516] [D loss: 0.053985, acc:  96%] [G loss: 13.206040, adv: 0.965808, cyc: 0.522122, id: 0.453778] time: 2:47:03.394642 
[Epoch 29/30] [Batch 500/516] [D loss: 0.058945, acc:  96%] [G loss: 13.413849, adv: 0.971933, cyc: 0.531143, id: 0.471598] time: 2:48:10.087427 
[Epoch 30/30] [Batch 0/516] [D loss: 0.055044, acc:  96%] [G loss: 13.238081, adv: 0.963453, cyc: 0.523783, id: 0.458600] time: 2:48:20.765218 
[Epoch 30/30] [Batch 100/516] [D loss: 0.043139, acc:  98%] [G loss: 13.332441, adv: 0.948204, cyc: 0.529352, id: 0.461882] time: 2:49:27.474827 
[Epoch 30/30] [Batch 200/516] [D loss: 0.049709, acc:  98%] [G loss: 13.224204, adv: 0.945503, cyc: 0.524549, id: 0.468393] time: 2:50:34.187905 
[Epoch 30/30] [Batch 300/516] [D loss: 0.055671, acc:  96%] [G loss: 13.219035, adv: 0.949597, cyc: 0.524109, id: 0.460032] time: 2:51:40.859846 
[Epoch 30/30] [Batch 400/516] [D loss: 0.052296, acc:  96%] [G loss: 13.130903, adv: 0.965302, cyc: 0.518891, id: 0.446782] time: 2:52:47.504553 
[Epoch 30/30] [Batch 500/516] [D loss: 0.057058, acc:  96%] [G loss: 13.356489, adv: 0.969825, cyc: 0.528825, id: 0.468052] time: 2:53:54.169832 
Train Finished.
