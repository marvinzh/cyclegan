_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         (None, 1, 600, 1)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 1, 600, 64)        256       
_________________________________________________________________
leaky_re_lu_5 (LeakyReLU)    (None, 1, 600, 64)        0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 1, 600, 128)       24704     
_________________________________________________________________
leaky_re_lu_6 (LeakyReLU)    (None, 1, 600, 128)       0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 76800)             0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               39322112  
_________________________________________________________________
leaky_re_lu_7 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 512)               262656    
_________________________________________________________________
leaky_re_lu_8 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
D_A_Out (Dense)              (None, 1)                 513       
=================================================================
Total params: 79,220,482
Trainable params: 39,610,241
Non-trainable params: 39,610,241
_________________________________________________________________
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 1, 600, 1)         0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 1, 600, 64)        256       
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU)    (None, 1, 600, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 1, 600, 128)       24704     
_________________________________________________________________
leaky_re_lu_2 (LeakyReLU)    (None, 1, 600, 128)       0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 76800)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               39322112  
_________________________________________________________________
leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
D_B_Out (Dense)              (None, 1)                 513       
=================================================================
Total params: 79,220,482
Trainable params: 39,610,241
Non-trainable params: 39,610,241
_________________________________________________________________
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
ivec_b (InputLayer)             (None, 1, 600, 1)    0                                            
__________________________________________________________________________________________________
ivec_a (InputLayer)             (None, 1, 600, 1)    0                                            
__________________________________________________________________________________________________
model_4 (Model)                 (None, 1, 600, 1)    653345      ivec_b[0][0]                     
                                                                 model_3[1][0]                    
                                                                 ivec_a[0][0]                     
__________________________________________________________________________________________________
model_3 (Model)                 (None, 1, 600, 1)    653345      ivec_a[0][0]                     
                                                                 model_4[1][0]                    
                                                                 ivec_b[0][0]                     
__________________________________________________________________________________________________
model_2 (Model)                 (None, 1)            39610241    model_4[1][0]                    
__________________________________________________________________________________________________
model_1 (Model)                 (None, 1)            39610241    model_3[1][0]                    
==================================================================================================
Total params: 80,527,172
Trainable params: 1,306,690
Non-trainable params: 79,220,482
__________________________________________________________________________________________________
Start CycleGAN training.
[Epoch 1/20] [Batch 0/516] [D loss: 2.900858, acc:   0%] [G loss: 21.583073, adv: 0.118247, cyc: 0.971358, id: 0.963303] time: 0:01:58.296637 
[Epoch 1/20] [Batch 100/516] [D loss: 0.219009, acc:  66%] [G loss: 14.171371, adv: 0.457137, cyc: 0.613080, id: 0.499431] time: 0:03:08.428979 
[Epoch 1/20] [Batch 200/516] [D loss: 0.100911, acc:  94%] [G loss: 12.867734, adv: 0.747256, cyc: 0.525758, id: 0.431000] time: 0:04:18.540118 
[Epoch 1/20] [Batch 300/516] [D loss: 0.019955, acc: 100%] [G loss: 12.005724, adv: 0.651615, cyc: 0.493819, id: 0.398584] time: 0:05:28.630845 
[Epoch 1/20] [Batch 400/516] [D loss: 0.021363, acc:  99%] [G loss: 11.317205, adv: 0.683867, cyc: 0.458696, id: 0.379520] time: 0:06:38.701331 
[Epoch 1/20] [Batch 500/516] [D loss: 0.009573, acc: 100%] [G loss: 11.811145, adv: 0.893872, cyc: 0.461679, id: 0.378789] time: 0:07:48.764801 
[Epoch 2/20] [Batch 0/516] [D loss: 0.013018, acc:  99%] [G loss: 11.368770, adv: 0.831480, cyc: 0.447117, id: 0.356895] time: 0:07:59.973480 
[Epoch 2/20] [Batch 100/516] [D loss: 0.018206, acc: 100%] [G loss: 11.300626, adv: 1.020425, cyc: 0.426368, id: 0.361747] time: 0:09:10.048787 
[Epoch 2/20] [Batch 200/516] [D loss: 0.011759, acc:  99%] [G loss: 10.675004, adv: 0.869654, cyc: 0.411707, id: 0.349975] time: 0:10:20.114653 
[Epoch 2/20] [Batch 300/516] [D loss: 0.004829, acc: 100%] [G loss: 10.692753, adv: 0.944379, cyc: 0.405310, id: 0.327931] time: 0:11:30.187270 
[Epoch 2/20] [Batch 400/516] [D loss: 0.067904, acc:  91%] [G loss: 9.810279, adv: 0.774911, cyc: 0.380361, id: 0.321739] time: 0:12:40.255490 
[Epoch 2/20] [Batch 500/516] [D loss: 0.003388, acc: 100%] [G loss: 10.402034, adv: 0.958389, cyc: 0.390175, id: 0.326797] time: 0:13:50.312890 
[Epoch 3/20] [Batch 0/516] [D loss: 0.049440, acc:  94%] [G loss: 10.519210, adv: 1.123639, cyc: 0.380557, id: 0.314492] time: 0:14:01.523014 
[Epoch 3/20] [Batch 100/516] [D loss: 0.006958, acc: 100%] [G loss: 10.087397, adv: 0.966363, cyc: 0.375192, id: 0.324710] time: 0:15:11.571966 
[Epoch 3/20] [Batch 200/516] [D loss: 0.007503, acc: 100%] [G loss: 10.059476, adv: 1.034887, cyc: 0.367768, id: 0.314770] time: 0:16:21.617559 
[Epoch 3/20] [Batch 300/516] [D loss: 0.014283, acc:  98%] [G loss: 9.604160, adv: 0.816986, cyc: 0.366679, id: 0.302337] time: 0:17:31.657629 
[Epoch 3/20] [Batch 400/516] [D loss: 0.005009, acc: 100%] [G loss: 9.552632, adv: 0.985214, cyc: 0.349112, id: 0.303028] time: 0:18:41.698148 
[Epoch 3/20] [Batch 500/516] [D loss: 0.018724, acc:  99%] [G loss: 9.737635, adv: 0.920383, cyc: 0.362945, id: 0.305974] time: 0:19:51.733420 
[Epoch 4/20] [Batch 0/516] [D loss: 0.002191, acc: 100%] [G loss: 9.662464, adv: 1.010229, cyc: 0.351769, id: 0.289394] time: 0:20:02.943136 
[Epoch 4/20] [Batch 100/516] [D loss: 0.003291, acc: 100%] [G loss: 9.545032, adv: 0.958599, cyc: 0.350705, id: 0.307148] time: 0:21:12.993589 
[Epoch 4/20] [Batch 200/516] [D loss: 0.005114, acc: 100%] [G loss: 9.435472, adv: 0.973291, cyc: 0.344294, id: 0.299346] time: 0:22:23.036036 
[Epoch 4/20] [Batch 300/516] [D loss: 0.009981, acc: 100%] [G loss: 9.554287, adv: 1.007953, cyc: 0.346305, id: 0.286015] time: 0:23:33.083746 
[Epoch 4/20] [Batch 400/516] [D loss: 0.006502, acc: 100%] [G loss: 9.146381, adv: 0.970630, cyc: 0.331415, id: 0.291297] time: 0:24:43.131339 
[Epoch 4/20] [Batch 500/516] [D loss: 0.029050, acc:  96%] [G loss: 9.494303, adv: 0.967893, cyc: 0.347231, id: 0.292087] time: 0:25:53.180939 
[Epoch 5/20] [Batch 0/516] [D loss: 0.008009, acc: 100%] [G loss: 9.427305, adv: 1.053094, cyc: 0.336421, id: 0.282141] time: 0:26:04.387005 
[Epoch 5/20] [Batch 100/516] [D loss: 0.000959, acc: 100%] [G loss: 9.323462, adv: 1.016164, cyc: 0.334931, id: 0.295636] time: 0:27:14.439169 
[Epoch 5/20] [Batch 200/516] [D loss: 0.004474, acc: 100%] [G loss: 9.060781, adv: 0.914651, cyc: 0.332556, id: 0.289225] time: 0:28:24.479977 
[Epoch 5/20] [Batch 300/516] [D loss: 0.001684, acc: 100%] [G loss: 9.247814, adv: 0.989937, cyc: 0.333933, id: 0.277814] time: 0:29:34.523588 
[Epoch 5/20] [Batch 400/516] [D loss: 0.001756, acc: 100%] [G loss: 8.944815, adv: 0.979370, cyc: 0.321159, id: 0.278009] time: 0:30:44.579433 
[Epoch 5/20] [Batch 500/516] [D loss: 0.006810, acc: 100%] [G loss: 9.408251, adv: 1.028733, cyc: 0.337362, id: 0.284903] time: 0:31:54.630603 
[Epoch 6/20] [Batch 0/516] [D loss: 0.015956, acc:  98%] [G loss: 9.210626, adv: 1.054423, cyc: 0.326142, id: 0.278985] time: 0:32:13.202006 
[Epoch 6/20] [Batch 100/516] [D loss: 0.000762, acc: 100%] [G loss: 9.034434, adv: 0.982631, cyc: 0.324749, id: 0.286079] time: 0:33:23.256389 
[Epoch 6/20] [Batch 200/516] [D loss: 0.027109, acc:  96%] [G loss: 8.942381, adv: 0.951140, cyc: 0.323280, id: 0.285332] time: 0:34:33.314069 
[Epoch 6/20] [Batch 300/516] [D loss: 0.022447, acc:  97%] [G loss: 9.324816, adv: 1.074763, cyc: 0.329271, id: 0.272336] time: 0:35:43.362580 
[Epoch 6/20] [Batch 400/516] [D loss: 0.002051, acc: 100%] [G loss: 8.745440, adv: 0.979047, cyc: 0.311879, id: 0.271687] time: 0:36:53.413832 
[Epoch 6/20] [Batch 500/516] [D loss: 0.004507, acc: 100%] [G loss: 9.118348, adv: 0.978231, cyc: 0.328718, id: 0.278352] time: 0:38:03.462150 
[Epoch 7/20] [Batch 0/516] [D loss: 0.002319, acc: 100%] [G loss: 8.873095, adv: 0.956590, cyc: 0.319260, id: 0.261483] time: 0:38:14.670782 
[Epoch 7/20] [Batch 100/516] [D loss: 0.000632, acc: 100%] [G loss: 8.922112, adv: 0.983426, cyc: 0.319312, id: 0.285367] time: 0:39:24.724305 
[Epoch 7/20] [Batch 200/516] [D loss: 0.000577, acc: 100%] [G loss: 8.874131, adv: 0.989572, cyc: 0.316773, id: 0.281140] time: 0:40:34.782782 
[Epoch 7/20] [Batch 300/516] [D loss: 0.012941, acc:  99%] [G loss: 9.030980, adv: 1.007427, cyc: 0.321758, id: 0.275091] time: 0:41:44.843278 
[Epoch 7/20] [Batch 400/516] [D loss: 0.026939, acc:  97%] [G loss: 8.644512, adv: 0.972812, cyc: 0.307644, id: 0.263612] time: 0:42:54.898473 
[Epoch 7/20] [Batch 500/516] [D loss: 0.003470, acc: 100%] [G loss: 8.619050, adv: 0.791485, cyc: 0.322905, id: 0.276690] time: 0:44:04.951664 
[Epoch 8/20] [Batch 0/516] [D loss: 0.000938, acc: 100%] [G loss: 8.839070, adv: 1.022545, cyc: 0.311804, id: 0.259203] time: 0:44:16.157324 
[Epoch 8/20] [Batch 100/516] [D loss: 0.000569, acc: 100%] [G loss: 8.815062, adv: 0.980994, cyc: 0.314353, id: 0.276583] time: 0:45:26.207681 
[Epoch 8/20] [Batch 200/516] [D loss: 0.001594, acc: 100%] [G loss: 8.735217, adv: 0.942633, cyc: 0.313983, id: 0.270274] time: 0:46:36.266242 
[Epoch 8/20] [Batch 300/516] [D loss: 0.001635, acc: 100%] [G loss: 8.842997, adv: 0.968092, cyc: 0.316764, id: 0.276254] time: 0:47:46.333730 
[Epoch 8/20] [Batch 400/516] [D loss: 0.001770, acc: 100%] [G loss: 8.560337, adv: 0.979301, cyc: 0.302860, id: 0.274263] time: 0:48:56.375960 
[Epoch 8/20] [Batch 500/516] [D loss: 0.001022, acc: 100%] [G loss: 8.957903, adv: 1.001098, cyc: 0.318799, id: 0.267823] time: 0:50:06.418515 
[Epoch 9/20] [Batch 0/516] [D loss: 0.000498, acc: 100%] [G loss: 8.646397, adv: 0.983272, cyc: 0.306413, id: 0.260986] time: 0:50:17.626443 
[Epoch 9/20] [Batch 100/516] [D loss: 0.001711, acc: 100%] [G loss: 8.703191, adv: 0.980581, cyc: 0.309219, id: 0.281237] time: 0:51:27.672160 
[Epoch 9/20] [Batch 200/516] [D loss: 0.007374, acc:  99%] [G loss: 8.634060, adv: 0.981060, cyc: 0.306280, id: 0.267979] time: 0:52:37.718546 
[Epoch 9/20] [Batch 300/516] [D loss: 0.000441, acc: 100%] [G loss: 8.776397, adv: 0.989687, cyc: 0.311684, id: 0.261550] time: 0:53:47.761390 
[Epoch 9/20] [Batch 400/516] [D loss: 0.001282, acc: 100%] [G loss: 8.454319, adv: 0.982241, cyc: 0.297790, id: 0.274454] time: 0:54:57.795628 
[Epoch 9/20] [Batch 500/516] [D loss: 0.001238, acc: 100%] [G loss: 8.835670, adv: 0.959490, cyc: 0.316669, id: 0.266553] time: 0:56:07.841248 
[Epoch 10/20] [Batch 0/516] [D loss: 0.001012, acc: 100%] [G loss: 8.597576, adv: 0.994133, cyc: 0.302894, id: 0.259018] time: 0:56:19.047981 
[Epoch 10/20] [Batch 100/516] [D loss: 0.002516, acc: 100%] [G loss: 8.677886, adv: 0.989172, cyc: 0.307318, id: 0.281949] time: 0:57:29.111032 
[Epoch 10/20] [Batch 200/516] [D loss: 0.000861, acc: 100%] [G loss: 8.551469, adv: 0.982844, cyc: 0.302033, id: 0.268918] time: 0:58:39.170541 
[Epoch 10/20] [Batch 300/516] [D loss: 0.000696, acc: 100%] [G loss: 8.635419, adv: 0.964197, cyc: 0.307306, id: 0.267912] time: 0:59:49.227562 
[Epoch 10/20] [Batch 400/516] [D loss: 0.001682, acc: 100%] [G loss: 8.321116, adv: 0.954439, cyc: 0.293844, id: 0.255833] time: 1:00:59.288699 
[Epoch 10/20] [Batch 500/516] [D loss: 0.000446, acc: 100%] [G loss: 8.779291, adv: 0.987238, cyc: 0.311888, id: 0.269239] time: 1:02:09.345476 
[Epoch 11/20] [Batch 0/516] [D loss: 0.002669, acc: 100%] [G loss: 8.537905, adv: 0.988141, cyc: 0.300463, id: 0.249286] time: 1:02:22.098575 
[Epoch 11/20] [Batch 100/516] [D loss: 0.001227, acc: 100%] [G loss: 8.570606, adv: 0.959956, cyc: 0.304426, id: 0.265489] time: 1:03:32.132240 
[Epoch 11/20] [Batch 200/516] [D loss: 0.001364, acc: 100%] [G loss: 8.446780, adv: 0.978082, cyc: 0.297538, id: 0.270070] time: 1:04:42.175374 
[Epoch 11/20] [Batch 300/516] [D loss: 0.000401, acc: 100%] [G loss: 8.499908, adv: 0.917995, cyc: 0.304869, id: 0.255770] time: 1:05:52.207219 
[Epoch 11/20] [Batch 400/516] [D loss: 0.000598, acc: 100%] [G loss: 8.191321, adv: 0.927348, cyc: 0.290460, id: 0.265980] time: 1:07:02.240594 
[Epoch 11/20] [Batch 500/516] [D loss: 0.001557, acc: 100%] [G loss: 8.697178, adv: 0.972320, cyc: 0.309276, id: 0.265848] time: 1:08:12.273201 
[Epoch 12/20] [Batch 0/516] [D loss: 0.000954, acc: 100%] [G loss: 8.385973, adv: 0.951167, cyc: 0.296797, id: 0.251854] time: 1:08:23.477035 
[Epoch 12/20] [Batch 100/516] [D loss: 0.001113, acc: 100%] [G loss: 8.448907, adv: 0.977197, cyc: 0.297486, id: 0.269479] time: 1:09:33.512263 
[Epoch 12/20] [Batch 200/516] [D loss: 0.001577, acc: 100%] [G loss: 8.386120, adv: 0.968134, cyc: 0.295495, id: 0.263364] time: 1:10:43.543500 
[Epoch 12/20] [Batch 300/516] [D loss: 0.001298, acc: 100%] [G loss: 8.525845, adv: 0.959495, cyc: 0.302754, id: 0.255947] time: 1:11:53.558840 
[Epoch 12/20] [Batch 400/516] [D loss: 0.000441, acc: 100%] [G loss: 8.247592, adv: 0.980599, cyc: 0.288106, id: 0.251835] time: 1:13:03.561247 
[Epoch 12/20] [Batch 500/516] [D loss: 0.000936, acc: 100%] [G loss: 8.661655, adv: 0.964003, cyc: 0.308189, id: 0.284506] time: 1:14:13.543101 
[Epoch 13/20] [Batch 0/516] [D loss: 0.011827, acc:  98%] [G loss: 8.069917, adv: 0.812943, cyc: 0.295131, id: 0.244926] time: 1:14:24.738789 
[Epoch 13/20] [Batch 100/516] [D loss: 0.001913, acc: 100%] [G loss: 8.428443, adv: 0.981126, cyc: 0.296099, id: 0.262279] time: 1:15:34.695664 
[Epoch 13/20] [Batch 200/516] [D loss: 0.000824, acc: 100%] [G loss: 8.340470, adv: 0.979472, cyc: 0.292278, id: 0.265383] time: 1:16:44.644422 
[Epoch 13/20] [Batch 300/516] [D loss: 0.000446, acc: 100%] [G loss: 8.479546, adv: 0.986003, cyc: 0.297738, id: 0.252010] time: 1:17:54.587801 
[Epoch 13/20] [Batch 400/516] [D loss: 0.001379, acc: 100%] [G loss: 8.192911, adv: 0.989402, cyc: 0.284563, id: 0.251998] time: 1:19:04.537210 
[Epoch 13/20] [Batch 500/516] [D loss: 0.000276, acc: 100%] [G loss: 8.569078, adv: 0.988428, cyc: 0.301688, id: 0.265100] time: 1:20:14.499571 
[Epoch 14/20] [Batch 0/516] [D loss: 0.000435, acc: 100%] [G loss: 8.324119, adv: 0.982821, cyc: 0.291024, id: 0.253779] time: 1:20:25.692613 
[Epoch 14/20] [Batch 100/516] [D loss: 0.001217, acc: 100%] [G loss: 8.353261, adv: 0.989147, cyc: 0.291684, id: 0.268838] time: 1:21:35.652519 
[Epoch 14/20] [Batch 200/516] [D loss: 0.000640, acc: 100%] [G loss: 8.298798, adv: 0.962815, cyc: 0.291594, id: 0.284019] time: 1:22:45.613652 
[Epoch 14/20] [Batch 300/516] [D loss: 0.000525, acc: 100%] [G loss: 8.434001, adv: 0.993458, cyc: 0.295366, id: 0.257858] time: 1:23:55.570277 
[Epoch 14/20] [Batch 400/516] [D loss: 0.000408, acc: 100%] [G loss: 8.103205, adv: 0.982547, cyc: 0.281071, id: 0.249711] time: 1:25:05.526161 
[Epoch 14/20] [Batch 500/516] [D loss: 0.000352, acc: 100%] [G loss: 8.486978, adv: 0.960429, cyc: 0.299959, id: 0.258869] time: 1:26:15.478592 
[Epoch 15/20] [Batch 0/516] [D loss: 0.000369, acc: 100%] [G loss: 8.278352, adv: 0.978486, cyc: 0.289196, id: 0.250278] time: 1:26:26.674473 
[Epoch 15/20] [Batch 100/516] [D loss: 0.002390, acc: 100%] [G loss: 8.101416, adv: 0.834410, cyc: 0.294122, id: 0.289767] time: 1:27:36.639704 
[Epoch 15/20] [Batch 200/516] [D loss: 0.000437, acc: 100%] [G loss: 8.256289, adv: 0.982444, cyc: 0.288273, id: 0.259300] time: 1:28:46.590979 
[Epoch 15/20] [Batch 300/516] [D loss: 0.000284, acc: 100%] [G loss: 8.356081, adv: 0.974281, cyc: 0.293074, id: 0.264877] time: 1:29:56.548711 
[Epoch 15/20] [Batch 400/516] [D loss: 0.000500, acc: 100%] [G loss: 7.931128, adv: 0.922760, cyc: 0.278541, id: 0.249482] time: 1:31:06.512750 
[Epoch 15/20] [Batch 500/516] [D loss: 0.000745, acc: 100%] [G loss: 8.416197, adv: 0.970201, cyc: 0.295992, id: 0.255914] time: 1:32:16.461418 
[Epoch 16/20] [Batch 0/516] [D loss: 0.000343, acc: 100%] [G loss: 8.209829, adv: 0.955770, cyc: 0.287830, id: 0.240418] time: 1:32:30.250732 
[Epoch 16/20] [Batch 100/516] [D loss: 0.000645, acc: 100%] [G loss: 8.223641, adv: 0.975589, cyc: 0.287032, id: 0.265691] time: 1:33:40.209086 
[Epoch 16/20] [Batch 200/516] [D loss: 0.002476, acc: 100%] [G loss: 8.069259, adv: 0.891254, cyc: 0.287244, id: 0.274019] time: 1:34:50.166539 
[Epoch 16/20] [Batch 300/516] [D loss: 0.000320, acc: 100%] [G loss: 8.319616, adv: 0.989502, cyc: 0.289951, id: 0.259815] time: 1:36:00.123929 
[Epoch 16/20] [Batch 400/516] [D loss: 0.003533, acc:  99%] [G loss: 8.062116, adv: 0.999147, cyc: 0.277402, id: 0.248878] time: 1:37:10.075366 
[Epoch 16/20] [Batch 500/516] [D loss: 0.000370, acc: 100%] [G loss: 8.375809, adv: 0.889429, cyc: 0.300502, id: 0.252561] time: 1:38:20.020757 
[Epoch 17/20] [Batch 0/516] [D loss: 0.000745, acc: 100%] [G loss: 7.956543, adv: 0.796077, cyc: 0.290128, id: 0.242734] time: 1:38:31.213057 
[Epoch 17/20] [Batch 100/516] [D loss: 0.000418, acc: 100%] [G loss: 8.228720, adv: 0.959502, cyc: 0.287983, id: 0.262497] time: 1:39:41.169023 
[Epoch 17/20] [Batch 200/516] [D loss: 0.001562, acc: 100%] [G loss: 8.208685, adv: 0.986641, cyc: 0.284703, id: 0.253669] time: 1:40:51.131418 
[Epoch 17/20] [Batch 300/516] [D loss: 0.001281, acc: 100%] [G loss: 8.372178, adv: 1.022076, cyc: 0.289176, id: 0.245815] time: 1:42:01.098469 
[Epoch 17/20] [Batch 400/516] [D loss: 0.017887, acc:  98%] [G loss: 7.913581, adv: 0.940865, cyc: 0.275992, id: 0.244025] time: 1:43:11.053783 
[Epoch 17/20] [Batch 500/516] [D loss: 0.001004, acc: 100%] [G loss: 8.361483, adv: 0.980815, cyc: 0.292428, id: 0.249108] time: 1:44:21.003529 
[Epoch 18/20] [Batch 0/516] [D loss: 0.000531, acc: 100%] [G loss: 8.145073, adv: 0.980532, cyc: 0.282184, id: 0.244739] time: 1:44:32.196641 
[Epoch 18/20] [Batch 100/516] [D loss: 0.000593, acc: 100%] [G loss: 8.166513, adv: 0.987227, cyc: 0.282889, id: 0.263842] time: 1:45:42.148046 
[Epoch 18/20] [Batch 200/516] [D loss: 0.000142, acc: 100%] [G loss: 8.102530, adv: 0.960208, cyc: 0.282401, id: 0.277717] time: 1:46:52.103631 
[Epoch 18/20] [Batch 300/516] [D loss: 0.000198, acc: 100%] [G loss: 8.202049, adv: 0.959275, cyc: 0.286841, id: 0.249313] time: 1:48:02.050456 
[Epoch 18/20] [Batch 400/516] [D loss: 0.000326, acc: 100%] [G loss: 7.924463, adv: 0.969075, cyc: 0.273451, id: 0.263423] time: 1:49:11.998374 
[Epoch 18/20] [Batch 500/516] [D loss: 0.000769, acc: 100%] [G loss: 8.124825, adv: 0.868179, cyc: 0.291184, id: 0.246836] time: 1:50:21.953914 
[Epoch 19/20] [Batch 0/516] [D loss: 0.000366, acc: 100%] [G loss: 8.054171, adv: 0.936588, cyc: 0.281799, id: 0.238206] time: 1:50:33.147325 
[Epoch 19/20] [Batch 100/516] [D loss: 0.000711, acc: 100%] [G loss: 8.009647, adv: 0.937084, cyc: 0.280399, id: 0.256963] time: 1:51:43.109810 
[Epoch 19/20] [Batch 200/516] [D loss: 0.000559, acc: 100%] [G loss: 8.094858, adv: 0.953261, cyc: 0.282269, id: 0.247693] time: 1:52:53.067458 
[Epoch 19/20] [Batch 300/516] [D loss: 0.000786, acc: 100%] [G loss: 8.174034, adv: 0.942485, cyc: 0.286382, id: 0.240316] time: 1:54:03.019982 
[Epoch 19/20] [Batch 400/516] [D loss: 0.000244, acc: 100%] [G loss: 7.868815, adv: 0.975947, cyc: 0.270177, id: 0.254812] time: 1:55:12.972152 
[Epoch 19/20] [Batch 500/516] [D loss: 0.000453, acc: 100%] [G loss: 8.249201, adv: 0.958813, cyc: 0.288533, id: 0.261830] time: 1:56:22.928781 
[Epoch 20/20] [Batch 0/516] [D loss: 0.000295, acc: 100%] [G loss: 8.002148, adv: 0.950385, cyc: 0.278300, id: 0.239046] time: 1:56:34.124684 
[Epoch 20/20] [Batch 100/516] [D loss: 0.000168, acc: 100%] [G loss: 8.081508, adv: 0.955378, cyc: 0.281177, id: 0.255358] time: 1:57:44.074121 
[Epoch 20/20] [Batch 200/516] [D loss: 0.000546, acc: 100%] [G loss: 8.010296, adv: 0.970837, cyc: 0.276668, id: 0.260248] time: 1:58:54.025801 
[Epoch 20/20] [Batch 300/516] [D loss: 0.000492, acc: 100%] [G loss: 8.208361, adv: 0.995456, cyc: 0.283578, id: 0.266276] time: 2:00:03.977136 
[Epoch 20/20] [Batch 400/516] [D loss: 0.000671, acc: 100%] [G loss: 7.832160, adv: 0.936738, cyc: 0.271312, id: 0.242703] time: 2:01:13.936389 
[Epoch 20/20] [Batch 500/516] [D loss: 0.001232, acc: 100%] [G loss: 8.213911, adv: 0.940120, cyc: 0.288129, id: 0.254689] time: 2:02:23.894466 
Train Finished.
