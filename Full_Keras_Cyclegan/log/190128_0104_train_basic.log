_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         (None, 1, 600, 1)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 1, 600, 64)        256       
_________________________________________________________________
leaky_re_lu_5 (LeakyReLU)    (None, 1, 600, 64)        0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 1, 600, 128)       24704     
_________________________________________________________________
leaky_re_lu_6 (LeakyReLU)    (None, 1, 600, 128)       0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 76800)             0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               39322112  
_________________________________________________________________
leaky_re_lu_7 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 512)               262656    
_________________________________________________________________
leaky_re_lu_8 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
D_A_Out (Dense)              (None, 1)                 513       
=================================================================
Total params: 79,220,482
Trainable params: 39,610,241
Non-trainable params: 39,610,241
_________________________________________________________________
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 1, 600, 1)         0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 1, 600, 64)        256       
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU)    (None, 1, 600, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 1, 600, 128)       24704     
_________________________________________________________________
leaky_re_lu_2 (LeakyReLU)    (None, 1, 600, 128)       0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 76800)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               39322112  
_________________________________________________________________
leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
D_B_Out (Dense)              (None, 1)                 513       
=================================================================
Total params: 79,220,482
Trainable params: 39,610,241
Non-trainable params: 39,610,241
_________________________________________________________________
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
ivec_b (InputLayer)             (None, 1, 600, 1)    0                                            
__________________________________________________________________________________________________
ivec_a (InputLayer)             (None, 1, 600, 1)    0                                            
__________________________________________________________________________________________________
model_4 (Model)                 (None, 1, 600, 1)    653345      ivec_b[0][0]                     
                                                                 model_3[1][0]                    
                                                                 ivec_a[0][0]                     
__________________________________________________________________________________________________
model_3 (Model)                 (None, 1, 600, 1)    653345      ivec_a[0][0]                     
                                                                 model_4[1][0]                    
                                                                 ivec_b[0][0]                     
__________________________________________________________________________________________________
model_2 (Model)                 (None, 1)            39610241    model_4[1][0]                    
__________________________________________________________________________________________________
model_1 (Model)                 (None, 1)            39610241    model_3[1][0]                    
==================================================================================================
Total params: 80,527,172
Trainable params: 1,306,690
Non-trainable params: 79,220,482
__________________________________________________________________________________________________
Start CycleGAN training.
[Epoch 1/20] [Batch 0/516] [D loss: 1.742272, acc:   0%] [G loss: 22.067835, adv: 0.277419, cyc: 0.978166, id: 0.960639] time: 0:01:54.971244 
[Epoch 1/20] [Batch 100/516] [D loss: 0.171456, acc:  78%] [G loss: 15.219289, adv: 0.537406, cyc: 0.654391, id: 0.529010] time: 0:03:05.104875 
[Epoch 1/20] [Batch 200/516] [D loss: 0.082498, acc:  93%] [G loss: 13.112092, adv: 0.659606, cyc: 0.544731, id: 0.447941] time: 0:04:15.216522 
[Epoch 1/20] [Batch 300/516] [D loss: 0.014792, acc: 100%] [G loss: 12.166861, adv: 0.720902, cyc: 0.495543, id: 0.393706] time: 0:05:25.295205 
[Epoch 1/20] [Batch 400/516] [D loss: 0.038225, acc:  98%] [G loss: 11.496991, adv: 0.521963, cyc: 0.481974, id: 0.392521] time: 0:06:35.364545 
[Epoch 1/20] [Batch 500/516] [D loss: 0.013292, acc: 100%] [G loss: 11.618607, adv: 0.788063, cyc: 0.462491, id: 0.390753] time: 0:07:45.412616 
[Epoch 2/20] [Batch 0/516] [D loss: 0.016770, acc:  99%] [G loss: 11.795113, adv: 0.944641, cyc: 0.456360, id: 0.366217] time: 0:07:56.618196 
[Epoch 2/20] [Batch 100/516] [D loss: 0.005379, acc: 100%] [G loss: 11.522546, adv: 0.874551, cyc: 0.449840, id: 0.383239] time: 0:09:06.669066 
[Epoch 2/20] [Batch 200/516] [D loss: 0.007175, acc: 100%] [G loss: 11.075921, adv: 0.943026, cyc: 0.423213, id: 0.357740] time: 0:10:16.722278 
[Epoch 2/20] [Batch 300/516] [D loss: 0.014535, acc:  99%] [G loss: 10.303499, adv: 0.804638, cyc: 0.400458, id: 0.331337] time: 0:11:26.776836 
[Epoch 2/20] [Batch 400/516] [D loss: 0.032583, acc:  96%] [G loss: 10.762418, adv: 1.077589, cyc: 0.396159, id: 0.333188] time: 0:12:36.833443 
[Epoch 2/20] [Batch 500/516] [D loss: 0.005169, acc: 100%] [G loss: 10.437244, adv: 1.027399, cyc: 0.385969, id: 0.332791] time: 0:13:46.883138 
[Epoch 3/20] [Batch 0/516] [D loss: 0.006560, acc: 100%] [G loss: 10.259955, adv: 0.998510, cyc: 0.380377, id: 0.315068] time: 0:13:58.090902 
[Epoch 3/20] [Batch 100/516] [D loss: 0.002600, acc: 100%] [G loss: 10.535637, adv: 1.012589, cyc: 0.391239, id: 0.337809] time: 0:15:08.140606 
[Epoch 3/20] [Batch 200/516] [D loss: 0.004834, acc: 100%] [G loss: 10.125344, adv: 1.009218, cyc: 0.372985, id: 0.319754] time: 0:16:18.201357 
[Epoch 3/20] [Batch 300/516] [D loss: 0.002595, acc: 100%] [G loss: 9.863216, adv: 1.012442, cyc: 0.360552, id: 0.298653] time: 0:17:28.251255 
[Epoch 3/20] [Batch 400/516] [D loss: 0.005473, acc: 100%] [G loss: 9.819442, adv: 1.002209, cyc: 0.359643, id: 0.304961] time: 0:18:38.289716 
[Epoch 3/20] [Batch 500/516] [D loss: 0.025416, acc:  97%] [G loss: 9.685424, adv: 0.974697, cyc: 0.355981, id: 0.311797] time: 0:19:48.322546 
[Epoch 4/20] [Batch 0/516] [D loss: 0.002212, acc: 100%] [G loss: 9.609921, adv: 0.987638, cyc: 0.351222, id: 0.291456] time: 0:19:59.530426 
[Epoch 4/20] [Batch 100/516] [D loss: 0.011017, acc:  99%] [G loss: 9.713974, adv: 0.889748, cyc: 0.364909, id: 0.318925] time: 0:21:09.574646 
[Epoch 4/20] [Batch 200/516] [D loss: 0.002157, acc: 100%] [G loss: 9.653065, adv: 1.011690, cyc: 0.350935, id: 0.303259] time: 0:22:19.625115 
[Epoch 4/20] [Batch 300/516] [D loss: 0.002871, acc: 100%] [G loss: 9.319549, adv: 0.985450, cyc: 0.338020, id: 0.283446] time: 0:23:29.656226 
[Epoch 4/20] [Batch 400/516] [D loss: 0.005320, acc: 100%] [G loss: 9.476201, adv: 1.012671, cyc: 0.342273, id: 0.290455] time: 0:24:39.705878 
[Epoch 4/20] [Batch 500/516] [D loss: 0.000720, acc: 100%] [G loss: 9.329678, adv: 0.987266, cyc: 0.338163, id: 0.299779] time: 0:25:49.748930 
[Epoch 5/20] [Batch 0/516] [D loss: 0.002116, acc: 100%] [G loss: 9.251675, adv: 1.002915, cyc: 0.333395, id: 0.279353] time: 0:26:00.956722 
[Epoch 5/20] [Batch 100/516] [D loss: 0.001706, acc: 100%] [G loss: 9.545657, adv: 0.991431, cyc: 0.347298, id: 0.306556] time: 0:27:10.999234 
[Epoch 5/20] [Batch 200/516] [D loss: 0.001988, acc: 100%] [G loss: 9.254289, adv: 0.981306, cyc: 0.335142, id: 0.294222] time: 0:28:21.044761 
[Epoch 5/20] [Batch 300/516] [D loss: 0.004770, acc: 100%] [G loss: 9.082474, adv: 0.995772, cyc: 0.325937, id: 0.274914] time: 0:29:31.095597 
[Epoch 5/20] [Batch 400/516] [D loss: 0.004189, acc: 100%] [G loss: 9.049447, adv: 0.923738, cyc: 0.331188, id: 0.282464] time: 0:30:41.144983 
[Epoch 5/20] [Batch 500/516] [D loss: 0.000943, acc: 100%] [G loss: 9.154613, adv: 0.968213, cyc: 0.331642, id: 0.295321] time: 0:31:51.191547 
[Epoch 6/20] [Batch 0/516] [D loss: 0.000887, acc: 100%] [G loss: 8.990711, adv: 0.984879, cyc: 0.322835, id: 0.273510] time: 0:32:09.716610 
[Epoch 6/20] [Batch 100/516] [D loss: 0.001709, acc: 100%] [G loss: 9.343074, adv: 0.957728, cyc: 0.340931, id: 0.296361] time: 0:33:19.765212 
[Epoch 6/20] [Batch 200/516] [D loss: 0.000937, acc: 100%] [G loss: 9.092943, adv: 0.975804, cyc: 0.328037, id: 0.287217] time: 0:34:29.803924 
[Epoch 6/20] [Batch 300/516] [D loss: 0.001651, acc: 100%] [G loss: 8.901737, adv: 0.969355, cyc: 0.319316, id: 0.266626] time: 0:35:39.853126 
[Epoch 6/20] [Batch 400/516] [D loss: 0.002008, acc: 100%] [G loss: 9.033228, adv: 0.974358, cyc: 0.324885, id: 0.275582] time: 0:36:49.906341 
[Epoch 6/20] [Batch 500/516] [D loss: 0.005170, acc: 100%] [G loss: 8.955363, adv: 0.972952, cyc: 0.321411, id: 0.284870] time: 0:37:59.950579 
[Epoch 7/20] [Batch 0/516] [D loss: 0.000456, acc: 100%] [G loss: 8.798281, adv: 0.973224, cyc: 0.314588, id: 0.266444] time: 0:38:11.157323 
[Epoch 7/20] [Batch 100/516] [D loss: 0.003067, acc: 100%] [G loss: 9.195879, adv: 0.998119, cyc: 0.330363, id: 0.296184] time: 0:39:21.200704 
[Epoch 7/20] [Batch 200/516] [D loss: 0.003928, acc: 100%] [G loss: 9.004266, adv: 1.012142, cyc: 0.320162, id: 0.280906] time: 0:40:31.237891 
[Epoch 7/20] [Batch 300/516] [D loss: 0.000569, acc: 100%] [G loss: 8.769051, adv: 0.932090, cyc: 0.316071, id: 0.260692] time: 0:41:41.267148 
[Epoch 7/20] [Batch 400/516] [D loss: 0.002843, acc: 100%] [G loss: 8.876925, adv: 0.974281, cyc: 0.317594, id: 0.278449] time: 0:42:51.293522 
[Epoch 7/20] [Batch 500/516] [D loss: 0.001906, acc: 100%] [G loss: 8.781431, adv: 0.976346, cyc: 0.313359, id: 0.279925] time: 0:44:01.314861 
[Epoch 8/20] [Batch 0/516] [D loss: 0.000574, acc: 100%] [G loss: 8.649256, adv: 0.966120, cyc: 0.308398, id: 0.269562] time: 0:44:12.521394 
[Epoch 8/20] [Batch 100/516] [D loss: 0.001573, acc: 100%] [G loss: 9.094583, adv: 0.976338, cyc: 0.327606, id: 0.289418] time: 0:45:22.554879 
[Epoch 8/20] [Batch 200/516] [D loss: 0.001437, acc: 100%] [G loss: 8.867515, adv: 0.968280, cyc: 0.317541, id: 0.275635] time: 0:46:32.579406 
[Epoch 8/20] [Batch 300/516] [D loss: 0.002678, acc: 100%] [G loss: 8.550529, adv: 0.918754, cyc: 0.307410, id: 0.262852] time: 0:47:42.608707 
[Epoch 8/20] [Batch 400/516] [D loss: 0.002085, acc: 100%] [G loss: 8.742337, adv: 0.957503, cyc: 0.312741, id: 0.281653] time: 0:48:52.649097 
[Epoch 8/20] [Batch 500/516] [D loss: 0.000951, acc: 100%] [G loss: 8.679256, adv: 0.981948, cyc: 0.307976, id: 0.281696] time: 0:50:02.692189 
[Epoch 9/20] [Batch 0/516] [D loss: 0.012037, acc:  98%] [G loss: 8.613778, adv: 0.960106, cyc: 0.307132, id: 0.280407] time: 0:50:13.896992 
[Epoch 9/20] [Batch 100/516] [D loss: 0.002444, acc: 100%] [G loss: 8.749155, adv: 0.865612, cyc: 0.321228, id: 0.291107] time: 0:51:23.948155 
[Epoch 9/20] [Batch 200/516] [D loss: 0.002054, acc: 100%] [G loss: 8.663654, adv: 0.957738, cyc: 0.309346, id: 0.273589] time: 0:52:33.986025 
[Epoch 9/20] [Batch 300/516] [D loss: 0.001627, acc: 100%] [G loss: 8.446630, adv: 0.953519, cyc: 0.299781, id: 0.260918] time: 0:53:44.017817 
[Epoch 9/20] [Batch 400/516] [D loss: 0.000500, acc: 100%] [G loss: 8.641234, adv: 0.975553, cyc: 0.306213, id: 0.265194] time: 0:54:54.042082 
[Epoch 9/20] [Batch 500/516] [D loss: 0.000465, acc: 100%] [G loss: 8.621935, adv: 0.939593, cyc: 0.308399, id: 0.284509] time: 0:56:04.074420 
[Epoch 10/20] [Batch 0/516] [D loss: 0.003542, acc: 100%] [G loss: 8.541307, adv: 0.960714, cyc: 0.303139, id: 0.282779] time: 0:56:15.278347 
[Epoch 10/20] [Batch 100/516] [D loss: 0.001045, acc: 100%] [G loss: 8.854859, adv: 0.946629, cyc: 0.318457, id: 0.309535] time: 0:57:25.316204 
[Epoch 10/20] [Batch 200/516] [D loss: 0.000769, acc: 100%] [G loss: 8.728955, adv: 0.991049, cyc: 0.308670, id: 0.291395] time: 0:58:35.359853 
[Epoch 10/20] [Batch 300/516] [D loss: 0.009715, acc:  99%] [G loss: 8.524991, adv: 0.977546, cyc: 0.300189, id: 0.262287] time: 0:59:45.388710 
[Epoch 10/20] [Batch 400/516] [D loss: 0.001404, acc: 100%] [G loss: 8.585633, adv: 0.980886, cyc: 0.303079, id: 0.267766] time: 1:00:55.413360 
[Epoch 10/20] [Batch 500/516] [D loss: 0.000682, acc: 100%] [G loss: 8.503574, adv: 0.931819, cyc: 0.303644, id: 0.269967] time: 1:02:05.432823 
[Epoch 11/20] [Batch 0/516] [D loss: 0.000649, acc: 100%] [G loss: 8.373947, adv: 0.934232, cyc: 0.297669, id: 0.267568] time: 1:02:18.113776 
[Epoch 11/20] [Batch 100/516] [D loss: 0.001674, acc: 100%] [G loss: 8.885455, adv: 0.946868, cyc: 0.318994, id: 0.280485] time: 1:03:28.135640 
[Epoch 11/20] [Batch 200/516] [D loss: 0.002346, acc: 100%] [G loss: 8.603413, adv: 0.995915, cyc: 0.302392, id: 0.270796] time: 1:04:38.131085 
[Epoch 11/20] [Batch 300/516] [D loss: 0.000993, acc: 100%] [G loss: 8.328041, adv: 0.970819, cyc: 0.292363, id: 0.254145] time: 1:05:48.123417 
[Epoch 11/20] [Batch 400/516] [D loss: 0.000731, acc: 100%] [G loss: 8.343659, adv: 0.877971, cyc: 0.300886, id: 0.287505] time: 1:06:58.089254 
[Epoch 11/20] [Batch 500/516] [D loss: 0.001048, acc: 100%] [G loss: 8.479898, adv: 0.909759, cyc: 0.304257, id: 0.266326] time: 1:08:08.047646 
[Epoch 12/20] [Batch 0/516] [D loss: 0.001377, acc: 100%] [G loss: 8.371922, adv: 0.916942, cyc: 0.298458, id: 0.280207] time: 1:08:19.241358 
[Epoch 12/20] [Batch 100/516] [D loss: 0.002113, acc: 100%] [G loss: 8.893553, adv: 0.954912, cyc: 0.318291, id: 0.299403] time: 1:09:29.207620 
[Epoch 12/20] [Batch 200/516] [D loss: 0.008120, acc:  99%] [G loss: 8.234377, adv: 0.690368, cyc: 0.311999, id: 0.336200] time: 1:10:39.162386 
[Epoch 12/20] [Batch 300/516] [D loss: 0.000345, acc: 100%] [G loss: 8.226484, adv: 0.777349, cyc: 0.303282, id: 0.305369] time: 1:11:49.110832 
[Epoch 12/20] [Batch 400/516] [D loss: 0.000808, acc: 100%] [G loss: 8.347087, adv: 0.822600, cyc: 0.305170, id: 0.316744] time: 1:12:59.070588 
[Epoch 12/20] [Batch 500/516] [D loss: 0.000343, acc: 100%] [G loss: 8.524066, adv: 0.859764, cyc: 0.309457, id: 0.339499] time: 1:14:09.019513 
[Epoch 13/20] [Batch 0/516] [D loss: 0.000896, acc: 100%] [G loss: 8.356757, adv: 0.859267, cyc: 0.301771, id: 0.311010] time: 1:14:20.214940 
[Epoch 13/20] [Batch 100/516] [D loss: 0.000969, acc: 100%] [G loss: 8.576587, adv: 0.675428, cyc: 0.327312, id: 0.368589] time: 1:15:30.168477 
[Epoch 13/20] [Batch 200/516] [D loss: 0.000554, acc: 100%] [G loss: 7.752072, adv: 0.473461, cyc: 0.309438, id: 0.310949] time: 1:16:40.116017 
[Epoch 13/20] [Batch 300/516] [D loss: 0.001843, acc: 100%] [G loss: 7.614943, adv: 0.447588, cyc: 0.304212, id: 0.320677] time: 1:17:50.058018 
[Epoch 13/20] [Batch 400/516] [D loss: 0.000757, acc: 100%] [G loss: 7.760256, adv: 0.493938, cyc: 0.306994, id: 0.331773] time: 1:19:00.001926 
[Epoch 13/20] [Batch 500/516] [D loss: 0.001331, acc: 100%] [G loss: 8.216546, adv: 0.732591, cyc: 0.306297, id: 0.352071] time: 1:20:09.946472 
[Epoch 14/20] [Batch 0/516] [D loss: 0.000222, acc: 100%] [G loss: 8.108286, adv: 0.698151, cyc: 0.304500, id: 0.347327] time: 1:20:21.140722 
[Epoch 14/20] [Batch 100/516] [D loss: 0.002081, acc: 100%] [G loss: 8.323178, adv: 0.474865, cyc: 0.332924, id: 0.367545] time: 1:21:31.089249 
[Epoch 14/20] [Batch 200/516] [D loss: 0.000428, acc: 100%] [G loss: 8.228252, adv: 0.523104, cyc: 0.324472, id: 0.355020] time: 1:22:41.030085 
[Epoch 14/20] [Batch 300/516] [D loss: 0.001038, acc: 100%] [G loss: 7.769882, adv: 0.544589, cyc: 0.301858, id: 0.329776] time: 1:23:50.963681 
[Epoch 14/20] [Batch 400/516] [D loss: 0.000423, acc: 100%] [G loss: 7.714250, adv: 0.510621, cyc: 0.303382, id: 0.327913] time: 1:25:00.897456 
[Epoch 14/20] [Batch 500/516] [D loss: 0.001010, acc: 100%] [G loss: 7.790146, adv: 0.559876, cyc: 0.301398, id: 0.333520] time: 1:26:10.851175 
[Epoch 15/20] [Batch 0/516] [D loss: 0.000690, acc: 100%] [G loss: 7.525899, adv: 0.493168, cyc: 0.295964, id: 0.312202] time: 1:26:22.039864 
[Epoch 15/20] [Batch 100/516] [D loss: 0.000265, acc: 100%] [G loss: 7.877539, adv: 0.519300, cyc: 0.309094, id: 0.342319] time: 1:27:31.985928 
[Epoch 15/20] [Batch 200/516] [D loss: 0.000427, acc: 100%] [G loss: 7.688759, adv: 0.524079, cyc: 0.300546, id: 0.346582] time: 1:28:41.929052 
[Epoch 15/20] [Batch 300/516] [D loss: 0.000742, acc: 100%] [G loss: 7.386798, adv: 0.520745, cyc: 0.288613, id: 0.297224] time: 1:29:51.869691 
[Epoch 15/20] [Batch 400/516] [D loss: 0.000462, acc: 100%] [G loss: 7.644408, adv: 0.532728, cyc: 0.296907, id: 0.320930] time: 1:31:01.821452 
[Epoch 15/20] [Batch 500/516] [D loss: 0.000336, acc: 100%] [G loss: 7.541862, adv: 0.563645, cyc: 0.290818, id: 0.328717] time: 1:32:11.758185 
[Epoch 16/20] [Batch 0/516] [D loss: 0.000221, acc: 100%] [G loss: 7.455431, adv: 0.552260, cyc: 0.288351, id: 0.316860] time: 1:32:24.632322 
[Epoch 16/20] [Batch 100/516] [D loss: 0.000159, acc: 100%] [G loss: 7.795226, adv: 0.457697, cyc: 0.310116, id: 0.344636] time: 1:33:34.574102 
[Epoch 16/20] [Batch 200/516] [D loss: 0.000521, acc: 100%] [G loss: 7.745811, adv: 0.541569, cyc: 0.300541, id: 0.345924] time: 1:34:44.514464 
[Epoch 16/20] [Batch 300/516] [D loss: 0.000230, acc: 100%] [G loss: 7.348536, adv: 0.512827, cyc: 0.286749, id: 0.317271] time: 1:35:54.465399 
[Epoch 16/20] [Batch 400/516] [D loss: 0.000591, acc: 100%] [G loss: 7.454821, adv: 0.461298, cyc: 0.294920, id: 0.340280] time: 1:37:04.420946 
[Epoch 16/20] [Batch 500/516] [D loss: 0.000326, acc: 100%] [G loss: 7.506722, adv: 0.548651, cyc: 0.290132, id: 0.340582] time: 1:38:14.363526 
[Epoch 17/20] [Batch 0/516] [D loss: 0.000223, acc: 100%] [G loss: 7.361481, adv: 0.529374, cyc: 0.285546, id: 0.322123] time: 1:38:25.558687 
[Epoch 17/20] [Batch 100/516] [D loss: 0.000598, acc: 100%] [G loss: 7.635747, adv: 0.468197, cyc: 0.302694, id: 0.351713] time: 1:39:35.489494 
[Epoch 17/20] [Batch 200/516] [D loss: 0.000391, acc: 100%] [G loss: 7.557120, adv: 0.532508, cyc: 0.292887, id: 0.347679] time: 1:40:45.435646 
[Epoch 17/20] [Batch 300/516] [D loss: 0.000253, acc: 100%] [G loss: 7.257377, adv: 0.494121, cyc: 0.283656, id: 0.317370] time: 1:41:55.379684 
[Epoch 17/20] [Batch 400/516] [D loss: 0.000609, acc: 100%] [G loss: 7.401932, adv: 0.501063, cyc: 0.289648, id: 0.317250] time: 1:43:05.325943 
[Epoch 17/20] [Batch 500/516] [D loss: 0.003681, acc:  99%] [G loss: 6.904992, adv: 0.290829, cyc: 0.285870, id: 0.328520] time: 1:44:15.270409 
[Epoch 18/20] [Batch 0/516] [D loss: 0.000117, acc: 100%] [G loss: 7.381608, adv: 0.562468, cyc: 0.283186, id: 0.329127] time: 1:44:26.458950 
[Epoch 18/20] [Batch 100/516] [D loss: 0.002765, acc:  99%] [G loss: 7.846790, adv: 0.532069, cyc: 0.305253, id: 0.336897] time: 1:45:36.409129 
[Epoch 18/20] [Batch 200/516] [D loss: 0.000242, acc: 100%] [G loss: 7.359679, adv: 0.502457, cyc: 0.287768, id: 0.317396] time: 1:46:46.343363 
[Epoch 18/20] [Batch 300/516] [D loss: 0.000266, acc: 100%] [G loss: 7.150465, adv: 0.493613, cyc: 0.278320, id: 0.302313] time: 1:47:56.283918 
[Epoch 18/20] [Batch 400/516] [D loss: 0.000303, acc: 100%] [G loss: 7.197711, adv: 0.393335, cyc: 0.288804, id: 0.318855] time: 1:49:06.227817 
[Epoch 18/20] [Batch 500/516] [D loss: 0.000282, acc: 100%] [G loss: 7.439539, adv: 0.548940, cyc: 0.287375, id: 0.336980] time: 1:50:16.168880 
[Epoch 19/20] [Batch 0/516] [D loss: 0.000257, acc: 100%] [G loss: 7.217718, adv: 0.511777, cyc: 0.279743, id: 0.318731] time: 1:50:27.361490 
[Epoch 19/20] [Batch 100/516] [D loss: 0.000289, acc: 100%] [G loss: 7.577641, adv: 0.470125, cyc: 0.298786, id: 0.326347] time: 1:51:37.304479 
[Epoch 19/20] [Batch 200/516] [D loss: 0.000255, acc: 100%] [G loss: 7.250277, adv: 0.470385, cyc: 0.284813, id: 0.328832] time: 1:52:47.246532 
[Epoch 19/20] [Batch 300/516] [D loss: 0.003218, acc: 100%] [G loss: 7.275047, adv: 0.535524, cyc: 0.278849, id: 0.311826] time: 1:53:57.181434 
[Epoch 19/20] [Batch 400/516] [D loss: 0.000116, acc: 100%] [G loss: 7.325008, adv: 0.526606, cyc: 0.282945, id: 0.331314] time: 1:55:07.118907 
[Epoch 19/20] [Batch 500/516] [D loss: 0.000208, acc: 100%] [G loss: 7.239727, adv: 0.517036, cyc: 0.280299, id: 0.328155] time: 1:56:17.070239 
[Epoch 20/20] [Batch 0/516] [D loss: 0.001016, acc: 100%] [G loss: 7.172527, adv: 0.519837, cyc: 0.277109, id: 0.321202] time: 1:56:28.263870 
[Epoch 20/20] [Batch 100/516] [D loss: 0.000495, acc: 100%] [G loss: 7.670728, adv: 0.556409, cyc: 0.295995, id: 0.362004] time: 1:57:38.216137 
[Epoch 20/20] [Batch 200/516] [D loss: 0.000234, acc: 100%] [G loss: 7.279725, adv: 0.495872, cyc: 0.283635, id: 0.346633] time: 1:58:48.159463 
[Epoch 20/20] [Batch 300/516] [D loss: 0.000293, acc: 100%] [G loss: 7.048676, adv: 0.472432, cyc: 0.274729, id: 0.313904] time: 1:59:58.105878 
[Epoch 20/20] [Batch 400/516] [D loss: 0.000165, acc: 100%] [G loss: 7.175305, adv: 0.501955, cyc: 0.278733, id: 0.311211] time: 2:01:08.048887 
[Epoch 20/20] [Batch 500/516] [D loss: 0.000249, acc: 100%] [G loss: 7.262612, adv: 0.530131, cyc: 0.279346, id: 0.337549] time: 2:02:17.998239 
Train Finished.
