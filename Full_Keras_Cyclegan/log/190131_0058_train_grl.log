_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         (None, 1, 600, 1)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 1, 600, 64)        256       
_________________________________________________________________
leaky_re_lu_5 (LeakyReLU)    (None, 1, 600, 64)        0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 1, 600, 128)       24704     
_________________________________________________________________
leaky_re_lu_6 (LeakyReLU)    (None, 1, 600, 128)       0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 76800)             0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               39322112  
_________________________________________________________________
leaky_re_lu_7 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 512)               262656    
_________________________________________________________________
leaky_re_lu_8 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
D_A_Out (Dense)              (None, 1)                 513       
=================================================================
Total params: 79,220,482
Trainable params: 39,610,241
Non-trainable params: 39,610,241
_________________________________________________________________
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 1, 600, 1)         0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 1, 600, 64)        256       
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU)    (None, 1, 600, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 1, 600, 128)       24704     
_________________________________________________________________
leaky_re_lu_2 (LeakyReLU)    (None, 1, 600, 128)       0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 76800)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               39322112  
_________________________________________________________________
leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
D_B_Out (Dense)              (None, 1)                 513       
=================================================================
Total params: 79,220,482
Trainable params: 39,610,241
Non-trainable params: 39,610,241
_________________________________________________________________
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
ivec_b (InputLayer)             (None, 1, 600, 1)    0                                            
__________________________________________________________________________________________________
ivec_a (InputLayer)             (None, 1, 600, 1)    0                                            
__________________________________________________________________________________________________
model_4 (Model)                 (None, 1, 600, 1)    653345      ivec_b[0][0]                     
                                                                 model_3[1][0]                    
                                                                 ivec_a[0][0]                     
__________________________________________________________________________________________________
model_3 (Model)                 (None, 1, 600, 1)    653345      ivec_a[0][0]                     
                                                                 model_4[1][0]                    
                                                                 ivec_b[0][0]                     
__________________________________________________________________________________________________
model_2 (Model)                 (None, 1)            39610241    model_4[1][0]                    
__________________________________________________________________________________________________
model_1 (Model)                 (None, 1)            39610241    model_3[1][0]                    
==================================================================================================
Total params: 80,527,172
Trainable params: 1,306,690
Non-trainable params: 79,220,482
__________________________________________________________________________________________________
Start CycleGAN training.
[Epoch 1/30] [Batch 0/516] [D loss: 19.785212, acc:   0%] [G loss: 21.853992, adv: 0.702484, cyc: 0.927853, id: 1.010150] time: 0:01:58.572037 
[Epoch 1/30] [Batch 100/516] [D loss: 0.111495, acc:  85%] [G loss: 14.185515, adv: 0.824019, cyc: 0.579714, id: 0.517774] time: 0:03:07.425432 
[Epoch 1/30] [Batch 200/516] [D loss: 0.048108, acc:  93%] [G loss: 12.814506, adv: 0.956954, cyc: 0.504290, id: 0.450704] time: 0:04:16.255343 
[Epoch 1/30] [Batch 300/516] [D loss: 0.013907, acc: 100%] [G loss: 11.804564, adv: 0.852775, cyc: 0.466826, id: 0.418541] time: 0:05:25.072471 
[Epoch 1/30] [Batch 400/516] [D loss: 0.018965, acc: 100%] [G loss: 11.365142, adv: 0.955067, cyc: 0.437055, id: 0.384047] time: 0:06:33.880743 
[Epoch 1/30] [Batch 500/516] [D loss: 0.008134, acc: 100%] [G loss: 11.134199, adv: 0.928611, cyc: 0.428423, id: 0.391757] time: 0:07:42.672489 
[Epoch 2/30] [Batch 0/516] [D loss: 0.005499, acc: 100%] [G loss: 10.847235, adv: 0.768423, cyc: 0.429593, id: 0.394513] time: 0:07:53.678232 
[Epoch 2/30] [Batch 100/516] [D loss: 0.006051, acc: 100%] [G loss: 10.576027, adv: 0.877217, cyc: 0.406960, id: 0.375848] time: 0:09:02.460414 
[Epoch 2/30] [Batch 200/516] [D loss: 0.006145, acc: 100%] [G loss: 10.415852, adv: 0.883371, cyc: 0.398415, id: 0.373665] time: 0:10:11.246915 
[Epoch 2/30] [Batch 300/516] [D loss: 0.007742, acc: 100%] [G loss: 10.344608, adv: 0.967184, cyc: 0.387556, id: 0.367044] time: 0:11:20.021295 
[Epoch 2/30] [Batch 400/516] [D loss: 0.004548, acc: 100%] [G loss: 9.875316, adv: 0.871000, cyc: 0.374447, id: 0.348264] time: 0:12:28.806088 
[Epoch 2/30] [Batch 500/516] [D loss: 0.017402, acc:  99%] [G loss: 10.150585, adv: 1.092986, cyc: 0.367609, id: 0.335537] time: 0:13:37.584005 
[Epoch 3/30] [Batch 0/516] [D loss: 0.004448, acc: 100%] [G loss: 9.971529, adv: 0.905498, cyc: 0.375839, id: 0.351358] time: 0:13:48.590712 
[Epoch 3/30] [Batch 100/516] [D loss: 0.004512, acc: 100%] [G loss: 9.719484, adv: 0.920215, cyc: 0.362990, id: 0.340636] time: 0:14:57.366942 
[Epoch 3/30] [Batch 200/516] [D loss: 0.002821, acc: 100%] [G loss: 9.465681, adv: 0.861401, cyc: 0.356466, id: 0.335388] time: 0:16:06.137209 
[Epoch 3/30] [Batch 300/516] [D loss: 0.002692, acc: 100%] [G loss: 9.457081, adv: 0.915813, cyc: 0.351141, id: 0.337321] time: 0:17:14.914711 
[Epoch 3/30] [Batch 400/516] [D loss: 0.004216, acc: 100%] [G loss: 9.332570, adv: 0.944337, cyc: 0.342997, id: 0.319301] time: 0:18:23.680912 
[Epoch 3/30] [Batch 500/516] [D loss: 0.003738, acc: 100%] [G loss: 9.457482, adv: 1.025931, cyc: 0.341246, id: 0.321903] time: 0:19:32.457588 
[Epoch 4/30] [Batch 0/516] [D loss: 0.001940, acc: 100%] [G loss: 9.364156, adv: 0.896962, cyc: 0.348305, id: 0.337205] time: 0:19:43.463195 
[Epoch 4/30] [Batch 100/516] [D loss: 0.002153, acc: 100%] [G loss: 9.153086, adv: 0.902686, cyc: 0.338567, id: 0.318011] time: 0:20:52.238371 
[Epoch 4/30] [Batch 200/516] [D loss: 0.002438, acc: 100%] [G loss: 9.133570, adv: 0.919590, cyc: 0.335874, id: 0.315434] time: 0:22:01.014452 
[Epoch 4/30] [Batch 300/516] [D loss: 0.025978, acc:  96%] [G loss: 9.335467, adv: 1.078305, cyc: 0.330514, id: 0.319766] time: 0:23:09.803030 
[Epoch 4/30] [Batch 400/516] [D loss: 0.019648, acc:  96%] [G loss: 9.115644, adv: 1.031249, cyc: 0.324920, id: 0.304562] time: 0:24:18.586582 
[Epoch 4/30] [Batch 500/516] [D loss: 0.003537, acc: 100%] [G loss: 8.935133, adv: 0.946540, cyc: 0.324181, id: 0.307154] time: 0:25:27.362775 
[Epoch 5/30] [Batch 0/516] [D loss: 0.002518, acc: 100%] [G loss: 9.186561, adv: 1.005594, cyc: 0.330108, id: 0.316796] time: 0:25:38.368047 
[Epoch 5/30] [Batch 100/516] [D loss: 0.005061, acc: 100%] [G loss: 8.936419, adv: 0.941225, cyc: 0.324681, id: 0.313494] time: 0:26:47.154386 
[Epoch 5/30] [Batch 200/516] [D loss: 0.001475, acc: 100%] [G loss: 8.809209, adv: 0.911722, cyc: 0.321454, id: 0.306115] time: 0:27:55.943354 
[Epoch 5/30] [Batch 300/516] [D loss: 0.001786, acc: 100%] [G loss: 8.788378, adv: 0.940329, cyc: 0.317622, id: 0.308523] time: 0:29:04.728389 
[Epoch 5/30] [Batch 400/516] [D loss: 0.002393, acc: 100%] [G loss: 8.769071, adv: 0.966889, cyc: 0.314757, id: 0.296139] time: 0:30:13.510755 
[Epoch 5/30] [Batch 500/516] [D loss: 0.002432, acc: 100%] [G loss: 8.860942, adv: 0.996629, cyc: 0.316390, id: 0.295676] time: 0:31:22.292019 
[Epoch 6/30] [Batch 0/516] [D loss: 0.000985, acc: 100%] [G loss: 8.891308, adv: 0.960271, cyc: 0.320501, id: 0.308586] time: 0:31:39.062627 
[Epoch 6/30] [Batch 100/516] [D loss: 0.002214, acc: 100%] [G loss: 8.732441, adv: 0.973337, cyc: 0.312405, id: 0.297919] time: 0:32:47.858298 
[Epoch 6/30] [Batch 200/516] [D loss: 0.001634, acc: 100%] [G loss: 8.721955, adv: 0.923926, cyc: 0.315815, id: 0.302752] time: 0:33:56.648215 
[Epoch 6/30] [Batch 300/516] [D loss: 0.005691, acc: 100%] [G loss: 8.865592, adv: 1.055952, cyc: 0.310709, id: 0.301565] time: 0:35:05.433703 
[Epoch 6/30] [Batch 400/516] [D loss: 0.002176, acc: 100%] [G loss: 8.563807, adv: 0.963760, cyc: 0.305711, id: 0.286221] time: 0:36:14.223301 
[Epoch 6/30] [Batch 500/516] [D loss: 0.003431, acc: 100%] [G loss: 8.503307, adv: 0.927863, cyc: 0.306001, id: 0.293305] time: 0:37:23.014058 
[Epoch 7/30] [Batch 0/516] [D loss: 0.002420, acc: 100%] [G loss: 8.664627, adv: 0.951373, cyc: 0.311110, id: 0.303195] time: 0:37:34.020313 
[Epoch 7/30] [Batch 100/516] [D loss: 0.002888, acc: 100%] [G loss: 8.596205, adv: 0.981395, cyc: 0.305366, id: 0.296493] time: 0:38:42.803447 
[Epoch 7/30] [Batch 200/516] [D loss: 0.001315, acc: 100%] [G loss: 8.497252, adv: 0.928341, cyc: 0.305257, id: 0.289637] time: 0:39:51.589902 
[Epoch 7/30] [Batch 300/516] [D loss: 0.003629, acc: 100%] [G loss: 8.557666, adv: 1.003882, cyc: 0.301735, id: 0.292919] time: 0:41:00.384499 
[Epoch 7/30] [Batch 400/516] [D loss: 0.001964, acc: 100%] [G loss: 8.467716, adv: 0.987444, cyc: 0.299050, id: 0.281125] time: 0:42:09.174730 
[Epoch 7/30] [Batch 500/516] [D loss: 0.001303, acc: 100%] [G loss: 8.357290, adv: 0.924710, cyc: 0.299341, id: 0.287493] time: 0:43:17.962125 
[Epoch 8/30] [Batch 0/516] [D loss: 0.001391, acc: 100%] [G loss: 8.518146, adv: 0.938009, cyc: 0.305537, id: 0.292799] time: 0:43:28.968745 
[Epoch 8/30] [Batch 100/516] [D loss: 0.001696, acc: 100%] [G loss: 8.357514, adv: 0.940834, cyc: 0.297990, id: 0.292838] time: 0:44:37.762276 
[Epoch 8/30] [Batch 200/516] [D loss: 0.002221, acc: 100%] [G loss: 8.304685, adv: 0.918363, cyc: 0.297452, id: 0.290070] time: 0:45:46.547538 
[Epoch 8/30] [Batch 300/516] [D loss: 0.003762, acc: 100%] [G loss: 8.444508, adv: 1.008275, cyc: 0.296175, id: 0.285968] time: 0:46:55.337829 
[Epoch 8/30] [Batch 400/516] [D loss: 0.000387, acc: 100%] [G loss: 8.333742, adv: 0.999537, cyc: 0.291748, id: 0.277271] time: 0:48:04.119160 
[Epoch 8/30] [Batch 500/516] [D loss: 0.002788, acc: 100%] [G loss: 8.323730, adv: 0.988688, cyc: 0.291795, id: 0.284874] time: 0:49:12.911154 
[Epoch 9/30] [Batch 0/516] [D loss: 0.000733, acc: 100%] [G loss: 8.411472, adv: 0.968911, cyc: 0.297513, id: 0.295312] time: 0:49:23.918513 
[Epoch 9/30] [Batch 100/516] [D loss: 0.001230, acc: 100%] [G loss: 8.403728, adv: 0.993534, cyc: 0.294896, id: 0.294990] time: 0:50:32.713231 
[Epoch 9/30] [Batch 200/516] [D loss: 0.000483, acc: 100%] [G loss: 8.294353, adv: 0.939914, cyc: 0.294708, id: 0.283842] time: 0:51:41.506235 
[Epoch 9/30] [Batch 300/516] [D loss: 0.000485, acc: 100%] [G loss: 8.323414, adv: 0.997022, cyc: 0.291234, id: 0.289016] time: 0:52:50.298608 
[Epoch 9/30] [Batch 400/516] [D loss: 0.001186, acc: 100%] [G loss: 8.056496, adv: 0.925602, cyc: 0.285578, id: 0.274629] time: 0:53:59.092272 
[Epoch 9/30] [Batch 500/516] [D loss: 0.000765, acc: 100%] [G loss: 8.105066, adv: 0.925334, cyc: 0.287627, id: 0.278276] time: 0:55:07.890413 
[Epoch 10/30] [Batch 0/516] [D loss: 0.001870, acc: 100%] [G loss: 8.236300, adv: 0.937522, cyc: 0.292079, id: 0.291617] time: 0:55:18.895738 
[Epoch 10/30] [Batch 100/516] [D loss: 0.001327, acc: 100%] [G loss: 8.235753, adv: 0.993011, cyc: 0.287199, id: 0.288321] time: 0:56:27.674331 
[Epoch 10/30] [Batch 200/516] [D loss: 0.000381, acc: 100%] [G loss: 8.129851, adv: 0.934192, cyc: 0.287791, id: 0.279883] time: 0:57:36.472371 
[Epoch 10/30] [Batch 300/516] [D loss: 0.002168, acc: 100%] [G loss: 8.192916, adv: 0.968860, cyc: 0.287777, id: 0.285812] time: 0:58:45.258258 
[Epoch 10/30] [Batch 400/516] [D loss: 0.001129, acc: 100%] [G loss: 8.119020, adv: 1.000878, cyc: 0.281214, id: 0.277629] time: 0:59:54.045692 
[Epoch 10/30] [Batch 500/516] [D loss: 0.000704, acc: 100%] [G loss: 8.109618, adv: 0.982803, cyc: 0.282256, id: 0.279699] time: 1:01:02.834084 
[Epoch 11/30] [Batch 0/516] [D loss: 0.000646, acc: 100%] [G loss: 8.223353, adv: 0.969710, cyc: 0.288833, id: 0.286557] time: 1:01:14.957993 
[Epoch 11/30] [Batch 100/516] [D loss: 0.000567, acc: 100%] [G loss: 8.096972, adv: 0.982045, cyc: 0.281757, id: 0.284919] time: 1:02:23.756939 
[Epoch 11/30] [Batch 200/516] [D loss: 0.000869, acc: 100%] [G loss: 8.199084, adv: 1.011262, cyc: 0.283677, id: 0.282222] time: 1:03:32.543937 
[Epoch 11/30] [Batch 300/516] [D loss: 0.000858, acc: 100%] [G loss: 8.080786, adv: 0.964817, cyc: 0.282925, id: 0.280752] time: 1:04:41.325803 
[Epoch 11/30] [Batch 400/516] [D loss: 0.000363, acc: 100%] [G loss: 7.936401, adv: 0.947164, cyc: 0.277470, id: 0.277392] time: 1:05:50.114005 
[Epoch 11/30] [Batch 500/516] [D loss: 0.001480, acc: 100%] [G loss: 7.957542, adv: 0.936953, cyc: 0.279733, id: 0.273515] time: 1:06:58.906879 
[Epoch 12/30] [Batch 0/516] [D loss: 0.000780, acc: 100%] [G loss: 8.175837, adv: 0.991280, cyc: 0.284483, id: 0.285389] time: 1:07:09.911312 
[Epoch 12/30] [Batch 100/516] [D loss: 0.001221, acc: 100%] [G loss: 7.981194, adv: 0.916871, cyc: 0.282284, id: 0.275221] time: 1:08:18.694384 
[Epoch 12/30] [Batch 200/516] [D loss: 0.000380, acc: 100%] [G loss: 7.985502, adv: 0.966887, cyc: 0.277955, id: 0.279795] time: 1:09:27.478071 
[Epoch 12/30] [Batch 300/516] [D loss: 0.000539, acc: 100%] [G loss: 7.971646, adv: 0.963599, cyc: 0.277803, id: 0.279891] time: 1:10:36.274182 
[Epoch 12/30] [Batch 400/516] [D loss: 0.000798, acc: 100%] [G loss: 7.923220, adv: 0.963812, cyc: 0.275816, id: 0.267726] time: 1:11:45.067813 
[Epoch 12/30] [Batch 500/516] [D loss: 0.000786, acc: 100%] [G loss: 7.977611, adv: 0.992642, cyc: 0.275274, id: 0.271017] time: 1:12:53.859618 
[Epoch 13/30] [Batch 0/516] [D loss: 0.000649, acc: 100%] [G loss: 8.075315, adv: 0.965777, cyc: 0.282226, id: 0.284942] time: 1:13:04.866308 
[Epoch 13/30] [Batch 100/516] [D loss: 0.001764, acc: 100%] [G loss: 7.977763, adv: 0.950168, cyc: 0.279165, id: 0.279673] time: 1:14:13.643167 
[Epoch 13/30] [Batch 200/516] [D loss: 0.002551, acc: 100%] [G loss: 7.909937, adv: 0.944700, cyc: 0.276993, id: 0.272555] time: 1:15:22.424074 
[Epoch 13/30] [Batch 300/516] [D loss: 0.000539, acc: 100%] [G loss: 7.966725, adv: 0.966246, cyc: 0.277139, id: 0.281337] time: 1:16:31.205270 
[Epoch 13/30] [Batch 400/516] [D loss: 0.000219, acc: 100%] [G loss: 7.809402, adv: 0.955804, cyc: 0.271169, id: 0.264556] time: 1:17:39.984802 
[Epoch 13/30] [Batch 500/516] [D loss: 0.001057, acc: 100%] [G loss: 7.900446, adv: 0.965681, cyc: 0.274077, id: 0.272616] time: 1:18:48.765698 
[Epoch 14/30] [Batch 0/516] [D loss: 0.000505, acc: 100%] [G loss: 7.978130, adv: 0.931713, cyc: 0.280623, id: 0.277105] time: 1:18:59.767366 
[Epoch 14/30] [Batch 100/516] [D loss: 0.000552, acc: 100%] [G loss: 7.883657, adv: 0.956401, cyc: 0.274000, id: 0.274637] time: 1:20:08.549027 
[Epoch 14/30] [Batch 200/516] [D loss: 0.000081, acc: 100%] [G loss: 7.867629, adv: 0.938772, cyc: 0.274934, id: 0.278036] time: 1:21:17.332567 
[Epoch 14/30] [Batch 300/516] [D loss: 0.000505, acc: 100%] [G loss: 8.045527, adv: 1.024867, cyc: 0.275629, id: 0.277199] time: 1:22:26.113303 
[Epoch 14/30] [Batch 400/516] [D loss: 0.000502, acc: 100%] [G loss: 7.884789, adv: 0.994454, cyc: 0.271155, id: 0.265388] time: 1:23:34.900932 
[Epoch 14/30] [Batch 500/516] [D loss: 0.000639, acc: 100%] [G loss: 7.838547, adv: 0.944535, cyc: 0.273002, id: 0.269249] time: 1:24:43.681520 
[Epoch 15/30] [Batch 0/516] [D loss: 0.001024, acc: 100%] [G loss: 7.964728, adv: 0.964738, cyc: 0.276696, id: 0.283809] time: 1:24:54.685613 
[Epoch 15/30] [Batch 100/516] [D loss: 0.000773, acc: 100%] [G loss: 7.717771, adv: 0.888147, cyc: 0.272625, id: 0.275590] time: 1:26:03.470248 
[Epoch 15/30] [Batch 200/516] [D loss: 0.000447, acc: 100%] [G loss: 7.813856, adv: 0.918929, cyc: 0.274074, id: 0.270981] time: 1:27:12.263832 
[Epoch 15/30] [Batch 300/516] [D loss: 0.000628, acc: 100%] [G loss: 7.779677, adv: 0.925274, cyc: 0.272274, id: 0.277277] time: 1:28:21.052711 
[Epoch 15/30] [Batch 400/516] [D loss: 0.000612, acc: 100%] [G loss: 7.788995, adv: 0.974763, cyc: 0.267779, id: 0.272425] time: 1:29:29.830254 
[Epoch 15/30] [Batch 500/516] [D loss: 0.000311, acc: 100%] [G loss: 7.826545, adv: 0.977956, cyc: 0.269684, id: 0.265527] time: 1:30:38.608041 
[Epoch 16/30] [Batch 0/516] [D loss: 0.000692, acc: 100%] [G loss: 7.888145, adv: 0.946003, cyc: 0.275003, id: 0.273941] time: 1:30:51.093378 
[Epoch 16/30] [Batch 100/516] [D loss: 0.000475, acc: 100%] [G loss: 7.793795, adv: 0.955274, cyc: 0.270117, id: 0.272381] time: 1:31:59.879329 
[Epoch 16/30] [Batch 200/516] [D loss: 0.000179, acc: 100%] [G loss: 7.735216, adv: 0.932967, cyc: 0.269063, id: 0.274618] time: 1:33:08.661451 
[Epoch 16/30] [Batch 300/516] [D loss: 0.000315, acc: 100%] [G loss: 7.756712, adv: 0.953067, cyc: 0.268326, id: 0.277554] time: 1:34:17.445658 
[Epoch 16/30] [Batch 400/516] [D loss: 0.000587, acc: 100%] [G loss: 7.731109, adv: 0.968757, cyc: 0.265675, id: 0.270232] time: 1:35:26.222046 
[Epoch 16/30] [Batch 500/516] [D loss: 0.000328, acc: 100%] [G loss: 7.813406, adv: 0.991134, cyc: 0.267346, id: 0.270021] time: 1:36:35.014833 
[Epoch 17/30] [Batch 0/516] [D loss: 0.000517, acc: 100%] [G loss: 7.919170, adv: 0.970874, cyc: 0.274069, id: 0.276181] time: 1:36:46.018560 
[Epoch 17/30] [Batch 100/516] [D loss: 0.000125, acc: 100%] [G loss: 7.744546, adv: 0.966662, cyc: 0.266532, id: 0.269708] time: 1:37:54.807064 
[Epoch 17/30] [Batch 200/516] [D loss: 0.000736, acc: 100%] [G loss: 7.715333, adv: 0.938395, cyc: 0.267625, id: 0.268763] time: 1:39:03.593821 
[Epoch 17/30] [Batch 300/516] [D loss: 0.000315, acc: 100%] [G loss: 7.749699, adv: 0.982044, cyc: 0.265661, id: 0.270804] time: 1:40:12.376904 
[Epoch 17/30] [Batch 400/516] [D loss: 0.000783, acc: 100%] [G loss: 7.588599, adv: 0.918398, cyc: 0.263792, id: 0.266518] time: 1:41:21.162377 
[Epoch 17/30] [Batch 500/516] [D loss: 0.000513, acc: 100%] [G loss: 7.704520, adv: 0.957820, cyc: 0.265199, id: 0.276714] time: 1:42:29.946865 
[Epoch 18/30] [Batch 0/516] [D loss: 0.002688, acc: 100%] [G loss: 8.016309, adv: 1.065285, cyc: 0.269712, id: 0.283907] time: 1:42:40.951968 
[Epoch 18/30] [Batch 100/516] [D loss: 0.000661, acc: 100%] [G loss: 7.742213, adv: 0.972337, cyc: 0.265654, id: 0.271370] time: 1:43:49.729037 
[Epoch 18/30] [Batch 200/516] [D loss: 0.000895, acc: 100%] [G loss: 7.704308, adv: 0.959755, cyc: 0.265064, id: 0.268053] time: 1:44:58.505540 
[Epoch 18/30] [Batch 300/516] [D loss: 0.002091, acc:  99%] [G loss: 7.768533, adv: 0.987795, cyc: 0.266032, id: 0.268604] time: 1:46:07.282070 
[Epoch 18/30] [Batch 400/516] [D loss: 0.000648, acc: 100%] [G loss: 7.681886, adv: 0.994289, cyc: 0.261155, id: 0.264637] time: 1:47:16.058605 
[Epoch 18/30] [Batch 500/516] [D loss: 0.000109, acc: 100%] [G loss: 7.700213, adv: 0.990939, cyc: 0.261987, id: 0.273836] time: 1:48:24.832164 
[Epoch 19/30] [Batch 0/516] [D loss: 0.000094, acc: 100%] [G loss: 7.779409, adv: 0.968008, cyc: 0.267867, id: 0.274185] time: 1:48:35.834225 
[Epoch 19/30] [Batch 100/516] [D loss: 0.000187, acc: 100%] [G loss: 7.719656, adv: 0.980570, cyc: 0.263941, id: 0.272787] time: 1:49:44.606889 
[Epoch 19/30] [Batch 200/516] [D loss: 0.000055, acc: 100%] [G loss: 7.594006, adv: 0.916149, cyc: 0.263969, id: 0.267010] time: 1:50:53.360645 
[Epoch 19/30] [Batch 300/516] [D loss: 0.000168, acc: 100%] [G loss: 7.727530, adv: 0.979501, cyc: 0.264197, id: 0.268785] time: 1:52:02.099934 
[Epoch 19/30] [Batch 400/516] [D loss: 0.000151, acc: 100%] [G loss: 7.645408, adv: 0.994320, cyc: 0.259379, id: 0.265386] time: 1:53:10.821945 
[Epoch 19/30] [Batch 500/516] [D loss: 0.000312, acc: 100%] [G loss: 7.566103, adv: 0.930127, cyc: 0.261353, id: 0.270353] time: 1:54:19.533202 
[Epoch 20/30] [Batch 0/516] [D loss: 0.000304, acc: 100%] [G loss: 7.778987, adv: 0.975511, cyc: 0.266831, id: 0.272707] time: 1:54:30.526813 
[Epoch 20/30] [Batch 100/516] [D loss: 0.000147, acc: 100%] [G loss: 7.665277, adv: 0.969442, cyc: 0.262492, id: 0.270444] time: 1:55:39.234220 
[Epoch 20/30] [Batch 200/516] [D loss: 0.000115, acc: 100%] [G loss: 7.586203, adv: 0.953487, cyc: 0.260261, id: 0.261956] time: 1:56:47.946330 
[Epoch 20/30] [Batch 300/516] [D loss: 0.000097, acc: 100%] [G loss: 7.632568, adv: 0.974955, cyc: 0.260548, id: 0.270275] time: 1:57:56.664047 
[Epoch 20/30] [Batch 400/516] [D loss: 0.000112, acc: 100%] [G loss: 7.564134, adv: 0.988345, cyc: 0.256080, id: 0.266493] time: 1:59:05.374136 
[Epoch 20/30] [Batch 500/516] [D loss: 0.000407, acc: 100%] [G loss: 7.633835, adv: 0.983254, cyc: 0.259350, id: 0.272472] time: 2:00:14.087037 
[Epoch 21/30] [Batch 0/516] [D loss: 0.000218, acc: 100%] [G loss: 7.707958, adv: 0.920209, cyc: 0.268464, id: 0.269428] time: 2:00:26.217304 
[Epoch 21/30] [Batch 100/516] [D loss: 0.000228, acc: 100%] [G loss: 7.590314, adv: 0.956332, cyc: 0.260261, id: 0.267910] time: 2:01:34.934446 
[Epoch 21/30] [Batch 200/516] [D loss: 0.000065, acc: 100%] [G loss: 7.600384, adv: 0.973906, cyc: 0.258810, id: 0.270509] time: 2:02:43.653808 
[Epoch 21/30] [Batch 300/516] [D loss: 0.000136, acc: 100%] [G loss: 7.571445, adv: 0.964548, cyc: 0.258739, id: 0.265738] time: 2:03:52.377322 
[Epoch 21/30] [Batch 400/516] [D loss: 0.000455, acc: 100%] [G loss: 7.585021, adv: 1.009950, cyc: 0.255048, id: 0.263281] time: 2:05:01.090955 
[Epoch 21/30] [Batch 500/516] [D loss: 0.000231, acc: 100%] [G loss: 7.655822, adv: 0.985011, cyc: 0.260093, id: 0.264519] time: 2:06:09.809284 
[Epoch 22/30] [Batch 0/516] [D loss: 0.000353, acc: 100%] [G loss: 7.796682, adv: 0.983429, cyc: 0.266271, id: 0.270165] time: 2:06:20.805934 
[Epoch 22/30] [Batch 100/516] [D loss: 0.000217, acc: 100%] [G loss: 7.573112, adv: 0.930771, cyc: 0.261231, id: 0.259142] time: 2:07:29.529327 
[Epoch 22/30] [Batch 200/516] [D loss: 0.000134, acc: 100%] [G loss: 7.604839, adv: 0.992836, cyc: 0.257220, id: 0.270188] time: 2:08:38.244511 
[Epoch 22/30] [Batch 300/516] [D loss: 0.000143, acc: 100%] [G loss: 7.584520, adv: 0.980020, cyc: 0.257754, id: 0.266060] time: 2:09:46.965658 
[Epoch 22/30] [Batch 400/516] [D loss: 0.000091, acc: 100%] [G loss: 7.535502, adv: 0.982451, cyc: 0.254764, id: 0.260301] time: 2:10:55.690321 
[Epoch 22/30] [Batch 500/516] [D loss: 0.000084, acc: 100%] [G loss: 7.626211, adv: 0.991725, cyc: 0.258069, id: 0.255734] time: 2:12:04.420348 
[Epoch 23/30] [Batch 0/516] [D loss: 0.000373, acc: 100%] [G loss: 7.652218, adv: 0.967058, cyc: 0.261434, id: 0.271304] time: 2:12:15.417881 
[Epoch 23/30] [Batch 100/516] [D loss: 0.000188, acc: 100%] [G loss: 7.521688, adv: 0.931044, cyc: 0.258645, id: 0.260610] time: 2:13:24.145722 
[Epoch 23/30] [Batch 200/516] [D loss: 0.000285, acc: 100%] [G loss: 7.450545, adv: 0.930216, cyc: 0.255774, id: 0.266980] time: 2:14:32.865437 
[Epoch 23/30] [Batch 300/516] [D loss: 0.000209, acc: 100%] [G loss: 7.539711, adv: 0.967212, cyc: 0.256208, id: 0.271006] time: 2:15:41.590386 
[Epoch 23/30] [Batch 400/516] [D loss: 0.000171, acc: 100%] [G loss: 7.486846, adv: 0.961325, cyc: 0.254897, id: 0.260364] time: 2:16:50.302197 
[Epoch 23/30] [Batch 500/516] [D loss: 0.000581, acc: 100%] [G loss: 7.433248, adv: 0.935385, cyc: 0.254465, id: 0.270420] time: 2:17:59.021156 
[Epoch 24/30] [Batch 0/516] [D loss: 0.000056, acc: 100%] [G loss: 7.670105, adv: 0.994792, cyc: 0.259440, id: 0.287387] time: 2:18:10.015665 
[Epoch 24/30] [Batch 100/516] [D loss: 0.000275, acc: 100%] [G loss: 7.600459, adv: 0.992394, cyc: 0.256904, id: 0.262918] time: 2:19:18.728344 
[Epoch 24/30] [Batch 200/516] [D loss: 0.000113, acc: 100%] [G loss: 7.492867, adv: 0.974959, cyc: 0.253758, id: 0.272379] time: 2:20:27.450715 
[Epoch 24/30] [Batch 300/516] [D loss: 0.001262, acc: 100%] [G loss: 7.502659, adv: 0.969462, cyc: 0.254997, id: 0.261400] time: 2:21:36.184470 
[Epoch 24/30] [Batch 400/516] [D loss: 0.000154, acc: 100%] [G loss: 7.410900, adv: 0.970741, cyc: 0.250328, id: 0.258012] time: 2:22:44.905960 
[Epoch 24/30] [Batch 500/516] [D loss: 0.000084, acc: 100%] [G loss: 7.472531, adv: 0.971573, cyc: 0.252699, id: 0.267936] time: 2:23:53.622453 
[Epoch 25/30] [Batch 0/516] [D loss: 0.000215, acc: 100%] [G loss: 7.604554, adv: 0.978548, cyc: 0.258205, id: 0.271351] time: 2:24:04.618743 
[Epoch 25/30] [Batch 100/516] [D loss: 0.000102, acc: 100%] [G loss: 7.525201, adv: 0.999010, cyc: 0.252606, id: 0.270116] time: 2:25:13.340394 
[Epoch 25/30] [Batch 200/516] [D loss: 0.000111, acc: 100%] [G loss: 7.485415, adv: 0.990753, cyc: 0.251773, id: 0.272786] time: 2:26:22.054564 
[Epoch 25/30] [Batch 300/516] [D loss: 0.000172, acc: 100%] [G loss: 7.434660, adv: 0.952588, cyc: 0.253063, id: 0.262006] time: 2:27:30.776736 
[Epoch 25/30] [Batch 400/516] [D loss: 0.000104, acc: 100%] [G loss: 7.405381, adv: 0.976440, cyc: 0.249717, id: 0.253613] time: 2:28:39.494970 
[Epoch 25/30] [Batch 500/516] [D loss: 0.000112, acc: 100%] [G loss: 7.429981, adv: 0.973664, cyc: 0.250591, id: 0.261815] time: 2:29:48.218921 
[Epoch 26/30] [Batch 0/516] [D loss: 0.000119, acc: 100%] [G loss: 7.548261, adv: 0.977593, cyc: 0.255789, id: 0.275911] time: 2:30:00.237347 
[Epoch 26/30] [Batch 100/516] [D loss: 0.000072, acc: 100%] [G loss: 7.480881, adv: 0.980397, cyc: 0.252579, id: 0.267619] time: 2:31:08.951934 
[Epoch 26/30] [Batch 200/516] [D loss: 0.000135, acc: 100%] [G loss: 7.479433, adv: 0.996872, cyc: 0.250533, id: 0.277906] time: 2:32:17.679508 
[Epoch 26/30] [Batch 300/516] [D loss: 0.000261, acc: 100%] [G loss: 7.442534, adv: 0.973028, cyc: 0.251463, id: 0.275197] time: 2:33:26.406240 
[Epoch 26/30] [Batch 400/516] [D loss: 0.000208, acc: 100%] [G loss: 7.388265, adv: 0.997000, cyc: 0.246586, id: 0.268091] time: 2:34:35.126973 
[Epoch 26/30] [Batch 500/516] [D loss: 0.000231, acc: 100%] [G loss: 7.337692, adv: 0.941461, cyc: 0.249483, id: 0.256597] time: 2:35:43.844070 
[Epoch 27/30] [Batch 0/516] [D loss: 0.000235, acc: 100%] [G loss: 7.572542, adv: 0.975467, cyc: 0.256606, id: 0.282234] time: 2:35:54.839139 
[Epoch 27/30] [Batch 100/516] [D loss: 0.000109, acc: 100%] [G loss: 7.479634, adv: 0.995461, cyc: 0.250699, id: 0.271960] time: 2:37:03.562152 
[Epoch 27/30] [Batch 200/516] [D loss: 0.000103, acc: 100%] [G loss: 7.445181, adv: 0.987468, cyc: 0.249839, id: 0.268108] time: 2:38:12.284364 
[Epoch 27/30] [Batch 300/516] [D loss: 0.000165, acc: 100%] [G loss: 7.401807, adv: 0.970312, cyc: 0.249820, id: 0.265937] time: 2:39:21.002645 
[Epoch 27/30] [Batch 400/516] [D loss: 0.000114, acc: 100%] [G loss: 7.309064, adv: 0.973711, cyc: 0.244965, id: 0.258479] time: 2:40:29.716988 
[Epoch 27/30] [Batch 500/516] [D loss: 0.000052, acc: 100%] [G loss: 7.336097, adv: 0.954627, cyc: 0.248073, id: 0.257587] time: 2:41:38.442766 
[Epoch 28/30] [Batch 0/516] [D loss: 0.000323, acc: 100%] [G loss: 7.516649, adv: 0.984230, cyc: 0.253524, id: 0.276771] time: 2:41:49.438696 
[Epoch 28/30] [Batch 100/516] [D loss: 0.000224, acc: 100%] [G loss: 7.468256, adv: 0.996759, cyc: 0.249797, id: 0.272837] time: 2:42:58.153349 
[Epoch 28/30] [Batch 200/516] [D loss: 0.000234, acc: 100%] [G loss: 7.369957, adv: 0.981811, cyc: 0.247078, id: 0.270300] time: 2:44:06.879032 
[Epoch 28/30] [Batch 300/516] [D loss: 0.000158, acc: 100%] [G loss: 7.397624, adv: 0.982272, cyc: 0.248532, id: 0.260413] time: 2:45:15.599164 
[Epoch 28/30] [Batch 400/516] [D loss: 0.000063, acc: 100%] [G loss: 7.359097, adv: 0.983932, cyc: 0.246052, id: 0.253675] time: 2:46:24.313526 
[Epoch 28/30] [Batch 500/516] [D loss: 0.000114, acc: 100%] [G loss: 7.326796, adv: 0.976615, cyc: 0.245313, id: 0.263749] time: 2:47:33.031366 
[Epoch 29/30] [Batch 0/516] [D loss: 0.000145, acc: 100%] [G loss: 7.476525, adv: 0.983862, cyc: 0.251748, id: 0.272870] time: 2:47:44.024469 
[Epoch 29/30] [Batch 100/516] [D loss: 0.000123, acc: 100%] [G loss: 7.346045, adv: 0.946532, cyc: 0.248577, id: 0.263538] time: 2:48:52.723729 
[Epoch 29/30] [Batch 200/516] [D loss: 0.000093, acc: 100%] [G loss: 7.407710, adv: 0.992058, cyc: 0.247680, id: 0.267382] time: 2:50:01.442747 
[Epoch 29/30] [Batch 300/516] [D loss: 0.000142, acc: 100%] [G loss: 7.313055, adv: 0.943758, cyc: 0.247781, id: 0.264854] time: 2:51:10.156403 
[Epoch 29/30] [Batch 400/516] [D loss: 0.000135, acc: 100%] [G loss: 7.309592, adv: 0.972574, cyc: 0.245561, id: 0.258040] time: 2:52:18.882663 
[Epoch 29/30] [Batch 500/516] [D loss: 0.000151, acc: 100%] [G loss: 7.357599, adv: 0.989505, cyc: 0.245498, id: 0.261466] time: 2:53:27.602758 
[Epoch 30/30] [Batch 0/516] [D loss: 0.000065, acc: 100%] [G loss: 7.373823, adv: 0.937442, cyc: 0.250795, id: 0.267046] time: 2:53:38.594197 
[Epoch 30/30] [Batch 100/516] [D loss: 0.000131, acc: 100%] [G loss: 7.350891, adv: 0.944019, cyc: 0.249177, id: 0.263510] time: 2:54:47.312272 
[Epoch 30/30] [Batch 200/516] [D loss: 0.000084, acc: 100%] [G loss: 7.325631, adv: 0.979254, cyc: 0.245104, id: 0.263482] time: 2:55:56.028136 
[Epoch 30/30] [Batch 300/516] [D loss: 0.000054, acc: 100%] [G loss: 7.413359, adv: 0.984373, cyc: 0.248704, id: 0.257088] time: 2:57:04.739111 
[Epoch 30/30] [Batch 400/516] [D loss: 0.000199, acc: 100%] [G loss: 7.305603, adv: 0.986112, cyc: 0.243891, id: 0.253707] time: 2:58:13.457079 
[Epoch 30/30] [Batch 500/516] [D loss: 0.000122, acc: 100%] [G loss: 7.339503, adv: 0.988966, cyc: 0.245005, id: 0.258218] time: 2:59:22.171662 
Train Finished.
