_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         (None, 1, 600, 1)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 1, 600, 64)        256       
_________________________________________________________________
leaky_re_lu_5 (LeakyReLU)    (None, 1, 600, 64)        0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 1, 600, 128)       24704     
_________________________________________________________________
leaky_re_lu_6 (LeakyReLU)    (None, 1, 600, 128)       0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 76800)             0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               39322112  
_________________________________________________________________
leaky_re_lu_7 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 512)               262656    
_________________________________________________________________
leaky_re_lu_8 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
D_A_Out (Dense)              (None, 1)                 513       
=================================================================
Total params: 79,220,482
Trainable params: 39,610,241
Non-trainable params: 39,610,241
_________________________________________________________________
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 1, 600, 1)         0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 1, 600, 64)        256       
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU)    (None, 1, 600, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 1, 600, 128)       24704     
_________________________________________________________________
leaky_re_lu_2 (LeakyReLU)    (None, 1, 600, 128)       0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 76800)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               39322112  
_________________________________________________________________
leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         
_________________________________________________________________
D_B_Out (Dense)              (None, 1)                 513       
=================================================================
Total params: 79,220,482
Trainable params: 39,610,241
Non-trainable params: 39,610,241
_________________________________________________________________
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
ivec_b (InputLayer)             (None, 1, 600, 1)    0                                            
__________________________________________________________________________________________________
ivec_a (InputLayer)             (None, 1, 600, 1)    0                                            
__________________________________________________________________________________________________
model_4 (Model)                 (None, 1, 600, 1)    653345      ivec_b[0][0]                     
                                                                 model_3[1][0]                    
                                                                 ivec_a[0][0]                     
__________________________________________________________________________________________________
model_3 (Model)                 (None, 1, 600, 1)    653345      ivec_a[0][0]                     
                                                                 model_4[1][0]                    
                                                                 ivec_b[0][0]                     
__________________________________________________________________________________________________
model_2 (Model)                 (None, 1)            39610241    model_4[1][0]                    
__________________________________________________________________________________________________
model_1 (Model)                 (None, 1)            39610241    model_3[1][0]                    
==================================================================================================
Total params: 80,527,172
Trainable params: 1,306,690
Non-trainable params: 79,220,482
__________________________________________________________________________________________________
Start CycleGAN training.
[Epoch 1/20] [Batch 0/516] [D loss: 2.431167, acc:   0%] [G loss: 21.694403, adv: 0.156291, cyc: 0.972672, id: 0.965957] time: 0:02:26.317123 
[Epoch 1/20] [Batch 100/516] [D loss: 0.205275, acc:  70%] [G loss: 14.574638, adv: 0.556203, cyc: 0.623100, id: 0.498106] time: 0:03:37.321297 
[Epoch 1/20] [Batch 200/516] [D loss: 0.021745, acc: 100%] [G loss: 12.771660, adv: 0.852562, cyc: 0.511263, id: 0.421481] time: 0:04:48.307044 
[Epoch 1/20] [Batch 300/516] [D loss: 0.026404, acc: 100%] [G loss: 11.920883, adv: 0.743580, cyc: 0.481381, id: 0.393197] time: 0:05:59.258498 
[Epoch 1/20] [Batch 400/516] [D loss: 0.018412, acc: 100%] [G loss: 11.712559, adv: 0.795710, cyc: 0.466224, id: 0.393927] time: 0:07:10.190027 
[Epoch 1/20] [Batch 500/516] [D loss: 0.009234, acc: 100%] [G loss: 11.526507, adv: 0.827271, cyc: 0.454245, id: 0.379587] time: 0:08:21.112695 
[Epoch 2/20] [Batch 0/516] [D loss: 0.009445, acc:  99%] [G loss: 11.488680, adv: 0.870312, cyc: 0.448441, id: 0.395455] time: 0:08:32.415082 
[Epoch 2/20] [Batch 100/516] [D loss: 0.008251, acc: 100%] [G loss: 10.754709, adv: 0.844018, cyc: 0.417520, id: 0.354545] time: 0:09:43.352951 
[Epoch 2/20] [Batch 200/516] [D loss: 0.006684, acc:  99%] [G loss: 10.416364, adv: 0.891196, cyc: 0.397975, id: 0.341067] time: 0:10:54.294145 
[Epoch 2/20] [Batch 300/516] [D loss: 0.004265, acc: 100%] [G loss: 10.498889, adv: 0.960885, cyc: 0.394952, id: 0.329178] time: 0:12:05.220484 
[Epoch 2/20] [Batch 400/516] [D loss: 0.005382, acc: 100%] [G loss: 10.316109, adv: 0.950061, cyc: 0.387224, id: 0.329813] time: 0:13:16.131286 
[Epoch 2/20] [Batch 500/516] [D loss: 0.008532, acc: 100%] [G loss: 10.381288, adv: 1.034480, cyc: 0.382221, id: 0.319903] time: 0:14:27.024992 
[Epoch 3/20] [Batch 0/516] [D loss: 0.003853, acc: 100%] [G loss: 10.258862, adv: 0.970150, cyc: 0.382794, id: 0.334298] time: 0:14:38.357003 
[Epoch 3/20] [Batch 100/516] [D loss: 0.004533, acc: 100%] [G loss: 9.902827, adv: 0.958322, cyc: 0.367360, id: 0.319227] time: 0:15:49.281254 
[Epoch 3/20] [Batch 200/516] [D loss: 0.008267, acc: 100%] [G loss: 9.825112, adv: 1.031360, cyc: 0.357231, id: 0.307368] time: 0:17:00.194762 
[Epoch 3/20] [Batch 300/516] [D loss: 0.003485, acc: 100%] [G loss: 9.719362, adv: 0.949674, cyc: 0.359935, id: 0.301980] time: 0:18:10.965040 
[Epoch 3/20] [Batch 400/516] [D loss: 0.002879, acc: 100%] [G loss: 9.648720, adv: 0.968455, cyc: 0.354719, id: 0.308670] time: 0:19:21.861184 
[Epoch 3/20] [Batch 500/516] [D loss: 0.016270, acc:  99%] [G loss: 9.772835, adv: 1.005478, cyc: 0.356742, id: 0.301492] time: 0:20:32.770242 
[Epoch 4/20] [Batch 0/516] [D loss: 0.001362, acc: 100%] [G loss: 9.681409, adv: 0.974381, cyc: 0.355541, id: 0.311967] time: 0:20:44.153337 
[Epoch 4/20] [Batch 100/516] [D loss: 0.005505, acc: 100%] [G loss: 9.401633, adv: 0.952069, cyc: 0.344801, id: 0.300605] time: 0:21:55.040422 
[Epoch 4/20] [Batch 200/516] [D loss: 0.001972, acc: 100%] [G loss: 9.271831, adv: 0.971858, cyc: 0.337296, id: 0.293252] time: 0:23:05.940445 
[Epoch 4/20] [Batch 300/516] [D loss: 0.001570, acc: 100%] [G loss: 9.400509, adv: 0.980163, cyc: 0.342142, id: 0.288424] time: 0:24:16.839744 
[Epoch 4/20] [Batch 400/516] [D loss: 0.001585, acc: 100%] [G loss: 9.339273, adv: 0.991691, cyc: 0.338318, id: 0.295282] time: 0:25:27.753889 
[Epoch 4/20] [Batch 500/516] [D loss: 0.002307, acc: 100%] [G loss: 9.507379, adv: 1.017875, cyc: 0.342977, id: 0.288708] time: 0:26:38.670600 
[Epoch 5/20] [Batch 0/516] [D loss: 0.002390, acc: 100%] [G loss: 9.463257, adv: 0.983738, cyc: 0.344058, id: 0.301715] time: 0:26:50.022919 
[Epoch 5/20] [Batch 100/516] [D loss: 0.005662, acc: 100%] [G loss: 9.163371, adv: 0.977955, cyc: 0.331438, id: 0.289672] time: 0:28:00.913186 
[Epoch 5/20] [Batch 200/516] [D loss: 0.001092, acc: 100%] [G loss: 9.029389, adv: 0.992152, cyc: 0.324036, id: 0.282028] time: 0:29:11.807388 
[Epoch 5/20] [Batch 300/516] [D loss: 0.001455, acc: 100%] [G loss: 9.162963, adv: 0.977300, cyc: 0.331110, id: 0.290812] time: 0:30:22.721220 
[Epoch 5/20] [Batch 400/516] [D loss: 0.000779, acc: 100%] [G loss: 9.076899, adv: 0.970581, cyc: 0.328005, id: 0.290459] time: 0:31:33.638156 
[Epoch 5/20] [Batch 500/516] [D loss: 0.002719, acc: 100%] [G loss: 9.158939, adv: 0.975390, cyc: 0.331166, id: 0.285062] time: 0:32:44.556489 
[Epoch 6/20] [Batch 0/516] [D loss: 0.002875, acc: 100%] [G loss: 9.146032, adv: 0.973613, cyc: 0.330711, id: 0.297137] time: 0:33:03.425030 
[Epoch 6/20] [Batch 100/516] [D loss: 0.002733, acc: 100%] [G loss: 9.089844, adv: 0.979409, cyc: 0.327441, id: 0.274661] time: 0:34:13.594386 
[Epoch 6/20] [Batch 200/516] [D loss: 0.003775, acc: 100%] [G loss: 8.563417, adv: 0.848831, cyc: 0.315614, id: 0.275220] time: 0:35:23.730772 
[Epoch 6/20] [Batch 300/516] [D loss: 0.003953, acc: 100%] [G loss: 9.072199, adv: 0.995615, cyc: 0.325028, id: 0.278303] time: 0:36:33.874117 
[Epoch 6/20] [Batch 400/516] [D loss: 0.001188, acc: 100%] [G loss: 8.868901, adv: 0.941846, cyc: 0.320576, id: 0.278842] time: 0:37:44.014379 
[Epoch 6/20] [Batch 500/516] [D loss: 0.002379, acc: 100%] [G loss: 9.017000, adv: 0.998823, cyc: 0.322027, id: 0.277848] time: 0:38:54.163186 
[Epoch 7/20] [Batch 0/516] [D loss: 0.002563, acc: 100%] [G loss: 8.978847, adv: 0.971465, cyc: 0.323202, id: 0.291617] time: 0:39:05.386312 
[Epoch 7/20] [Batch 100/516] [D loss: 0.001559, acc: 100%] [G loss: 8.806366, adv: 0.977899, cyc: 0.314785, id: 0.273080] time: 0:40:15.527471 
[Epoch 7/20] [Batch 200/516] [D loss: 0.002249, acc: 100%] [G loss: 8.713148, adv: 0.995500, cyc: 0.309027, id: 0.266990] time: 0:41:25.663281 
[Epoch 7/20] [Batch 300/516] [D loss: 0.001435, acc: 100%] [G loss: 8.859462, adv: 0.979877, cyc: 0.316856, id: 0.272401] time: 0:42:35.798039 
[Epoch 7/20] [Batch 400/516] [D loss: 0.001491, acc: 100%] [G loss: 8.810041, adv: 0.954014, cyc: 0.316596, id: 0.287746] time: 0:43:45.931318 
[Epoch 7/20] [Batch 500/516] [D loss: 0.000990, acc: 100%] [G loss: 8.927811, adv: 0.961370, cyc: 0.321326, id: 0.289889] time: 0:44:56.062161 
[Epoch 8/20] [Batch 0/516] [D loss: 0.003193, acc: 100%] [G loss: 8.886362, adv: 0.966035, cyc: 0.318988, id: 0.302222] time: 0:45:07.283323 
[Epoch 8/20] [Batch 100/516] [D loss: 0.000593, acc: 100%] [G loss: 8.731597, adv: 0.984344, cyc: 0.310314, id: 0.270050] time: 0:46:17.407207 
[Epoch 8/20] [Batch 200/516] [D loss: 0.001138, acc: 100%] [G loss: 8.527484, adv: 0.961110, cyc: 0.303267, id: 0.269899] time: 0:47:27.534396 
[Epoch 8/20] [Batch 300/516] [D loss: 0.002333, acc: 100%] [G loss: 8.759381, adv: 0.959141, cyc: 0.313664, id: 0.262356] time: 0:48:37.668256 
[Epoch 8/20] [Batch 400/516] [D loss: 0.000507, acc: 100%] [G loss: 8.719131, adv: 0.995379, cyc: 0.308980, id: 0.276950] time: 0:49:47.792708 
[Epoch 8/20] [Batch 500/516] [D loss: 0.001689, acc: 100%] [G loss: 8.897753, adv: 0.949774, cyc: 0.320677, id: 0.295114] time: 0:50:57.916288 
[Epoch 9/20] [Batch 0/516] [D loss: 0.001929, acc: 100%] [G loss: 8.866139, adv: 0.948135, cyc: 0.319158, id: 0.279416] time: 0:51:09.135777 
[Epoch 9/20] [Batch 100/516] [D loss: 0.031069, acc:  95%] [G loss: 8.734797, adv: 1.022741, cyc: 0.306570, id: 0.264738] time: 0:52:19.263451 
[Epoch 9/20] [Batch 200/516] [D loss: 0.000771, acc: 100%] [G loss: 8.377426, adv: 0.926925, cyc: 0.299073, id: 0.276249] time: 0:53:29.404182 
[Epoch 9/20] [Batch 300/516] [D loss: 0.002248, acc: 100%] [G loss: 8.675745, adv: 0.972672, cyc: 0.308682, id: 0.260545] time: 0:54:39.547502 
[Epoch 9/20] [Batch 400/516] [D loss: 0.003932, acc: 100%] [G loss: 8.591908, adv: 0.969136, cyc: 0.305161, id: 0.272257] time: 0:55:49.686956 
[Epoch 9/20] [Batch 500/516] [D loss: 0.000783, acc: 100%] [G loss: 8.729055, adv: 0.988271, cyc: 0.309276, id: 0.267248] time: 0:56:59.819936 
[Epoch 10/20] [Batch 0/516] [D loss: 0.000811, acc: 100%] [G loss: 8.774618, adv: 0.980622, cyc: 0.311676, id: 0.277884] time: 0:57:11.039844 
[Epoch 10/20] [Batch 100/516] [D loss: 0.001356, acc: 100%] [G loss: 8.660627, adv: 0.994554, cyc: 0.305428, id: 0.261939] time: 0:58:21.169354 
[Epoch 10/20] [Batch 200/516] [D loss: 0.001623, acc: 100%] [G loss: 8.372897, adv: 0.946901, cyc: 0.296831, id: 0.263402] time: 0:59:31.297590 
[Epoch 10/20] [Batch 300/516] [D loss: 0.000855, acc: 100%] [G loss: 8.580398, adv: 0.983803, cyc: 0.303276, id: 0.265474] time: 1:00:41.425246 
[Epoch 10/20] [Batch 400/516] [D loss: 0.000602, acc: 100%] [G loss: 8.573000, adv: 0.962067, cyc: 0.304610, id: 0.266844] time: 1:01:51.540147 
[Epoch 10/20] [Batch 500/516] [D loss: 0.000777, acc: 100%] [G loss: 8.712669, adv: 0.978372, cyc: 0.309180, id: 0.262783] time: 1:03:01.664321 
[Epoch 11/20] [Batch 0/516] [D loss: 0.000955, acc: 100%] [G loss: 8.613699, adv: 0.953444, cyc: 0.306997, id: 0.275507] time: 1:03:14.440603 
[Epoch 11/20] [Batch 100/516] [D loss: 0.000382, acc: 100%] [G loss: 8.432263, adv: 0.971444, cyc: 0.297352, id: 0.264366] time: 1:04:24.559983 
[Epoch 11/20] [Batch 200/516] [D loss: 0.001125, acc: 100%] [G loss: 8.347178, adv: 0.995896, cyc: 0.291478, id: 0.262166] time: 1:05:34.675441 
[Epoch 11/20] [Batch 300/516] [D loss: 0.000589, acc: 100%] [G loss: 8.433226, adv: 0.966524, cyc: 0.297941, id: 0.261006] time: 1:06:44.774238 
[Epoch 11/20] [Batch 400/516] [D loss: 0.000411, acc: 100%] [G loss: 8.487516, adv: 0.982004, cyc: 0.299099, id: 0.262864] time: 1:07:54.864737 
[Epoch 11/20] [Batch 500/516] [D loss: 0.001461, acc: 100%] [G loss: 8.378371, adv: 0.867244, cyc: 0.304205, id: 0.279729] time: 1:09:04.938634 
[Epoch 12/20] [Batch 0/516] [D loss: 0.000479, acc: 100%] [G loss: 8.534770, adv: 0.958611, cyc: 0.302945, id: 0.292291] time: 1:09:16.148166 
[Epoch 12/20] [Batch 100/516] [D loss: 0.000862, acc: 100%] [G loss: 8.220264, adv: 0.897161, cyc: 0.294418, id: 0.258186] time: 1:10:26.207593 
[Epoch 12/20] [Batch 200/516] [D loss: 0.000334, acc: 100%] [G loss: 8.278011, adv: 0.989000, cyc: 0.288666, id: 0.261495] time: 1:11:36.273960 
[Epoch 12/20] [Batch 300/516] [D loss: 0.001267, acc: 100%] [G loss: 8.371014, adv: 0.943731, cyc: 0.296933, id: 0.266052] time: 1:12:46.334030 
[Epoch 12/20] [Batch 400/516] [D loss: 0.001092, acc: 100%] [G loss: 8.415627, adv: 0.980122, cyc: 0.295717, id: 0.271206] time: 1:13:56.389469 
[Epoch 12/20] [Batch 500/516] [D loss: 0.001684, acc: 100%] [G loss: 8.483996, adv: 0.969161, cyc: 0.299471, id: 0.258999] time: 1:15:06.443762 
[Epoch 13/20] [Batch 0/516] [D loss: 0.000393, acc: 100%] [G loss: 8.507528, adv: 0.955683, cyc: 0.301686, id: 0.268712] time: 1:15:17.655440 
[Epoch 13/20] [Batch 100/516] [D loss: 0.000263, acc: 100%] [G loss: 8.384933, adv: 0.974935, cyc: 0.294300, id: 0.251409] time: 1:16:27.718485 
[Epoch 13/20] [Batch 200/516] [D loss: 0.000229, acc: 100%] [G loss: 8.175446, adv: 0.965263, cyc: 0.285794, id: 0.253614] time: 1:17:37.779826 
[Epoch 13/20] [Batch 300/516] [D loss: 0.001119, acc: 100%] [G loss: 8.317325, adv: 0.974114, cyc: 0.291601, id: 0.257010] time: 1:18:47.841579 
[Epoch 13/20] [Batch 400/516] [D loss: 0.000295, acc: 100%] [G loss: 8.340401, adv: 0.953272, cyc: 0.294231, id: 0.288570] time: 1:19:57.899570 
[Epoch 13/20] [Batch 500/516] [D loss: 0.001653, acc: 100%] [G loss: 8.495550, adv: 0.994286, cyc: 0.297580, id: 0.275447] time: 1:21:07.961242 
[Epoch 14/20] [Batch 0/516] [D loss: 0.000530, acc: 100%] [G loss: 8.404697, adv: 0.955648, cyc: 0.296791, id: 0.291708] time: 1:21:19.171906 
[Epoch 14/20] [Batch 100/516] [D loss: 0.001358, acc: 100%] [G loss: 8.275887, adv: 0.987518, cyc: 0.288494, id: 0.255733] time: 1:22:29.232042 
[Epoch 14/20] [Batch 200/516] [D loss: 0.000266, acc: 100%] [G loss: 7.961127, adv: 0.856148, cyc: 0.285738, id: 0.279210] time: 1:23:39.306583 
[Epoch 14/20] [Batch 300/516] [D loss: 0.000433, acc: 100%] [G loss: 8.262403, adv: 0.971109, cyc: 0.289046, id: 0.254833] time: 1:24:49.374357 
[Epoch 14/20] [Batch 400/516] [D loss: 0.000198, acc: 100%] [G loss: 8.243686, adv: 0.927183, cyc: 0.292154, id: 0.256483] time: 1:25:59.443964 
[Epoch 14/20] [Batch 500/516] [D loss: 0.000312, acc: 100%] [G loss: 8.360182, adv: 0.980748, cyc: 0.292477, id: 0.258238] time: 1:27:09.505878 
[Epoch 15/20] [Batch 0/516] [D loss: 0.000215, acc: 100%] [G loss: 8.260582, adv: 0.904632, cyc: 0.294774, id: 0.279668] time: 1:27:20.715409 
[Epoch 15/20] [Batch 100/516] [D loss: 0.003593, acc:  99%] [G loss: 8.141013, adv: 0.949671, cyc: 0.285372, id: 0.261574] time: 1:28:30.783896 
[Epoch 15/20] [Batch 200/516] [D loss: 0.000392, acc: 100%] [G loss: 8.119006, adv: 0.979879, cyc: 0.282146, id: 0.265272] time: 1:29:40.851852 
[Epoch 15/20] [Batch 300/516] [D loss: 0.000531, acc: 100%] [G loss: 8.190386, adv: 0.958823, cyc: 0.286772, id: 0.265417] time: 1:30:50.920749 
[Epoch 15/20] [Batch 400/516] [D loss: 0.024606, acc:  96%] [G loss: 8.433146, adv: 0.988290, cyc: 0.295005, id: 0.293178] time: 1:32:00.990317 
[Epoch 15/20] [Batch 500/516] [D loss: 0.000644, acc: 100%] [G loss: 8.300097, adv: 0.949405, cyc: 0.292416, id: 0.249498] time: 1:33:11.061545 
[Epoch 16/20] [Batch 0/516] [D loss: 0.000504, acc: 100%] [G loss: 8.304263, adv: 0.955506, cyc: 0.292028, id: 0.298517] time: 1:33:24.058609 
[Epoch 16/20] [Batch 100/516] [D loss: 0.000719, acc: 100%] [G loss: 8.120714, adv: 0.981129, cyc: 0.281616, id: 0.255935] time: 1:34:34.123115 
[Epoch 16/20] [Batch 200/516] [D loss: 0.000489, acc: 100%] [G loss: 8.002419, adv: 0.981482, cyc: 0.276417, id: 0.253854] time: 1:35:44.192502 
[Epoch 16/20] [Batch 300/516] [D loss: 0.002343, acc: 100%] [G loss: 7.883474, adv: 0.798698, cyc: 0.287231, id: 0.242873] time: 1:36:54.261015 
[Epoch 16/20] [Batch 400/516] [D loss: 0.000371, acc: 100%] [G loss: 8.271017, adv: 1.004552, cyc: 0.286066, id: 0.284022] time: 1:38:04.338916 
[Epoch 16/20] [Batch 500/516] [D loss: 0.000666, acc: 100%] [G loss: 8.274440, adv: 0.984467, cyc: 0.288275, id: 0.256074] time: 1:39:14.407212 
[Epoch 17/20] [Batch 0/516] [D loss: 0.002445, acc: 100%] [G loss: 8.343069, adv: 1.006364, cyc: 0.289281, id: 0.266395] time: 1:39:25.617747 
[Epoch 17/20] [Batch 100/516] [D loss: 0.000745, acc: 100%] [G loss: 7.907452, adv: 0.908587, cyc: 0.278540, id: 0.256139] time: 1:40:35.683077 
[Epoch 17/20] [Batch 200/516] [D loss: 0.009896, acc: 100%] [G loss: 8.048514, adv: 0.992147, cyc: 0.277094, id: 0.248793] time: 1:41:45.753072 
[Epoch 17/20] [Batch 300/516] [D loss: 0.000716, acc: 100%] [G loss: 8.118426, adv: 0.970687, cyc: 0.282354, id: 0.249059] time: 1:42:55.831461 
[Epoch 17/20] [Batch 400/516] [D loss: 0.000762, acc: 100%] [G loss: 7.676835, adv: 0.768094, cyc: 0.280645, id: 0.258512] time: 1:44:05.905735 
[Epoch 17/20] [Batch 500/516] [D loss: 0.003058, acc:  99%] [G loss: 8.197227, adv: 0.886174, cyc: 0.293148, id: 0.290891] time: 1:45:15.968614 
[Epoch 18/20] [Batch 0/516] [D loss: 0.000518, acc: 100%] [G loss: 8.114526, adv: 0.932924, cyc: 0.285672, id: 0.267168] time: 1:45:27.179296 
[Epoch 18/20] [Batch 100/516] [D loss: 0.000296, acc: 100%] [G loss: 8.017408, adv: 0.982862, cyc: 0.276638, id: 0.252141] time: 1:46:37.250959 
[Epoch 18/20] [Batch 200/516] [D loss: 0.000403, acc: 100%] [G loss: 7.936519, adv: 0.981787, cyc: 0.273187, id: 0.244771] time: 1:47:47.324284 
[Epoch 18/20] [Batch 300/516] [D loss: 0.000177, acc: 100%] [G loss: 8.014611, adv: 0.953178, cyc: 0.279051, id: 0.253306] time: 1:48:57.390879 
[Epoch 18/20] [Batch 400/516] [D loss: 0.000150, acc: 100%] [G loss: 8.074582, adv: 0.966428, cyc: 0.280576, id: 0.247299] time: 1:50:07.457763 
[Epoch 18/20] [Batch 500/516] [D loss: 0.000578, acc: 100%] [G loss: 8.153632, adv: 0.986557, cyc: 0.282503, id: 0.255731] time: 1:51:17.515248 
[Epoch 19/20] [Batch 0/516] [D loss: 0.000953, acc: 100%] [G loss: 8.126317, adv: 0.980498, cyc: 0.281911, id: 0.266377] time: 1:51:28.724548 
[Epoch 19/20] [Batch 100/516] [D loss: 0.000329, acc: 100%] [G loss: 7.983727, adv: 0.972747, cyc: 0.275639, id: 0.249799] time: 1:52:38.789846 
[Epoch 19/20] [Batch 200/516] [D loss: 0.000488, acc: 100%] [G loss: 7.845129, adv: 0.977295, cyc: 0.269309, id: 0.246805] time: 1:53:48.861869 
[Epoch 19/20] [Batch 300/516] [D loss: 0.000351, acc: 100%] [G loss: 7.947465, adv: 0.938312, cyc: 0.276949, id: 0.261322] time: 1:54:58.926762 
[Epoch 19/20] [Batch 400/516] [D loss: 0.000661, acc: 100%] [G loss: 7.997947, adv: 0.975012, cyc: 0.276454, id: 0.250471] time: 1:56:08.989141 
[Epoch 19/20] [Batch 500/516] [D loss: 0.000196, acc: 100%] [G loss: 8.077291, adv: 0.968332, cyc: 0.280556, id: 0.248324] time: 1:57:19.058631 
[Epoch 20/20] [Batch 0/516] [D loss: 0.000254, acc: 100%] [G loss: 8.142608, adv: 0.988045, cyc: 0.281926, id: 0.256864] time: 1:57:30.268858 
[Epoch 20/20] [Batch 100/516] [D loss: 0.000754, acc: 100%] [G loss: 7.940819, adv: 0.981120, cyc: 0.273544, id: 0.252262] time: 1:58:40.343055 
[Epoch 20/20] [Batch 200/516] [D loss: 0.000495, acc: 100%] [G loss: 7.885103, adv: 0.986570, cyc: 0.269691, id: 0.246623] time: 1:59:50.409669 
[Epoch 20/20] [Batch 300/516] [D loss: 0.000504, acc: 100%] [G loss: 7.980616, adv: 0.965663, cyc: 0.276139, id: 0.258776] time: 2:01:00.474451 
[Epoch 20/20] [Batch 400/516] [D loss: 0.000155, acc: 100%] [G loss: 7.955798, adv: 0.980511, cyc: 0.273949, id: 0.252676] time: 2:02:10.534691 
[Epoch 20/20] [Batch 500/516] [D loss: 0.000439, acc: 100%] [G loss: 8.063890, adv: 0.939084, cyc: 0.281598, id: 0.281144] time: 2:03:20.607505 
Train Finished.
